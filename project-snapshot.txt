This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
debug/
  substack-politics.html
  substack-puppeteer.html
docs/
  ARCHITECTURE.md
  CLOUD_INVENTORY.md
  DEPLOY_SNAPSHOT.md
  INGEST_AUDIT_SUMMARY.md
  INGEST_CLOUD_PLAN.md
  INGEST_IMPLEMENTATION_SUMMARY.md
  INGEST_OAUTH_DECISION.md
  LATEST_IMAGE.txt
  MIGRATION_PLAYBOOK.md
  PHASE0_SPEC.md
  RUNBOOK.md
  SCHEDULING_PLAN.md
  SECRETS_GMAIL.md
  SESSION_SUMMARY_2025-11-05.md
  STATUS_SNAPSHOT.md
  STRATEGIC_STATUS.md
  TODO_BACKLOG.md
  UNIFIED_VIEWS.sql
  VALIDATION_SQL.sql
newsletter-search/
  public/
    file.svg
    globe.svg
    next.svg
    vercel.svg
    window.svg
  src/
    app/
      api/
        intelligence/
          query/
            route.ts
        newsletter/
          [id]/
            route.ts
        search/
          route.ts
      newsletter/
        [id]/
          page.tsx
      favicon.ico
      globals.css
      layout.tsx
      page-semantic.tsx
      page.tsx
    lib/
      newsletter-cleaning.ts
  .gitignore
  eslint.config.mjs
  next.config.ts
  postcss.config.mjs
  README.md
scripts/
  cloud/
    auth-sa.ts
    bootstrap-cloud-access.ts
    build-image.ts
    deploy-jobs.ts
    deploy-runner.ts
    diagnose-build-perms.ts
    discover-inventory.ts
    ensure-iam.ts
    fix-build-perms.ts
    gcloud-doctor.ts
    grant-view-roles.ts
    plan.ts
    print-key-sa.ts
    remediate-from-issues.ts
    schedule-jobs.ts
    scheduler-toggle.ts
    show-issues.ts
    snapshot.ts
  dev/
    doctor-env.ts
  gmail/
    mint-refresh-token.ts
    run-live-test.ts
    spot-check.ts
    update-refresh-secrets.ts
  ingest/
    preflight.ts
  legacy/
    full-chunk-and-embed.ts
    process-newsletters.ts
    README.md
    run-overnight-tranche1.sh
    setup-service-account.sh
  ops/
    check-recent-inserts.ts
    create-uptime-and-alert.ts
    pipeline-status.ts
    verify-health.ts
    verify-ingest-live.ts
  publishers/
    add-manual-override-fields.ts
    calculate-citations-pattern-based-robust.ts
    calculate-citations-pattern-based.ts
    calculate-citations.ts
    calculate-quality-scores.ts
    create-publishers-table.ts
    export-publishers-simple.ts
    export-quality-scores.ts
    extract-existing-publishers.ts
    link-discovered-newsletters.ts
    manual-override-quality-score.ts
    populate-quality-signals.ts
    update-citation-counts.ts
    update-citations-only.ts
  add-doc-ids-provenance.ts
  backfill-sent-date.ts
  check-labels-and-recent.ts
  check-processed-data.ts
  chunk-new.ts
  classify-recent.ts
  create-unified-views.ts
  deduplicate-chunks.ts
  embed-new-chunks.ts
  evaluate-rag.ts
  exchange-code-for-token.ts
  find-nsm-emails.ts
  get-bigquery-refresh-token.ts
  get-gmail-token.js
  historical-report.ts
  ingest-and-chunk-inbox.ts
  ingest-gmail.ts
  ingest-recent-inbox.ts
  ingest-specific-inbox.ts
  ingest-to-bigquery.ts
  list-recent.ts
  migrate-is-paid-column.ts
  migrate-legacy-to-prod.ts
  migrate-schema-dual-inbox.ts
  optimize-bigquery-tables.ts
  preview-vip.ts
  refresh-auth.ts
  report-legacy-schema.ts
  report-reconcile.ts
  report-unified.ts
  run-pipeline.ts
  setup-bigquery.ts
  smoke-check.ts
  smoke.ts
  update-vip-flags.ts
  verify-env.ts
  verify-gcp-auth.ts
  whoami.ts
src/
  api/
    admin.ts
    intelligence.ts
    jobs-runner.ts
    search.ts
  bq/
    client.ts
  core/
    checkpoint.ts
    ingestion.ts
    processor.ts
    publisher.ts
  embeddings/
    vertex.ts
  gmail/
    client.ts
    token-provider.ts
  lib/
    bigquery.ts
    config.ts
    deduplication.ts
    gmail.ts
    parseMessage.ts
    vertex.ts
  ops/
    health.ts
  types/
    index.ts
  index.js
  types.ts
.env.example
.gcloudignore
.gitignore
CHECK_28K_CORPUS.sh
CHECK_PROGRESS.sh
create-service-account-key.sh
DEPLOY_DISCOVERY.sh
DEPLOY_EVAL.sh
DEPLOY_FIX.sh
Dockerfile
Dockerfile.discovery
Dockerfile.eval
FIX_AND_RESTART.sh
live-monitor.sh
monitor-discovery-cloud.sh
monitor-discovery-live.sh
monitor-discovery.sh
monitor-live.sh
README.md
REDEPLOY_AND_RUN.sh
refresh-adc.sh
RUN_EVAL_IN_CLOUD_SHELL.sh
START_25K_BATCH.sh
START_REMAINING_BATCH.sh
WATCH_LOGS.sh
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/CLOUD_INVENTORY.md">
# Cloud Inventory (Generated)

**Generated:** 2025-11-04T22:11:20.695Z
**Project:** newsletter-control-center
**BigQuery Location:** US
**Cloud Run Region:** us-central1

## Cloud Run Jobs

| Name | Image | Created | Updated |
|------|-------|---------|---------|
| unknown | unknown | 2025-11-03T01:45:38.579984Z | unknown |
| unknown | unknown | 2025-10-31T16:16:14.857905Z | unknown |
| unknown | unknown | 2025-10-30T20:39:14.321174Z | BETA |

## Cloud Run Services

| Name | Image | Created |
|------|-------|---------|
| ncc | us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc:latest | 2025-10-27T23:53:03.231462Z |

## Cloud Scheduler

| Name | Schedule | Target |
|------|----------|--------|
| ncc-daily | 5 7 * * * | https://ncc-d6cqllgv7a-uc.a.run.app/run |

## Container Images

### GCR Images (gcr.io/newsletter-control-center)

- gcr.io/newsletter-control-center/discover-newsletters
- gcr.io/newsletter-control-center/eval-rag
- gcr.io/newsletter-control-center/newsletter-processor
- gcr.io/newsletter-control-center/process-newsletters

### Artifact Registry Repositories

- projects/newsletter-control-center/locations/us/repositories/gcr.io
- projects/newsletter-control-center/locations/us-central1/repositories/ncc

## Service Accounts

| Email | Display Name | Key Roles |
|-------|--------------|-----------|
| 274400323957-compute@developer.gserviceaccount.com | Default compute service account | roles/aiplatform.user, roles/artifactregistry.admin, roles/bigquery.dataEditor... |
| ncc-scheduler@newsletter-control-center.iam.gserviceaccount.com | NCC Scheduler | - |
| newsletter-bigquery-sa@newsletter-control-center.iam.gserviceaccount.com | newsletter-bigquery-sa | roles/bigquery.dataEditor, roles/bigquery.jobUser |
| newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com | Newsletter Control Center - Local Dev | roles/aiplatform.user, roles/bigquery.dataEditor, roles/bigquery.jobUser... |

## Secrets

- projects/274400323957/secrets/GMAIL_OAUTH_CLIENT_ID
- projects/274400323957/secrets/GMAIL_OAUTH_CLIENT_SECRET
- projects/274400323957/secrets/GMAIL_OAUTH_REFRESH_TOKEN
- projects/274400323957/secrets/bigquery-project
- projects/274400323957/secrets/gmail-clean-token
- projects/274400323957/secrets/gmail-client-id
- projects/274400323957/secrets/gmail-client-secret
- projects/274400323957/secrets/gmail-legacy-token
- projects/274400323957/secrets/google-custom-search-api-key
- projects/274400323957/secrets/google-custom-search-engine-id

## BigQuery

### Datasets (matching /ncc/i)

- ncc_newsletters
- ncc_production

### ncc_production

| Table | Row Count |
|-------|-----------|
| chunk_embeddings | 50 |
| chunks | 237 |
| email_labels | 59 |
| ingest_state | 0 |
| processing_status | 0 |
| publisher_aliases | 0 |
| publishers | 0 |
| raw_emails | 20 |
| v_all_chunk_embeddings | 50 |
| v_all_chunks | >100k rows |
| v_all_raw_emails | 74611 |

### ncc_newsletters

| Table | Row Count |
|-------|-----------|
| chunks | >100k rows |
| chunks_duplicates_backup_1762039374 | >100k rows |
| discovered_newsletters | 1013 |
| eval_results | 70 |
| messages | 74591 |
| publishers | 4931 |

## Notes

- No anomalies detected.
</file>

<file path="docs/DEPLOY_SNAPSHOT.md">
# Deploy Snapshot (2025-11-06 13:41:45 ET)

## Image

- Latest image URI: us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc-worker:ae64157

## Runner Service

- Status: NOT FOUND or ERROR
- Error: Command failed: gcloud run services describe ncc-jobs-runner --region=us-central1 --project=newsletter-control-center --format=json
ERROR: (gcloud.run.services.describe) There was a problem refreshing your current auth tokens: Reauthentication failed. cannot prompt during non-interactive execution.
Please run:

  $ gcloud auth login

to obtain new credentials.

If you have already logged in with a different account, run:

  $ gcloud config set account ACCOUNT

to select an already authenticated account to use.


## Cloud Run Jobs

| Job | Last Status | Last Started | Last Completed |
|-----|-------------|--------------|----------------|
| ncc-chunks | N/A | N/A | N/A |
| ncc-embeddings | N/A | N/A | N/A |
| ncc-smoke | N/A | N/A | N/A |
| ncc-ingest-me | N/A | N/A | N/A |
| ncc-ingest-other | N/A | N/A | N/A |

## Cloud Scheduler

| Job | Cron | Time Zone | Target | Next Run |
|-----|------|-----------|--------|----------|
| schedule-ncc-chunks | NOT FOUND | - | - | - |
| schedule-ncc-embeddings | NOT FOUND | - | - | - |
| schedule-ncc-smoke | NOT FOUND | - | - | - |
| schedule-ncc-ingest-me-0710 | NOT FOUND | - | - | - |
| schedule-ncc-ingest-me-1210 | NOT FOUND | - | - | - |
| schedule-ncc-ingest-me-1710 | NOT FOUND | - | - | - |
| schedule-ncc-ingest-other-0710 | NOT FOUND | - | - | - |
| schedule-ncc-ingest-other-1210 | NOT FOUND | - | - | - |
| schedule-ncc-ingest-other-1710 | NOT FOUND | - | - | - |

## Reconcile

```
> newsletter-control-center@0.1.0 report:reconcile
> ts-node scripts/report-reconcile.ts

---
RECONCILIATION REPORT (PROD)

Window: last_24h
raw_emails: 51
emails_chunked: 51 (100%)
chunks: 686
chunks_embedded: 686 (100%)

Window: all_time
raw_emails: 74662
emails_chunked: 74162 (99.3%)
chunks: 1006134
chunks_embedded: 1006434 (100%)
---
```

## How to Resume

1. `npm run cloud:build:stream`
2. `npm run cloud:runner:apply`
3. `npm run cloud:jobs:apply`
4. `npm run cloud:schedule:apply`
5. `npm run report:reconcile`
</file>

<file path="docs/INGEST_AUDIT_SUMMARY.md">
# Gmail Ingest Code Audit Summary

**Date:** 2025-11-05  
**Scope:** Audit Gmail client and ingest code paths for headless Cloud Run deployment

---

## Files Audited

1. `src/gmail/client.ts` - Primary Gmail authentication client
2. `scripts/ingest-gmail.ts` - Main ingest script
3. `src/lib/parseMessage.ts` - Message parsing utilities
4. `src/lib/gmail.ts` - Alternative Gmail client (not used by ingest)
5. `package.json` - Dependencies

---

## Libraries Used for Auth

### Primary Dependencies
1. **`googleapis`** (v131.0.0)
   - Provides `google.gmail()` API client
   - Provides `google.auth.fromJSON()` for loading saved credentials
   - Provides `OAuth2Client` class

2. **`@google-cloud/local-auth`** (v3.0.1)
   - Provides `authenticate()` function for interactive OAuth flow
   - Used only for initial token acquisition (interactive browser flow)
   - Not needed for headless operation once tokens exist

3. **`google-auth-library`** (v10.4.2)
   - Underlying auth library (used via googleapis)
   - Handles OAuth2 token refresh automatically

### Code Flow
```
ingest-gmail.ts
  └─> getGmail() from src/gmail/client.ts
      ├─> loadSavedCredentials() → google.auth.fromJSON() [HEADLESS PATH]
      └─> authenticate() from @google-cloud/local-auth [INTERACTIVE PATH, only if no token]
```

---

## OAuth Scopes

### Read-Only Mode (Default)
- **Scope:** `https://www.googleapis.com/auth/gmail.readonly`
- **Triggered by:** `GMAIL_READONLY=true` (default) or not set
- **Capabilities:** Read emails, list messages, get metadata
- **Cannot:** Modify labels, mark as read, delete messages

### Modify Mode
- **Scope:** `https://www.googleapis.com/auth/gmail.modify`
- **Triggered by:** `GMAIL_READONLY=false`
- **Capabilities:** All read-only + apply labels, mark as read, modify messages
- **Used for:** Applying "Ingested" label, marking messages as read

**Code Location:** `src/gmail/client.ts` lines 87-93

---

## Token Storage

### Local File Paths

**Credentials (OAuth Client):**
- **Path:** `./credentials.json` (relative to project root)
- **Format:** OAuth 2.0 client credentials JSON
- **Contains:** `client_id`, `client_secret`, `redirect_uris`
- **Source:** Google Cloud Console → APIs & Services → Credentials

**Tokens (User Auth):**
- **Path:** `.tokens/token.me.json` (for 'me' inbox)
- **Path:** `.tokens/token.other.json` (for 'other' inbox)
- **Format:** JSON with `authorized_user` payload
- **Contains:** `type`, `client_id`, `client_secret`, `refresh_token`

**Code Locations:**
- Token directory: `src/gmail/client.ts` line 11: `const TOKEN_DIR = path.resolve('.tokens')`
- Token path: `src/gmail/client.ts` line 13: `TOKEN_PATH(inbox) => path.join(TOKEN_DIR, 'token.${inbox}.json')`
- Credentials path: `src/gmail/client.ts` line 15: `const CREDENTIALS_PATH = path.resolve('credentials.json')`

### Token Format

**Saved Token Structure** (from `saveCredentials()` lines 55-69):
```json
{
  "type": "authorized_user",
  "client_id": "xxx.apps.googleusercontent.com",
  "client_secret": "GOCSPX-xxx",
  "refresh_token": "1//xxx"
}
```

**Loading Token** (from `loadSavedCredentials()` lines 27-42):
```typescript
const content = await fs.readFile(TOKEN_PATH(inbox), 'utf8');
const creds = JSON.parse(content);
return google.auth.fromJSON(creds); // Creates OAuth2Client with refresh token
```

---

## Headless Operation Support

### Current Capability: ✅ YES (with existing tokens)

**Headless Path:**
1. `getGmail()` is called (line 81)
2. `loadSavedCredentials()` reads token file (lines 27-42)
3. If token exists, `google.auth.fromJSON()` creates OAuth2Client (line 35)
4. OAuth2Client uses refresh_token to auto-refresh access tokens (no user interaction)
5. Gmail API calls proceed normally

**Interactive Path (only if no token):**
1. `getGmail()` is called
2. `loadSavedCredentials()` fails (no token file)
3. `authenticate()` from `@google-cloud/local-auth` is called (lines 145-151)
4. Opens browser for OAuth consent (requires user interaction)
5. Token is saved after successful auth (line 169)

### Key Finding

**The code IS already headless-compatible** once tokens exist. The `loadSavedCredentials()` → `google.auth.fromJSON()` path requires no user interaction and works in Cloud Run.

**Requirements for Headless Operation:**
- ✅ Token files must exist at expected paths
- ✅ Token files must contain valid refresh tokens
- ✅ Credentials file (`credentials.json`) must exist (contains client_id/client_secret)
- ✅ Code must be able to read files (file system access in Cloud Run)

**For Cloud Run:**
- Store tokens and credentials in Secret Manager
- Mount secrets as files at `/secrets/gmail/`
- Set env vars: `GMAIL_CREDENTIALS_PATH=/secrets/gmail/credentials.json`, `GMAIL_TOKEN_DIR=/secrets/gmail/tokens`
- Code will read from mounted paths (no code changes needed beyond env var support)

---

## Idempotency Confirmation

### Email Deduplication
**Location:** `scripts/ingest-gmail.ts` lines 149-168

**Mechanism:**
1. Query BigQuery for existing `gmail_message_id` values
2. Filter out IDs that already exist
3. Only insert new messages

**Code:**
```typescript
const existingQuery = `
  SELECT gmail_message_id
  FROM \`${projectId}.${dataset}.raw_emails\`
  WHERE gmail_message_id IN UNNEST(@messageIds)
`;
const [existingRows] = await rawEmailsTable.bigQuery.query({...});
existingIds = new Set(existingRows.map((row: any) => row.gmail_message_id));
const newIds = messageIds.filter(id => !existingIds.has(id));
```

### Label Deduplication
**Location:** `scripts/ingest-gmail.ts` lines 361-397

**Mechanism:**
1. Query BigQuery for existing `(gmail_message_id, label_name)` pairs
2. Filter out pairs that already exist
3. Only insert new label pairs

**Code:**
```typescript
const existingLabelsQuery = `
  SELECT gmail_message_id, label_name
  FROM \`${projectId}.${dataset}.email_labels\`
  WHERE gmail_message_id IN UNNEST(@gmailIds)
`;
// ... query and build existingLabelPairs Set
// Filter before insert
if (!existingLabelPairs.has(pairKey)) {
  // Insert
}
```

**Conclusion:** ✅ Idempotency is already implemented. No reimplementation needed.

---

## Code Paths Used by Ingest

### Main Flow
1. **Entry:** `scripts/ingest-gmail.ts` → `main()`
2. **Auth:** `getGmail(config.inbox)` → `src/gmail/client.ts` → `getGmail()`
3. **Labels:** `gmail.users.labels.list()` → Get label map
4. **Messages:** `gmail.users.messages.list()` → Get message IDs
5. **Dedupe:** Query BigQuery for existing IDs
6. **Fetch:** `gmail.users.messages.get()` → Get full message data
7. **Parse:** `extractPlaintext()`, `getHeader()`, `extractHtmlContent()` → Extract fields
8. **Insert:** `rawEmailsTable.insert()` → Insert to BigQuery
9. **Labels:** `emailLabelsTable.insert()` → Insert labels (with deduplication)
10. **Modify:** `gmail.users.messages.modify()` → Apply labels/mark as read (if not readonly)

### Dependencies
- `src/lib/parseMessage.ts` - `extractPlaintext()`, `getHeader()`, `htmlToText()`
- `src/lib/gmail.ts` - `extractEmailAddress()` (not the alternative `getGmail()` function)

---

## Summary

✅ **Headless Operation:** Fully supported once tokens exist  
✅ **Idempotency:** Already implemented (emails and labels)  
✅ **Token Format:** Compatible with headless operation  
✅ **Code Changes:** Only need env var support for configurable paths (already planned)  
✅ **No Blockers:** Ready for Cloud Run deployment with Secret Manager

**Recommendation:** Proceed with Option 1 (headless OAuth with refresh tokens). See `docs/INGEST_OAUTH_DECISION.md` for detailed comparison.
</file>

<file path="docs/INGEST_CLOUD_PLAN.md">
# Gmail Ingest Cloud Deployment Plan

**Date:** 2025-11-05  
**Goal:** Set up Gmail ingest in Cloud Run for both inboxes (me, other) with 3x/day schedule (ET) using Secret Manager for OAuth artifacts.

---

## Files to Create/Modify

### 1. New Files
- `scripts/cloud/deploy-ingest.ts` - Deploy script for ingest Cloud Run jobs and scheduler (plan/apply pattern)
- `scripts/report-ingest-health.ts` - Health report script for ingest metrics (last 24h)

### 2. Modified Files
- `scripts/ingest-gmail.ts` - Add support for configurable `GMAIL_CREDENTIALS_PATH` and `GMAIL_TOKEN_DIR` env vars
- `src/gmail/client.ts` - Add support for configurable paths via env vars (defaults to current behavior)
- `package.json` - Add npm scripts:
  - `cloud:ingest:plan` → `ts-node scripts/cloud/deploy-ingest.ts`
  - `cloud:ingest:apply` → `ts-node scripts/cloud/deploy-ingest.ts --apply`
  - `report:ingest-health` → `ts-node scripts/report-ingest-health.ts`
- `scripts/cloud/snapshot.ts` - Add ingest jobs to snapshot output

---

## Cloud Run Jobs Configuration

### Job 1: `ncc-ingest-me`
- **Image:** Same as other jobs (from `docs/LATEST_IMAGE.txt` or Artifact Registry)
- **Region:** `us-central1`
- **Service Account:** `newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com`
- **Command:** `node`
- **Args:** `["dist/scripts/ingest-gmail.js", "--inbox", "me", "--limit", "500", "--no-dry-run"]`
- **Environment Variables:**
  - `BQ_PROJECT_ID` (from env)
  - `BQ_DATASET=ncc_production`
  - `BQ_LOCATION=US`
  - `GMAIL_READONLY=false`
  - `GMAIL_PROCESSED_LABEL=Ingested`
  - `GMAIL_PAID_LABEL=Paid $`
  - `GMAIL_MARK_READ=true`
  - `GMAIL_QUERY` (from env, typically `is:unread -label:Ingested`)
  - `GMAIL_CREDENTIALS_PATH=/secrets/gmail/credentials.json`
  - `GMAIL_TOKEN_DIR=/secrets/gmail/tokens`
- **Secret Mounts:**
  - `gmail-credentials-json` → `/secrets/gmail/credentials.json` (read-only)
  - `gmail-token-me-json` → `/secrets/gmail/tokens/token.me.json` (read-only)

### Job 2: `ncc-ingest-other`
- **Image:** Same as other jobs
- **Region:** `us-central1`
- **Service Account:** `newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com`
- **Command:** `node`
- **Args:** `["dist/scripts/ingest-gmail.js", "--inbox", "other", "--limit", "500", "--no-dry-run"]`
- **Environment Variables:** Same as `ncc-ingest-me`
- **Secret Mounts:**
  - `gmail-credentials-json` → `/secrets/gmail/credentials.json` (read-only)
  - `gmail-token-other-json` → `/secrets/gmail/tokens/token.other.json` (read-only)

**Note:** Secret Manager volumes are mounted read-only at `/secrets/gmail/` with paths:
- Credentials: `/secrets/gmail/credentials.json`
- Token (me): `/secrets/gmail/tokens/token.me.json`
- Token (other): `/secrets/gmail/tokens/token.other.json`

---

## Cloud Scheduler Configuration

### Schedule 1: `schedule-ncc-ingest-me-0710`
- **Cron:** `10 12 * * *` (07:10 ET = 12:10 UTC, accounting for DST)
- **Time Zone:** `America/New_York` (handles DST automatically)
- **Target:** Cloud Run Job `ncc-ingest-me`
- **Description:** Daily at 07:10 ET

### Schedule 2: `schedule-ncc-ingest-me-1210`
- **Cron:** `10 17 * * *` (12:10 ET = 17:10 UTC, accounting for DST)
- **Time Zone:** `America/New_York`
- **Target:** Cloud Run Job `ncc-ingest-me`
- **Description:** Daily at 12:10 ET

### Schedule 3: `schedule-ncc-ingest-me-1710`
- **Cron:** `10 22 * * *` (17:10 ET = 22:10 UTC, accounting for DST)
- **Time Zone:** `America/New_York`
- **Target:** Cloud Run Job `ncc-ingest-me`
- **Description:** Daily at 17:10 ET

### Schedule 4: `schedule-ncc-ingest-other-0710`
- **Cron:** `10 12 * * *` (07:10 ET = 12:10 UTC)
- **Time Zone:** `America/New_York`
- **Target:** Cloud Run Job `ncc-ingest-other`
- **Description:** Daily at 07:10 ET

### Schedule 5: `schedule-ncc-ingest-other-1210`
- **Cron:** `10 17 * * *` (12:10 ET = 17:10 UTC)
- **Time Zone:** `America/New_York`
- **Target:** Cloud Run Job `ncc-ingest-other`
- **Description:** Daily at 12:10 ET

### Schedule 6: `schedule-ncc-ingest-other-1710`
- **Cron:** `10 22 * * *` (17:10 ET = 22:10 UTC)
- **Time Zone:** `America/New_York`
- **Target:** Cloud Run Job `ncc-ingest-other`
- **Description:** Daily at 17:10 ET

**Note:** Scheduler uses Cloud Run Jobs API (not runner service) to invoke jobs directly.

---

## Secret Manager Secrets

### Required Secrets
1. **`gmail-credentials-json`** - OAuth client credentials (JSON)
2. **`gmail-token-me-json`** - OAuth token for 'me' inbox (JSON)
3. **`gmail-token-other-json`** - OAuth token for 'other' inbox (JSON)

### Secret Mounting
- Secrets mounted as volumes at `/secrets/gmail/` (read-only)
- Path structure:
  - `/secrets/gmail/credentials.json` (from `gmail-credentials-json`)
  - `/secrets/gmail/tokens/token.me.json` (from `gmail-token-me-json`)
  - `/secrets/gmail/tokens/token.other.json` (from `gmail-token-other-json`)

### IAM Permissions
- Service account `newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com` needs:
  - `roles/secretmanager.secretAccessor` on all three secrets

---

## Code Changes Summary

### `src/gmail/client.ts`
- Replace hardcoded `TOKEN_DIR` and `CREDENTIALS_PATH` with env-based values:
  - `GMAIL_CREDENTIALS_PATH` (default: `./credentials.json`)
  - `GMAIL_TOKEN_DIR` (default: `.tokens/`)
- Ensure `ensureTokenDir()` uses `GMAIL_TOKEN_DIR` if set
- Update `TOKEN_PATH()` to use `GMAIL_TOKEN_DIR`

### `scripts/ingest-gmail.ts`
- No code-path changes (already idempotent)
- Script will use env vars passed from Cloud Run (no changes needed)

---

## Health Report Script

### `scripts/report-ingest-health.ts`
Prints for last 24h:
- `raw_emails` count
- Emails with at least one chunk (JOIN to chunks)
- Rows in `email_labels` with `label_name='Ingested'`
- % marked read vs unread (from BigQuery only, no live Gmail calls)

Uses same pattern as `scripts/report-reconcile.ts`.

---

## Deployment Script Pattern

### `scripts/cloud/deploy-ingest.ts`
Follows same pattern as `deploy-jobs.ts` and `schedule-jobs.ts`:
- `--apply` flag for plan/apply mode
- Preview mode shows gcloud commands
- Apply mode creates/updates jobs and schedules
- Handles human auth switching (if needed)
- Idempotent (create or update)

**Key differences:**
- Uses `--update-secrets` to mount Secret Manager secrets
- Uses Cloud Run Jobs API directly (not runner service)
- Scheduler invokes jobs via `gcloud run jobs execute` pattern (or direct API)

---

## Acceptance Test Sequence

1. **Plan Preview:**
   ```bash
   npm run cloud:ingest:plan
   ```
   - Shows two Cloud Run jobs (`ncc-ingest-me`, `ncc-ingest-other`)
   - Shows six scheduler triggers (3 per inbox at 07:10, 12:10, 17:10 ET)
   - Shows env vars and args as specified

2. **Apply Deployment:**
   ```bash
   npm run cloud:ingest:apply
   ```
   - Creates/updates jobs and schedules
   - Running again is a no-op (idempotent)

3. **Manual Job Execution:**
   ```bash
   # Test 'me' inbox
   gcloud run jobs execute ncc-ingest-me --region=us-central1 --project=newsletter-control-center
   
   # Test 'other' inbox
   gcloud run jobs execute ncc-ingest-other --region=us-central1 --project=newsletter-control-center
   ```
   - If secrets/IAM missing, provide exact remedial one-liners

4. **Health Check:**
   ```bash
   npm run report:ingest-health
   ```
   - Prints non-zero counts
   - Verifies data flow

5. **Smoke Test:**
   ```bash
   npm run smoke
   npm run report:reconcile
   ```
   - Both still pass (no regressions)

6. **Snapshot Verification:**
   ```bash
   npm run cloud:snapshot
   ```
   - Shows two ingest jobs and six scheduler entries

---

## Human Commands (One-liners)

### Create Secrets (if not exists)
```bash
# Create credentials secret (replace <path> with actual path)
gcloud secrets create gmail-credentials-json --data-file=<path>/credentials.json --project=newsletter-control-center

# Create token secrets (replace <path> with actual paths)
gcloud secrets create gmail-token-me-json --data-file=<path>/token.me.json --project=newsletter-control-center
gcloud secrets create gmail-token-other-json --data-file=<path>/token.other.json --project=newsletter-control-center
```

### Grant Secret Accessor Role
```bash
# Grant access to service account
gcloud secrets add-iam-policy-binding gmail-credentials-json --member=serviceAccount:newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com --role=roles/secretmanager.secretAccessor --project=newsletter-control-center

gcloud secrets add-iam-policy-binding gmail-token-me-json --member=serviceAccount:newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com --role=roles/secretmanager.secretAccessor --project=newsletter-control-center

gcloud secrets add-iam-policy-binding gmail-token-other-json --member=serviceAccount:newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com --role=roles/secretmanager.secretAccessor --project=newsletter-control-center
```

---

## Idempotency Verification

### `ingest-gmail.ts` Already Idempotent
- Lines 149-168: Checks existing `gmail_message_id` before insert
- Lines 361-397: Checks existing `(gmail_message_id, label_name)` before insert
- **No reimplementation needed** - just ensure script is used as-is

---

## Guardrails Compliance

✅ **Do not modify** chunks/embeddings/smoke job definitions or schedules  
✅ **Do not change** runner or existing IAM (if IAM issues, output exact one-liners)  
✅ **No secrets in code** - Secret Manager only  
✅ **Region:** `us-central1`  
✅ **Dataset:** `ncc_production`  
✅ **Location:** `US`  
✅ **All changes idempotent** and re-runnable

---

## Next Steps

1. Review this plan
2. Implement changes (after approval)
3. Test locally with env overrides
4. Deploy to cloud
5. Verify acceptance tests
</file>

<file path="docs/INGEST_IMPLEMENTATION_SUMMARY.md">
# Gmail Ingest Cloud Implementation Summary

**Date:** 2025-11-05  
**Status:** Ready for review (pending "apply" command)

---

## What Changed

### 1. Token Provider (`src/gmail/token-provider.ts`) - NEW
- **Purpose:** Unified OAuth credentials provider that supports both cloud (env vars) and local (file-based) paths
- **Cloud path:** Reads `GMAIL_CLIENT_ID`, `GMAIL_CLIENT_SECRET`, `GMAIL_REFRESH_TOKEN_ME`, `GMAIL_REFRESH_TOKEN_OTHER` from environment variables
- **Local path:** Falls back to `credentials.json` and `.tokens/token.{me|other}.json` files
- **Returns:** `{client_id, client_secret, refresh_token}` or `null`

### 2. Gmail Client (`src/gmail/client.ts`) - MODIFIED
- **Change:** Added token provider integration
- **Flow:**
  1. Try token provider first (env vars or local files)
  2. If credentials found, construct `OAuth2Client` directly with refresh token (headless path)
  3. Fall back to existing local file-based token loading
  4. Last resort: interactive OAuth flow (local dev only)
- **Behavior:** Maintains backward compatibility with existing local development workflow
- **Scopes:** Still respects `GMAIL_READONLY` env var (readonly vs modify)

### 3. Ingest Script (`scripts/ingest-gmail.ts`) - MODIFIED
- **Change:** Added post-run reconcile summary logging
- **New Output:**
  ```
  RECONCILE SUMMARY:
    New emails ingested: <count>
    New labels applied: <count>
    Existing emails skipped: <count>
    Gmail labels applied: <count> (<count> already had label)
    Messages marked read: <count>
  ```
- **Idempotency:** Confirmed working (email and label deduplication already implemented)

### 4. Deploy Jobs Script (`scripts/cloud/deploy-jobs.ts`) - MODIFIED
- **Change:** Added ingest jobs configuration
- **New Jobs:**
  - `ncc-ingest-me`: Processes 'me' inbox with `--inbox me --limit 500 --no-dry-run`
  - `ncc-ingest-other`: Processes 'other' inbox with `--inbox other --limit 500 --no-dry-run`
- **Env Vars:**
  - `BQ_PROJECT_ID`, `BQ_DATASET=ncc_production`, `BQ_LOCATION=US`
  - `GMAIL_READONLY=false`, `GMAIL_PROCESSED_LABEL=Ingested`, `GMAIL_PAID_LABEL=Paid $`, `GMAIL_MARK_READ=true`, `GMAIL_QUERY=is:unread -label:Ingested`
- **Secrets:** Bound via `--set-secrets`:
  - `ncc-ingest-me`: `GMAIL_CLIENT_ID`, `GMAIL_CLIENT_SECRET`, `GMAIL_REFRESH_TOKEN_ME`
  - `ncc-ingest-other`: `GMAIL_CLIENT_ID`, `GMAIL_CLIENT_SECRET`, `GMAIL_REFRESH_TOKEN_OTHER`

### 5. Schedule Jobs Script (`scripts/cloud/schedule-jobs.ts`) - MODIFIED
- **Change:** Added 6 new scheduler jobs (3 per inbox)
- **Schedules:**
  - `schedule-ncc-ingest-me-0710`: Daily at 07:10 ET
  - `schedule-ncc-ingest-me-1210`: Daily at 12:10 ET
  - `schedule-ncc-ingest-me-1710`: Daily at 17:10 ET
  - `schedule-ncc-ingest-other-0710`: Daily at 07:10 ET
  - `schedule-ncc-ingest-other-1210`: Daily at 12:10 ET
  - `schedule-ncc-ingest-other-1710`: Daily at 17:10 ET
- **Pattern:** Uses existing runner service + OIDC auth pattern (same as chunks/embeddings)

### 6. Snapshot Script (`scripts/cloud/snapshot.ts`) - MODIFIED
- **Change:** Added ingest jobs and schedules to snapshot output
- **New Jobs in Snapshot:** `ncc-ingest-me`, `ncc-ingest-other`
- **New Schedules in Snapshot:** All 6 ingest scheduler jobs

### 7. Documentation

#### New Files:
- **`docs/SECRETS_GMAIL.md`** - Secret Manager setup and rotation instructions
- **`docs/INGEST_OAUTH_DECISION.md`** - OAuth approach decision (Option 1: headless with refresh tokens)
- **`docs/INGEST_AUDIT_SUMMARY.md`** - Code audit findings
- **`docs/INGEST_IMPLEMENTATION_SUMMARY.md`** - This file

#### Modified Files:
- **`docs/SCHEDULING_PLAN.md`** - Updated ingest section with cloud deployment details
- **`docs/INGEST_CLOUD_PLAN.md`** - Existing plan (now implemented)

---

## Environment Variables

### Cloud Run Jobs (via Secret Manager)
- `GMAIL_CLIENT_ID` - OAuth client ID
- `GMAIL_CLIENT_SECRET` - OAuth client secret
- `GMAIL_REFRESH_TOKEN_ME` - Refresh token for 'me' inbox
- `GMAIL_REFRESH_TOKEN_OTHER` - Refresh token for 'other' inbox

### Cloud Run Jobs (via env vars)
- `BQ_PROJECT_ID` - BigQuery project
- `BQ_DATASET=ncc_production` - BigQuery dataset
- `BQ_LOCATION=US` - BigQuery location
- `GMAIL_READONLY=false` - Enable label/mark-read modifications
- `GMAIL_PROCESSED_LABEL=Ingested` - Label to apply after ingestion
- `GMAIL_PAID_LABEL=Paid $` - Label to check for paid emails
- `GMAIL_MARK_READ=true` - Mark emails as read after ingestion
- `GMAIL_QUERY=is:unread -label:Ingested` - Gmail query for unread, unprocessed emails

---

## Deployment Steps (After "apply")

### Prerequisites
1. Create secrets in Secret Manager (see `docs/SECRETS_GMAIL.md`)
2. Grant service account access to secrets

### Deployment Sequence
```bash
# 1. Build and push image
npm run cloud:build:stream

# 2. Deploy jobs (includes ingest jobs)
npm run cloud:jobs:apply

# 3. Deploy schedules (includes ingest schedules)
npm run cloud:schedule:apply

# 4. Verify snapshot
npm run cloud:snapshot
```

### Manual Testing
```bash
# Test 'me' inbox job
gcloud run jobs execute ncc-ingest-me --region=us-central1 --project=newsletter-control-center

# Test 'other' inbox job
gcloud run jobs execute ncc-ingest-other --region=us-central1 --project=newsletter-control-center
```

---

## Idempotency Verification

✅ **Email Deduplication:** 
- Checks existing `gmail_message_id` in BigQuery before insert
- Skips emails that already exist
- Location: `scripts/ingest-gmail.ts` lines 149-168

✅ **Label Deduplication:**
- Checks existing `(gmail_message_id, label_name)` pairs before insert
- Skips labels that already exist
- Location: `scripts/ingest-gmail.ts` lines 361-397

✅ **Gmail Label Application:**
- Checks if label already exists before applying
- Only applies if missing
- Location: `scripts/ingest-gmail.ts` lines 464-473

---

## Security

- ✅ **No secrets in code** - All credentials via Secret Manager
- ✅ **Read-only secret access** - Service account has `secretmanager.secretAccessor` role only
- ✅ **Encrypted at rest** - Secrets encrypted by Google Cloud
- ✅ **Audit logging** - Secret access logged in Cloud Audit Logs
- ✅ **Token revocation** - Refresh tokens can be revoked by user

---

## Next Steps

1. **Review diffs** - Check all changes before applying
2. **Create secrets** - Follow `docs/SECRETS_GMAIL.md` to create Secret Manager secrets
3. **Grant access** - Run IAM policy binding commands for service account
4. **Deploy** - Run deployment sequence after saying "apply"
5. **Monitor** - Check Cloud Run logs for reconcile summaries
6. **Verify** - Run `npm run cloud:snapshot` to see ingest jobs and schedules

---

## Files Changed

### Created
- `src/gmail/token-provider.ts`
- `docs/SECRETS_GMAIL.md`
- `docs/INGEST_OAUTH_DECISION.md`
- `docs/INGEST_AUDIT_SUMMARY.md`
- `docs/INGEST_IMPLEMENTATION_SUMMARY.md`

### Modified
- `src/gmail/client.ts`
- `scripts/ingest-gmail.ts`
- `scripts/cloud/deploy-jobs.ts`
- `scripts/cloud/schedule-jobs.ts`
- `scripts/cloud/snapshot.ts`
- `docs/SCHEDULING_PLAN.md`

### Unchanged (No Regressions)
- `scripts/cloud/deploy-runner.ts` - Runner service unchanged
- Existing jobs (chunks, embeddings, smoke) - Unchanged
- Existing schedules - Unchanged
</file>

<file path="docs/INGEST_OAUTH_DECISION.md">
# Gmail OAuth Authentication Decision

**Date:** 2025-11-05  
**Purpose:** Determine the best OAuth approach for headless Cloud Run deployment of Gmail ingest jobs.

---

## Current Code Audit

### Libraries Used

1. **`googleapis`** (v131.0.0) - Gmail API client
2. **`@google-cloud/local-auth`** (v3.0.1) - Interactive OAuth flow helper
3. **`google-auth-library`** (v10.4.2) - OAuth2 client and token management (via googleapis)

### Code Paths

#### Primary Client: `src/gmail/client.ts`
- **Used by:** `scripts/ingest-gmail.ts` (line 4)
- **Auth Flow:**
  1. Tries to load saved token from `.tokens/token.{me|other}.json`
  2. If token exists, uses `google.auth.fromJSON()` to create auth client
  3. If no token, calls `authenticate()` from `@google-cloud/local-auth` (interactive browser flow)
  4. Saves token after successful auth

#### Alternative Client: `src/lib/gmail.ts`
- **Used by:** Some legacy scripts (not used by ingest-gmail.ts)
- **Auth Flow:** Direct OAuth2Client with refresh token from env vars (fully headless)
- **Status:** Not used by current ingest pipeline

### Scopes Requested

**Read-only mode** (`GMAIL_READONLY=true` or default):
- `https://www.googleapis.com/auth/gmail.readonly`

**Modify mode** (`GMAIL_READONLY=false`):
- `https://www.googleapis.com/auth/gmail.modify`

### Token Storage

**Local Storage:**
- **Credentials:** `./credentials.json` (OAuth client credentials)
- **Tokens:** 
  - `.tokens/token.me.json` (for 'me' inbox)
  - `.tokens/token.other.json` (for 'other' inbox)

**Token Format** (from `saveCredentials()` lines 55-69):
```json
{
  "type": "authorized_user",
  "client_id": "...",
  "client_secret": "...",
  "refresh_token": "..."
}
```

This format is compatible with `google.auth.fromJSON()` which creates an OAuth2Client that auto-refreshes access tokens using the refresh_token.

### Headless Capability Assessment

**Current State: PARTIALLY HEADLESS**

The code **CAN** run headless if:
1. Token files (`.tokens/token.me.json`, `.tokens/token.other.json`) already exist
2. Token files contain valid refresh tokens
3. `loadSavedCredentials()` successfully loads the token (lines 27-42)
4. `google.auth.fromJSON()` creates an auth client that uses the refresh token

**The code CANNOT** run headless if:
1. Token files don't exist
2. Token files are missing or invalid
3. Refresh tokens have been revoked
4. First-time setup (requires interactive `authenticate()` call)

**Key Finding:** The token storage format (`authorized_user` with `refresh_token`) is **fully compatible with headless operation**. Once tokens are saved, the code path through `loadSavedCredentials()` → `google.auth.fromJSON()` is completely headless.

---

## OAuth Options Comparison

### Option 1: Headless OAuth with Existing Refresh Tokens (RECOMMENDED)

**How It Works:**
- Use existing human-consented refresh tokens stored in Secret Manager
- Tokens are loaded from mounted secrets at `/secrets/gmail/tokens/token.{me|other}.json`
- Code uses `google.auth.fromJSON()` to create OAuth2Client
- OAuth2Client auto-refreshes access tokens using refresh_token (no user interaction)

**Pros:**
- ✅ **Zero code changes** - Current code already supports this
- ✅ **Zero Workspace admin work** - No domain-wide delegation needed
- ✅ **User consent already granted** - Tokens represent existing user consent
- ✅ **Works with existing code path** - `loadSavedCredentials()` → `fromJSON()` is headless
- ✅ **Simple secret management** - Just store token JSON files in Secret Manager
- ✅ **No additional permissions** - Uses standard OAuth scopes
- ✅ **Revocable by user** - User can revoke access in Google Account settings

**Cons:**
- ⚠️ **Initial token setup required** - Must obtain refresh tokens via interactive flow once
- ⚠️ **Token expiration risk** - Refresh tokens can be revoked (rare, but possible)
- ⚠️ **Manual refresh if revoked** - Would need to re-run interactive flow if token invalidated

**Security Considerations:**
- Refresh tokens are long-lived but can be revoked
- Tokens stored in Secret Manager (encrypted at rest)
- Read-only secrets mounted at runtime (no write access)
- Service account has minimal permissions (only secret accessor role)

**Implementation:**
1. Store existing `.tokens/token.me.json` and `.tokens/token.other.json` in Secret Manager
2. Mount secrets at `/secrets/gmail/tokens/` in Cloud Run jobs
3. Set `GMAIL_TOKEN_DIR=/secrets/gmail/tokens` env var
4. Code loads tokens via `loadSavedCredentials()` (no changes needed)

**Code Changes Required:**
- ✅ **None** - Just add env var support for `GMAIL_TOKEN_DIR` (already planned)

---

### Option 2: Domain-Wide Delegation (Service Account Impersonation)

**How It Works:**
- Create a Google Cloud service account
- Workspace admin grants domain-wide delegation to service account
- Service account impersonates user accounts (e.g., user@example.com)
- Uses service account credentials instead of OAuth tokens

**Pros:**
- ✅ **No user tokens** - Uses service account key
- ✅ **Centralized control** - Workspace admin can revoke access
- ✅ **No token expiration** - Service account keys don't expire (unless rotated)

**Cons:**
- ❌ **Workspace admin required** - Must be a Google Workspace admin
- ❌ **Major code changes** - Would need to rewrite auth flow
- ❌ **Security risk** - Service account can impersonate any user (if misconfigured)
- ❌ **Complex setup** - Requires OAuth consent screen configuration + domain-wide delegation
- ❌ **Not compatible with current code** - Would need to replace `getGmail()` entirely
- ❌ **Workspace requirement** - Requires Google Workspace (not personal Gmail)

**Security Considerations:**
- Service account keys have broad permissions if delegated
- Domain-wide delegation grants access to all users in domain
- Requires careful scope limitation
- More complex IAM management

**Implementation:**
- Would require rewriting `src/gmail/client.ts` to use service account impersonation
- Would need to change `getGmail()` signature to accept user email
- Would need to configure OAuth consent screen for domain-wide delegation
- Would need Workspace admin to grant delegation

**Code Changes Required:**
- ❌ **Major rewrite** - Replace entire auth flow in `src/gmail/client.ts`
- ❌ **API changes** - Modify `getGmail()` to use service account
- ❌ **New dependencies** - May need different auth libraries

---

## Recommendation: Option 1 (Headless OAuth with Refresh Tokens)

**Rationale:**

1. **Zero Code Churn:** Current code already supports headless operation via `loadSavedCredentials()` → `google.auth.fromJSON()`. Only need to add env var support for configurable paths (already planned).

2. **No Workspace Admin Dependency:** Works with personal Gmail accounts or Workspace accounts without requiring admin privileges.

3. **Proven Pattern:** The token format (`authorized_user` with `refresh_token`) is the standard Google OAuth pattern for long-lived access. Google's own examples use this approach.

4. **Security:** Refresh tokens are revocable, auditable, and scoped. User maintains control.

5. **Simplicity:** Just store existing token files in Secret Manager. No complex IAM setup.

**Required Changes:**
- Add `GMAIL_TOKEN_DIR` env var support to `src/gmail/client.ts` (already in plan)
- Add `GMAIL_CREDENTIALS_PATH` env var support to `src/gmail/client.ts` (already in plan)
- Store token files in Secret Manager
- Mount secrets in Cloud Run jobs

**No Blockers Found:** The code path is fully compatible with headless operation once tokens exist.

---

## Token Refresh Token Validation

**Important:** Refresh tokens can be revoked in these scenarios:
1. User revokes access in Google Account settings
2. User changes password (if "Less secure app access" was required - but this is deprecated)
3. Token is compromised and user revokes
4. App is unpublished or OAuth consent screen changes significantly

**Mitigation:**
- Store refresh tokens securely in Secret Manager
- Monitor auth failures and alert on `invalid_grant` errors
- Have a process to re-run interactive OAuth flow if token is revoked (rare)

**Token Longevity:**
- Refresh tokens typically last indefinitely unless revoked
- Google recommends treating them as long-lived credentials
- No automatic expiration (unlike access tokens which expire in 1 hour)

---

## Next Steps

1. ✅ **Proceed with Option 1** (headless OAuth with refresh tokens)
2. ✅ **Implement env var support** for `GMAIL_TOKEN_DIR` and `GMAIL_CREDENTIALS_PATH`
3. ✅ **Store token files in Secret Manager** (one-time setup)
4. ✅ **Mount secrets in Cloud Run jobs** (via deploy script)
5. ✅ **Test headless operation** locally with env vars pointing to secrets

No blockers for Option 1. The code is already headless-compatible once tokens exist.
</file>

<file path="docs/LATEST_IMAGE.txt">
us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc-worker:ae64157
</file>

<file path="docs/MIGRATION_PLAYBOOK.md">
# MIGRATION PLAYBOOK: Legacy → Production Schema

**Date:** 2025-01-27  
**Goal:** Migrate 69K newsletters from `ncc_newsletters.*` to `ncc_production.*` with zero data loss and minimal downtime.

---

## 1. Scope and End State

**Current State:**
- Legacy: `ncc_newsletters.messages` (69K rows), `ncc_newsletters.chunks` (938K rows)
- Production: `ncc_production.raw_emails` (20 rows), `ncc_production.chunks` (237 rows)
- Search APIs query legacy dataset only

**End State:**
- Single dataset: `ncc_production.*` with all data
- Unified views (`v_all_*`) as temporary bridge during migration
- Search APIs query unified views, then switch to production-only
- Legacy dataset archived (not deleted) for 30 days

**Migration Objects:**
- `ncc_newsletters.messages` → `ncc_production.raw_emails`
- `ncc_newsletters.chunks` → `ncc_production.chunks`
- `ncc_newsletters.chunks.chunk_embedding` → `ncc_production.chunk_embeddings` (extract arrays)

---

## 2. Pilot Batch (1,000 Messages)

**Selection:** Oldest 1,000 messages by `sent_date` (highest risk if lost).

**Transform Steps:**
1. Query `ncc_newsletters.messages` LIMIT 1000 ORDER BY sent_date ASC
2. Map columns per Phase 0 spec:
   - `id` → `gmail_message_id`
   - `sender` → `from_email`
   - NULL for: `inbox`, `history_id`, `message_id_header`, `from_name`, `reply_to`, `content_hash`
   - Compute `content_hash` from `body_text || body_html` (SHA-256)
   - `received_date` → `ingested_at`
3. Check idempotency: `SELECT gmail_message_id FROM raw_emails WHERE gmail_message_id IN (...)`
4. Insert batch into `ncc_production.raw_emails`
5. For each message, migrate chunks:
   - Map `newsletter_id` → `gmail_message_id`
   - NULL for: `publisher_id`, `source_part`, `char_start`, `char_end`, `created_at`
   - Insert into `ncc_production.chunks` (check `(gmail_message_id, chunk_index)` idempotency)
6. Extract embeddings from `ncc_newsletters.chunks.chunk_embedding`:
   - If `chunk_embedding` IS NOT NULL, insert into `chunk_embeddings` with `model='text-embedding-004'`, `dim=ARRAY_LENGTH(chunk_embedding)`

**Validation Steps:**
- [ ] Row count: 1,000 emails inserted
- [ ] Chunk count matches legacy (query both datasets)
- [ ] Embedding count matches (legacy chunks with non-null `chunk_embedding`)
- [ ] No duplicate `gmail_message_id` in production
- [ ] No orphaned chunks (all chunks have parent email)
- [ ] No orphaned embeddings (all embeddings have parent chunk)
- [ ] Sample queries: verify 10 random `gmail_message_id` appear in both datasets

**Rollback Plan:**
- If validation fails: `DELETE FROM ncc_production.raw_emails WHERE gmail_message_id IN (SELECT ... FROM legacy WHERE ...)`
- Delete cascading chunks/embeddings via same `gmail_message_id` list
- Re-run pilot after fix

---

## 3. Batch Migration Loop

**Batch Size:** 5,000–10,000 emails per batch (adjust based on BigQuery quota limits).

**Loop Steps:**
1. **Select batch:** `SELECT id FROM ncc_newsletters.messages WHERE id NOT IN (SELECT gmail_message_id FROM ncc_production.raw_emails) ORDER BY sent_date ASC LIMIT 10000`
2. **Idempotency check:** Query production for existing `gmail_message_id` values → filter out
3. **Transform & insert:** Same mapping as pilot (raw_emails → chunks → embeddings)
4. **Logging:** Print `batch_num=<N>, emails=<count>, chunks=<count>, embeddings=<count>, duration=<ms>`
5. **Validation:** Run acceptance gates (see Section 5)
6. **Progress tracking:** Store last migrated `gmail_message_id` in control table

**Embeddings Backfill Pacing:**
- **Daily cap:** 100K embeddings/day (to avoid Vertex AI quota exhaustion)
- **Hourly cap:** 5K embeddings/hour (rate limiting)
- **Batch size:** 32 embeddings per API call (Vertex AI batch limit)
- **Schedule:** Run embeddings backfill as separate job after chunks migrated

**Pause Conditions:**
- BigQuery quota exceeded → wait 1 hour, retry
- Validation fails → stop, investigate, rollback batch
- Embedding API errors → pause, retry with exponential backoff

---

## 4. Cutover Plan

**Phase 1: Dual Read (Week 1–2)**
- Update search APIs to query unified views (`v_all_raw_emails`, `v_all_chunks`, `v_all_chunk_embeddings`)
- Deploy and monitor: search results should match legacy-only queries
- Validate: sample 100 queries, compare results

**Phase 2: Production Only (Week 3)**
- After all data migrated, update views to point to production-only:
  ```sql
  CREATE OR REPLACE VIEW v_all_raw_emails AS SELECT *, 'prod' AS source FROM raw_emails;
  ```
- Monitor for 48 hours
- If stable, remove legacy dataset reads from views entirely

**Phase 3: Archive (Week 4)**
- Export legacy dataset to Cloud Storage (backup)
- Drop legacy views (keep tables for 30 days)
- Final validation: run smoke tests on production-only

**Rollback Switch:**
- If issues found, revert views to include legacy: `CREATE OR REPLACE VIEW v_all_raw_emails AS ... UNION ALL ...` (revert to dual-read)
- Keep legacy tables for 30 days before deletion
- Rollback script: restore views from backup SQL file

---

## 5. Acceptance Gates

### Per Batch (5K–10K emails):
- [ ] Row count matches: `COUNT(*) FROM legacy WHERE id IN batch` = `COUNT(*) FROM prod WHERE gmail_message_id IN batch`
- [ ] No duplicates: `SELECT gmail_message_id, COUNT(*) FROM prod WHERE gmail_message_id IN batch GROUP BY gmail_message_id HAVING COUNT(*) > 1` → 0 rows
- [ ] Chunk coverage: Legacy chunks migrated for all emails in batch
- [ ] Embedding coverage: All legacy `chunk_embedding` arrays extracted (if present)
- [ ] Referential integrity: No orphaned chunks/embeddings
- [ ] Sample validation: 10 random `gmail_message_id` queried in both datasets → match

### Final Cutover:
- [ ] All 69K emails migrated (verify count: `SELECT COUNT(*) FROM v_all_raw_emails WHERE source='prod'` = 69K)
- [ ] All chunks migrated (verify count matches legacy)
- [ ] All embeddings extracted (verify count: `SELECT COUNT(*) FROM chunk_embeddings` matches legacy `chunk_embedding` non-null count)
- [ ] Search API tested: 100 sample queries return identical results on unified views vs legacy-only
- [ ] No errors in logs for 48 hours
- [ ] Performance: Query latency < 2x legacy-only queries

---

## 6. Risk Register

| Risk | Mitigation |
|------|------------|
| **Data loss during migration** | Idempotency checks, batch validation, rollback script ready. Keep legacy tables for 30 days. |
| **Duplicate rows if migration reruns** | All inserts check for existing `gmail_message_id` / `(gmail_message_id, chunk_index)` / `chunk_id` before insert. |
| **BigQuery quota exceeded** | Batch size 5K–10K, pause between batches, monitor quota usage. |
| **Vertex AI embedding quota exceeded** | Daily cap 100K, hourly cap 5K, separate job with retry logic. |
| **Search API downtime during cutover** | Dual-read phase (views query both), then gradual cutover. Rollback views if issues. |
| **Schema mismatch causes data corruption** | Phase 0 spec validation, column mapping documented, pilot batch validates transforms. |
| **Performance degradation with unified views** | Monitor query latency, use production-only views once stable. Partition/cluster production tables. |
| **Legacy data quality issues (NULLs, malformed)** | Transform logic handles NULLs, validation checks data quality per batch. |

---

**Estimated Timeline:** 2–3 weeks (1 week pilot + validation, 1 week batch migration, 1 week cutover + monitoring)

**Success Criteria:** All 69K emails searchable via unified views, then production-only, with zero data loss and < 5% performance degradation.
</file>

<file path="docs/PHASE0_SPEC.md">
# PHASE 0 — Target Schema & Keys (Authoritative Spec)

**Date:** 2025-01-27  
**Purpose:** Authoritative specification for canonical tables, keys, transforms, and idempotency rules. All future code must align to this spec.

---

## 1. Canonical Tables

### `ncc_production.raw_emails`

**DDL Source:** `scripts/setup-bigquery.ts` (lines 32-50)

**Columns:**
- `gmail_message_id` STRING — Gmail message ID (primary key at app level)
- `inbox` STRING — Source inbox: 'me' or 'other'
- `history_id` STRING — Gmail history ID for incremental sync
- `message_id_header` STRING — Message-ID header value
- `subject` STRING — Email subject
- `from_email` STRING — Extracted email address from From header
- `from_name` STRING — Extracted display name from From header
- `reply_to` STRING — Reply-To header value
- `list_id` STRING — List-Id header value
- `sent_date` TIMESTAMP — Email sent date (prefer Date header, fallback to internalDate)
- `body_html` STRING — Full HTML body content
- `body_text` STRING — Plain text body content
- `content_hash` STRING — SHA-256 hash of body content (for deduplication)
- `is_paid` BOOL — Whether email is marked as paid (from label matching)
- `ingested_at` TIMESTAMP — When email was ingested into BigQuery

**Partitioning:** `PARTITION BY DATE(ingested_at)`

**Clustering:** `CLUSTER BY inbox, gmail_message_id`

**Canonical Primary Key (app-level):** `gmail_message_id` (unique per message, regardless of inbox)

**Idempotency Rule:** Before insert, check for existing `gmail_message_id` in table. Skip if exists.

---

### `ncc_production.email_labels`

**DDL Source:** `scripts/setup-bigquery.ts` (lines 52-56)

**Columns:**
- `gmail_message_id` STRING — FK to `raw_emails.gmail_message_id`
- `label_id` STRING — Gmail label ID
- `label_name` STRING — Gmail label name (human-readable)

**Partitioning:** None

**Clustering:** None

**Canonical Primary Key (app-level):** `(gmail_message_id, label_name)` — composite key for idempotency

**Idempotency Rule:** Before insert, check for existing `(gmail_message_id, label_name)` pairs. Skip if exists.

**Note:** Multiple labels per message are stored as multiple rows.

---

### `ncc_production.chunks`

**DDL Source:** `scripts/setup-bigquery.ts` (lines 74-86)

**Columns:**
- `chunk_id` STRING — UUID v4 generated per chunk (unique identifier)
- `gmail_message_id` STRING — FK to `raw_emails.gmail_message_id`
- `publisher_id` STRING — FK to `publishers.publisher_id` (currently NULL in chunk-new.ts)
- `source_part` STRING — Source part identifier (currently NULL in chunk-new.ts)
- `char_start` INT64 — Character start position in source text (may be NULL if not found)
- `char_end` INT64 — Character end position in source text (may be NULL if not found)
- `chunk_index` INT64 — Sequential index within email (0-based)
- `chunk_text` STRING — The chunk content text
- `created_at` TIMESTAMP — When chunk was created

**Partitioning:** `PARTITION BY DATE(created_at)`

**Clustering:** `CLUSTER BY publisher_id, gmail_message_id`

**Canonical Primary Key (app-level):** `(gmail_message_id, chunk_index)` — composite key for idempotency

**Idempotency Rule:** Before insert, check for existing `(gmail_message_id, chunk_index)` pairs. Skip if exists.

**Note:** `chunk_id` is a UUID for uniqueness, but idempotency is enforced on `(gmail_message_id, chunk_index)` to prevent duplicate chunking of the same email.

---

### `ncc_production.chunk_embeddings`

**DDL Source:** `scripts/setup-bigquery.ts` (lines 88-95)

**Columns:**
- `chunk_id` STRING — FK to `chunks.chunk_id` (primary key at app level)
- `model` STRING — Embedding model name (default: 'text-embedding-004')
- `dim` INT64 — Embedding dimension (typically 768 for text-embedding-004)
- `embedding` ARRAY<FLOAT64> — The embedding vector
- `created_at` TIMESTAMP — When embedding was generated

**Partitioning:** None

**Clustering:** `CLUSTER BY chunk_id`

**Canonical Primary Key (app-level):** `chunk_id` (one-to-one with chunks)

**Idempotency Rule:** Before insert, check for existing `chunk_id` in table. Skip if exists.

---

## 2. Canonical Keys and Dedupe Rules

### Email Identity

**Primary Key:** `gmail_message_id` (STRING)

**Uniqueness:** One row per `gmail_message_id` in `raw_emails`, regardless of inbox source.

**Deduplication Logic:**
- Before inserting into `raw_emails`, query existing `gmail_message_id` values in the batch.
- Filter out any `gmail_message_id` that already exists.
- Only insert rows for new `gmail_message_id` values.

**Implementation:** `scripts/ingest-gmail.ts` (lines 149-168)

---

### Chunk Identity

**Primary Key:** `(gmail_message_id, chunk_index)` (composite)

**Uniqueness:** One chunk per `(gmail_message_id, chunk_index)` pair in `chunks`.

**Deduplication Logic:**
- Before inserting into `chunks`, query existing `(gmail_message_id, chunk_index)` pairs.
- Filter out any pairs that already exist.
- Only insert rows for new `(gmail_message_id, chunk_index)` pairs.

**Implementation:** `scripts/chunk-new.ts` (lines 139-160)

**Note:** `chunk_id` is a UUID generated per chunk, but deduplication is based on `(gmail_message_id, chunk_index)` to prevent re-chunking the same email.

---

### Label Identity

**Primary Key:** `(gmail_message_id, label_name)` (composite)

**Uniqueness:** One row per `(gmail_message_id, label_name)` pair in `email_labels`.

**Deduplication Logic:**
- Before inserting into `email_labels`, query existing `(gmail_message_id, label_name)` pairs.
- Filter out any pairs that already exist.
- Only insert rows for new `(gmail_message_id, label_name)` pairs.

**Implementation:** `scripts/ingest-gmail.ts` (lines 361-397)

**Note:** `label_id` is also stored but not used for deduplication (multiple labels can have the same name if they come from different inboxes, but we dedupe by name).

---

### Embedding Identity

**Primary Key:** `chunk_id` (STRING)

**Uniqueness:** One row per `chunk_id` in `chunk_embeddings` (one-to-one with chunks).

**Deduplication Logic:**
- Before inserting into `chunk_embeddings`, query existing `chunk_id` values.
- Filter out any `chunk_id` that already exists.
- Only insert rows for new `chunk_id` values.

**Implementation:** `scripts/embed-new-chunks.ts` (lines 149-174)

---

## 3. Canonical Transforms

### HTML→Text Normalization

**Function:** `htmlToText()` in `src/lib/parseMessage.ts` (lines 19-30)

**Rules:**
1. Remove `<script>` tags and content (case-insensitive)
2. Remove `<style>` tags and content (case-insensitive)
3. Remove all HTML tags (replace with space)
4. Decode HTML entities:
   - `&nbsp;` → space
   - `&amp;` → `&`
   - `&lt;` → `<`
   - `&gt;` → `>`
5. Collapse all whitespace sequences to single space
6. Trim leading/trailing whitespace

**Usage:** Applied when chunking HTML content (`scripts/chunk-new.ts` line 92).

---

### Content Extraction Priority

**Function:** `extractPlaintext()` in `src/lib/parseMessage.ts` (lines 34-84)

**Priority Order:**
1. **HTML content** (if `text/html` MIME type exists) → apply `htmlToText()`
2. **Plain text** (if `text/plain` MIME type exists)
3. **Gmail snippet** (fallback if no body parts found)

**Usage:** `scripts/ingest-gmail.ts` uses `extractPlaintext()` for `body_text` field.

---

### Chunk Sizing and Overlap

**Function:** `splitIntoChunks()` in `scripts/chunk-new.ts` (lines 177-211)

**Parameters:**
- `targetSize`: 800 characters (default)
- `overlap`: 100 characters (default)

**Rules:**
1. If text length ≤ `targetSize`, return single chunk.
2. Otherwise, split into chunks of `targetSize` characters with `overlap` character overlap.
3. Each chunk has `chunk_index` starting at 0, incrementing by 1.
4. `char_start` and `char_end` are calculated from original text position (may be NULL if not found).
5. Safety check: ensure `start` always advances (prevents infinite loops).

**Usage:** `scripts/chunk-new.ts` (line 101)

---

### sent_date Derivation Order

**Function:** `parseHeaderDate()` and date logic in `scripts/ingest-gmail.ts` (lines 267-300)

**Priority Order:**
1. **Date header** — Parse "Date" header string (remove UTC comments, parse as Date)
2. **internalDate** — Use Gmail `internalDate` (milliseconds since epoch) if header parsing fails
3. **NULL** — If both fail, set `sent_date` to NULL

**Implementation:**
```typescript
const dateHeaderString = getHeader(msg, 'Date');
const headerDate = parseHeaderDate(dateHeaderString);
const internalMs = Number(msg.internalDate);
const sentDateObj = headerDate ?? (Number.isFinite(internalMs) ? new Date(internalMs) : null);
const sentDate = sentDateObj ? sentDateObj.toISOString() : null;
```

**Usage:** `scripts/ingest-gmail.ts` (lines 295-300)

---

### Base64 Decoding

**Function:** `decodeBase64Url()` in `src/lib/parseMessage.ts` (lines 4-16)

**Rules:**
1. Try `base64url` encoding first (Gmail's native format)
2. Fallback to `base64` encoding if base64url fails
3. Return empty string if both fail

**Usage:** Used when extracting body content from Gmail message parts.

---

## 4. Non-Goals (What We're NOT Doing Yet)

- **Publisher canonicalization in chunks:** `publisher_id` is currently NULL in `chunk-new.ts` (TODO: link to `publishers` table)
- **Source part tracking:** `source_part` is currently NULL in `chunk-new.ts` (TODO: track if chunk came from HTML vs plain text)
- **Legacy data migration:** No migration from `ncc_newsletters.messages` to `ncc_production.raw_emails` yet
- **Search API integration:** Search APIs still query legacy `ncc_newsletters` dataset (not Phase 0 scope)
- **Automated scheduling:** No cron/launchd/Cloud Scheduler setup (all runs are manual)
- **Error recovery:** No automatic retry logic for failed chunks/embeddings
- **Content deduplication:** `content_hash` is stored but not used for cross-inbox deduplication yet
- **Gmail History API:** Currently using simple query-based ingestion (History API not implemented)

---

## 5. Acceptance Criteria

### DDL Alignment

- [ ] All DDLs in `scripts/setup-bigquery.ts` match this spec exactly (column names, types, partitioning, clustering)
- [ ] All scripts that insert data use column names that match DDL exactly
- [ ] No scripts invent columns not defined in DDL

### Key Alignment

- [ ] `ingest-gmail.ts` checks for existing `gmail_message_id` before insert
- [ ] `chunk-new.ts` checks for existing `(gmail_message_id, chunk_index)` before insert
- [ ] `embed-new-chunks.ts` checks for existing `chunk_id` before insert
- [ ] `ingest-gmail.ts` checks for existing `(gmail_message_id, label_name)` before insert

### Transform Alignment

- [ ] HTML content is normalized using `htmlToText()` function
- [ ] Chunks are sized at ~800 chars with ~100 char overlap
- [ ] `sent_date` uses Date header → internalDate → NULL priority
- [ ] Content extraction uses HTML → plain text → snippet priority

### Logging Alignment

- [ ] All scripts log the IDs they process (e.g., `gmail_message_id`, `chunk_id`)
- [ ] All scripts log counts: existing/skipped, inserted, errors
- [ ] All scripts log idempotency checks (e.g., "existing/skipped=<N>")

---

## 6. Quick Validation SQL

### Uniqueness Checks

```sql
-- Check for duplicate gmail_message_id in raw_emails
SELECT gmail_message_id, COUNT(*) as count
FROM `newsletter-control-center.ncc_production.raw_emails`
GROUP BY gmail_message_id
HAVING COUNT(*) > 1;

-- Check for duplicate (gmail_message_id, chunk_index) in chunks
SELECT gmail_message_id, chunk_index, COUNT(*) as count
FROM `newsletter-control-center.ncc_production.chunks`
GROUP BY gmail_message_id, chunk_index
HAVING COUNT(*) > 1;

-- Check for duplicate chunk_id in chunk_embeddings
SELECT chunk_id, COUNT(*) as count
FROM `newsletter-control-center.ncc_production.chunk_embeddings`
GROUP BY chunk_id
HAVING COUNT(*) > 1;
```

### Referential Integrity Checks

```sql
-- Check for orphaned chunks
SELECT ch.gmail_message_id, COUNT(*) as orphan_count
FROM `newsletter-control-center.ncc_production.chunks` ch
LEFT JOIN `newsletter-control-center.ncc_production.raw_emails` re
  ON ch.gmail_message_id = re.gmail_message_id
WHERE re.gmail_message_id IS NULL
GROUP BY ch.gmail_message_id;

-- Check for orphaned embeddings
SELECT ce.chunk_id, COUNT(*) as orphan_count
FROM `newsletter-control-center.ncc_production.chunk_embeddings` ce
LEFT JOIN `newsletter-control-center.ncc_production.chunks` ch
  ON ce.chunk_id = ch.chunk_id
WHERE ch.chunk_id IS NULL;
```

---

## 7. Mismatches (If Any)

**Status:** No mismatches found between DDL and code usage.

**Verified:**
- ✅ `raw_emails` columns match DDL exactly
- ✅ `email_labels` columns match DDL exactly
- ✅ `chunks` columns match DDL exactly (publisher_id and source_part are NULL as expected)
- ✅ `chunk_embeddings` columns match DDL exactly

**Notes:**
- `publisher_id` and `source_part` in `chunks` are currently NULL in `chunk-new.ts` — this is intentional (non-goal for Phase 0)
- `chunk_id` is generated as UUID v4, but idempotency is enforced on `(gmail_message_id, chunk_index)` composite key
</file>

<file path="docs/SCHEDULING_PLAN.md">
# SCHEDULING PLAN SUMMARY

**Schedule Configuration:**

## Ingest (Both Inboxes)
- **Schedule:** 07:10, 12:10, 17:10 ET (daily) - each inbox runs separately
- **Cloud Run Jobs:** `ncc-ingest-me`, `ncc-ingest-other`
- **Cloud Scheduler Jobs:**
  - `schedule-ncc-ingest-me-0710`, `schedule-ncc-ingest-me-1210`, `schedule-ncc-ingest-me-1710`
  - `schedule-ncc-ingest-other-0710`, `schedule-ncc-ingest-other-1210`, `schedule-ncc-ingest-other-1710`
- **Command:** `node dist/scripts/ingest-gmail.js --inbox {me|other} --limit 500 --no-dry-run`
- **Env:** `GMAIL_READONLY=false` (labels and mark-read enabled)
- **Secrets:** Gmail OAuth credentials via Secret Manager (GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET, GMAIL_REFRESH_TOKEN_ME, GMAIL_REFRESH_TOKEN_OTHER)
- **Notes:** Idempotent (skips existing emails and labels), processes up to 500 emails per run per inbox

## Chunking
- **Schedule:** Hourly at :20 (e.g., 00:20, 01:20, 02:20, ...)
- **Command:** `npm run process:chunks:run -- --limit 500`
- **Notes:** Idempotent (skips existing chunks), processes up to 500 emails per run

## Embeddings
- **Schedule:** Hourly at :35 (e.g., 00:35, 01:35, 02:35, ...)
- **Command:** `npm run process:embeddings:run -- --limit 1000`
- **Notes:** Idempotent (skips existing embeddings), processes up to 1000 chunks per run

## Smoke Test
- **Schedule:** Daily at 18:00 ET
- **Command:** `npm run smoke`
- **Notes:** Read-only health check, no parameters needed

---

## Logging

All scheduled job logs will be written to `scripts/schedule/logs/` directory:
- `scripts/schedule/logs/ingest-YYYY-MM-DD-HHMM.log`
- `scripts/schedule/logs/chunk-YYYY-MM-DD-HHMM.log`
- `scripts/schedule/logs/embeddings-YYYY-MM-DD-HHMM.log`
- `scripts/schedule/logs/smoke-YYYY-MM-DD.log`

Log rotation: Keep last 30 days, archive older logs.

---

## Implementation Notes

- Use macOS `launchd` plist files or Cloud Scheduler for automation
- All jobs should redirect stdout/stderr to log files
- Exit codes: 0 = success, non-zero = error (for monitoring/alerting)
- Week one: Start with `GMAIL_READONLY=true` to validate pipeline before enabling label application

---

## Pause/Resume

To pause all scheduled jobs:

1. Preview: `npm run cloud:schedule:disable:plan`
2. Apply: `npm run cloud:schedule:disable:apply`

To resume all scheduled jobs:

1. Preview: `npm run cloud:schedule:enable:plan`
2. Apply: `npm run cloud:schedule:enable:apply`

To enable/disable specific jobs, use the toggle script directly:

```bash
# Disable specific jobs
ts-node scripts/cloud/scheduler-toggle.ts --disable --jobs schedule-ncc-chunks schedule-ncc-smoke --apply

# Enable specific jobs
ts-node scripts/cloud/scheduler-toggle.ts --enable --jobs schedule-ncc-chunks schedule-ncc-smoke --apply
```

The toggle commands are idempotent and will not error if jobs are already in the desired state.
</file>

<file path="docs/SECRETS_GMAIL.md">
# Gmail OAuth Secrets Management

**Purpose:** Store Gmail OAuth credentials and refresh tokens in Google Secret Manager for Cloud Run deployment.

---

## Required Secrets

We need four secrets in Secret Manager:

1. **`GMAIL_CLIENT_ID`** - OAuth 2.0 client ID
2. **`GMAIL_CLIENT_SECRET`** - OAuth 2.0 client secret
3. **`GMAIL_REFRESH_TOKEN_ME`** - Refresh token for 'me' inbox
4. **`GMAIL_REFRESH_TOKEN_OTHER`** - Refresh token for 'other' inbox

---

## Initial Setup (One-Time)

### Step 1: Extract Values from Local Files

**Client ID and Secret:**
- Located in: `./credentials.json` (local project root)
- Extract: `installed.client_id` or `web.client_id` and `installed.client_secret` or `web.client_secret`

**Refresh Tokens:**
- Located in: `.tokens/token.me.json` and `.tokens/token.other.json`
- Extract: `refresh_token` field from each file

**Example token file format:**
```json
{
  "type": "authorized_user",
  "client_id": "xxx.apps.googleusercontent.com",
  "client_secret": "GOCSPX-xxx",
  "refresh_token": "1//xxx"
}
```

### Step 2: Create Secrets

**Create client ID secret:**
```bash
# Replace <client_id> with the actual value from credentials.json
echo -n "<client_id>" | gcloud secrets create GMAIL_CLIENT_ID \
  --data-file=- \
  --project=newsletter-control-center \
  --replication-policy="automatic"
```

**Create client secret:**
```bash
# Replace <client_secret> with the actual value from credentials.json
echo -n "<client_secret>" | gcloud secrets create GMAIL_CLIENT_SECRET \
  --data-file=- \
  --project=newsletter-control-center \
  --replication-policy="automatic"
```

**Create refresh token for 'me' inbox:**
```bash
# Replace <refresh_token_me> with the refresh_token from .tokens/token.me.json
echo -n "<refresh_token_me>" | gcloud secrets create GMAIL_REFRESH_TOKEN_ME \
  --data-file=- \
  --project=newsletter-control-center \
  --replication-policy="automatic"
```

**Create refresh token for 'other' inbox:**
```bash
# Replace <refresh_token_other> with the refresh_token from .tokens/token.other.json
echo -n "<refresh_token_other>" | gcloud secrets create GMAIL_REFRESH_TOKEN_OTHER \
  --data-file=- \
  --project=newsletter-control-center \
  --replication-policy="automatic"
```

### Step 3: Grant Access to Service Account

The Cloud Run jobs service account needs `secretmanager.secretAccessor` role:

```bash
SA="newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com"

gcloud secrets add-iam-policy-binding GMAIL_CLIENT_ID \
  --member="serviceAccount:${SA}" \
  --role="roles/secretmanager.secretAccessor" \
  --project=newsletter-control-center

gcloud secrets add-iam-policy-binding GMAIL_CLIENT_SECRET \
  --member="serviceAccount:${SA}" \
  --role="roles/secretmanager.secretAccessor" \
  --project=newsletter-control-center

gcloud secrets add-iam-policy-binding GMAIL_REFRESH_TOKEN_ME \
  --member="serviceAccount:${SA}" \
  --role="roles/secretmanager.secretAccessor" \
  --project=newsletter-control-center

gcloud secrets add-iam-policy-binding GMAIL_REFRESH_TOKEN_OTHER \
  --member="serviceAccount:${SA}" \
  --role="roles/secretmanager.secretAccessor" \
  --project=newsletter-control-center
```

---

## Updating Secrets (Rotation)

### Update Client ID

```bash
echo -n "<new_client_id>" | gcloud secrets versions add GMAIL_CLIENT_ID \
  --data-file=- \
  --project=newsletter-control-center
```

### Update Client Secret

```bash
echo -n "<new_client_secret>" | gcloud secrets versions add GMAIL_CLIENT_SECRET \
  --data-file=- \
  --project=newsletter-control-center
```

### Update Refresh Token (Me)

```bash
echo -n "<new_refresh_token_me>" | gcloud secrets versions add GMAIL_REFRESH_TOKEN_ME \
  --data-file=- \
  --project=newsletter-control-center
```

### Update Refresh Token (Other)

```bash
echo -n "<new_refresh_token_other>" | gcloud secrets versions add GMAIL_REFRESH_TOKEN_OTHER \
  --data-file=- \
  --project=newsletter-control-center
```

**Note:** Cloud Run jobs automatically use the `latest` version, so new versions take effect on the next job execution.

---

## Verifying Secrets

### List Secrets

```bash
gcloud secrets list --project=newsletter-control-center | grep GMAIL
```

### Check Secret Versions

```bash
gcloud secrets versions list GMAIL_CLIENT_ID --project=newsletter-control-center
```

### View Secret Metadata (not values)

```bash
gcloud secrets describe GMAIL_CLIENT_ID --project=newsletter-control-center
```

---

## Troubleshooting

### Secret Not Found

If Cloud Run job fails with "secret not found":
1. Verify secret exists: `gcloud secrets list | grep GMAIL_CLIENT_ID`
2. Verify service account has access: `gcloud secrets get-iam-policy GMAIL_CLIENT_ID --project=newsletter-control-center`

### Invalid Grant Error

If Gmail API returns `invalid_grant`:
- Refresh token may have been revoked
- Re-run interactive OAuth flow locally to get new token
- Update secret with new refresh token (see "Updating Secrets" above)

### Permission Denied

If service account cannot access secrets:
- Run the IAM policy binding commands in Step 3 above
- Verify service account email is correct: `newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com`

---

## Security Notes

- **Never commit secrets to git** - All secrets are stored in Secret Manager only
- **Secrets are encrypted at rest** - Google Cloud automatically encrypts secret values
- **Access is logged** - Secret access is logged in Cloud Audit Logs
- **Rotate regularly** - Refresh tokens should be rotated if compromised
- **Least privilege** - Service account only has `secretAccessor` role (read-only)

---

## Modify Scope Remint

If your refresh tokens were minted with `gmail.readonly` scope, you need to remint them with `gmail.modify` scope to enable label application and mark-as-read functionality.

### Quick Remint Flow

**Step A (mint for ME):**
1. Run: `npm run gmail:mint:me`
2. Open the URL shown in your browser
3. Choose the correct Gmail account (the one for 'me' inbox)
4. Approve access
5. Copy the code from the browser address bar (after `code=`)
6. Paste the code into the terminal
7. Copy the printed refresh token (labeled "ME REFRESH TOKEN:")
8. Run: `npm run gmail:secret:me -- --token="PASTE_TOKEN_HERE"`

**Step B (mint for OTHER):**
1. Run: `npm run gmail:mint:other`
2. Repeat steps 2-7 above (choose the 'other' inbox Gmail account)
3. Run: `npm run gmail:secret:other -- --token="PASTE_TOKEN_HERE"`

**Step C (verify):**
- Run: `npm run ingest:preflight -- --apply`
- Should pass modify capability check for both inboxes

**Step D (test with tiny live job):**
```bash
gcloud run jobs execute ncc-ingest-me --region=us-central1 \
  --project=newsletter-control-center \
  --args="--limit=3","--dry-run=false","--mark-read=false"
```

Confirm in logs:
- BigQuery writes happen (inserted X messages)
- Gmail applies the processed label (labeled=X)
- No 403 errors

**Step E (enable mark-read, optional):**
- When ready, ensure `GMAIL_MARK_READ=true` in job env (already set by default)
- Re-run preflight to confirm everything works

### Notes

- The mint scripts read OAuth credentials from Secret Manager (preferred) or `.env` (fallback)
- They request `gmail.modify` and `gmail.labels` scopes
- The update script adds a new version to Secret Manager (Cloud Run jobs automatically use latest)
- Preflight now includes a modify capability check that verifies tokens have the correct scope

---

## Related Documentation

- [Google Secret Manager Documentation](https://cloud.google.com/secret-manager/docs)
- [Cloud Run Secrets](https://cloud.google.com/run/docs/configuring/secrets)
</file>

<file path="docs/SESSION_SUMMARY_2025-11-05.md">
# Session Summary - November 5, 2025

## Major Accomplishments

### 1. Cloud Run Jobs Deployment
- ✅ Deployed 3 Cloud Run Jobs: `ncc-chunks`, `ncc-embeddings`, `ncc-smoke`
- ✅ Deployed runner service: `ncc-jobs-runner` (HTTP endpoint for scheduler triggers)
- ✅ Configured Cloud Scheduler with hourly/daily schedules
- ✅ All jobs operational and executing successfully

### 2. IAM & Permissions
- ✅ Created `scripts/cloud/ensure-iam.ts` for IAM guardrails
- ✅ Granted `roles/run.developer` (project-level) for Cloud Run Jobs execution
- ✅ Granted `roles/run.invoker` (service-level) for scheduler → runner invocations
- ✅ All IAM checks passing

### 3. Smoke Test Fixes
- ✅ Fixed SQL syntax error (replaced `FILTER (WHERE ...)` with BigQuery-compatible `COUNT(CASE ...)`)
- ✅ Added robust error logging with full SQL queries
- ✅ Added PASS summary output
- ✅ Smoke test now passes with 100% embedding coverage

### 4. Legacy Migration
- ✅ Created `scripts/migrate-legacy-to-prod.ts`
- ✅ Migrated 74,591 messages → `raw_emails`
- ✅ Migrated 939,556 chunks → `chunks`
- ✅ Migrated 939,556 embeddings → `chunk_embeddings`
- ✅ All legacy data successfully migrated to production

### 5. Embedding Pipeline Fixes
- ✅ Fixed "RangeError: Invalid string length" by implementing batch inserts
- ✅ Added `insertRowsSafe()` with automatic batch splitting
- ✅ Added `--insert-batch` CLI option (default 500)
- ✅ Processed 60,655 new embeddings in batches of 500 without errors
- ✅ Achieved 100% embedding coverage (1,005,748 embedded chunks)

### 6. Scheduler Management
- ✅ Created `scripts/cloud/scheduler-toggle.ts` for pause/resume
- ✅ Added npm scripts: `cloud:schedule:disable:apply`, `cloud:schedule:enable:apply`
- ✅ Successfully paused/resumed embeddings scheduler during bulk processing

### 7. Deployment Snapshot
- ✅ Created `scripts/cloud/snapshot.ts` for deployment state capture
- ✅ Snapshot includes: image URI, runner service, Cloud Run Jobs, Cloud Scheduler, reconcile report
- ✅ Next run times populated for all scheduler jobs

### 8. Code Organization
- ✅ Moved legacy scripts to `scripts/legacy/` folder:
  - `process-newsletters.ts`
  - `full-chunk-and-embed.ts`
  - `run-overnight-tranche1.sh`
  - `setup-service-account.sh`
- ✅ Updated `tsconfig.json` to exclude legacy folder

## Current System State

### Data Metrics
- **Raw Emails**: 74,611
- **Chunked Emails**: 74,111 (99.3%)
- **Total Chunks**: 1,005,448
- **Embedded Chunks**: 1,005,748 (100%)
- **Last 24h**: 21 emails, 100% chunked, 238 chunks, 100% embedded

### Cloud Infrastructure
- **Image**: `us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc-worker:ae64157`
- **Runner Service**: `ncc-jobs-runner` (active)
- **Cloud Run Jobs**: 3 jobs (all operational)
- **Cloud Scheduler**: 3 jobs (all active)

### Scheduler Schedule
- `schedule-ncc-chunks`: Hourly at :20 ET
- `schedule-ncc-embeddings`: Hourly at :35 ET
- `schedule-ncc-smoke`: Daily at 18:00 ET

## Key Files Created/Modified

### New Scripts
- `scripts/cloud/ensure-iam.ts` - IAM permissions checker/fixer
- `scripts/cloud/scheduler-toggle.ts` - Pause/resume scheduler jobs
- `scripts/cloud/snapshot.ts` - Deployment state snapshot
- `scripts/migrate-legacy-to-prod.ts` - Legacy data migration

### Modified Scripts
- `scripts/smoke.ts` - Fixed SQL syntax, added error logging
- `scripts/embed-new-chunks.ts` - Added batch inserts with auto-splitting
- `scripts/cloud/deploy-jobs.ts` - Fixed command building, added human auth switching
- `scripts/cloud/deploy-runner.ts` - Fixed command building, added human auth switching
- `scripts/cloud/schedule-jobs.ts` - Fixed cron for smoke test, added human auth switching
- `scripts/cloud/snapshot.ts` - Added next run time population

### Documentation
- `docs/DEPLOY_SNAPSHOT.md` - Current deployment state
- `docs/SCHEDULING_PLAN.md` - Updated with pause/resume steps
- `scripts/legacy/README.md` - Documentation for moved scripts

## NPM Scripts Added

```json
"cloud:iam:plan": "ts-node scripts/cloud/ensure-iam.ts",
"cloud:iam:apply": "ts-node scripts/cloud/ensure-iam.ts --apply",
"cloud:snapshot": "ts-node scripts/cloud/snapshot.ts",
"cloud:schedule:disable:plan": "ts-node scripts/cloud/scheduler-toggle.ts --disable --all",
"cloud:schedule:disable:apply": "ts-node scripts/cloud/scheduler-toggle.ts --disable --all --apply",
"cloud:schedule:enable:plan": "ts-node scripts/cloud/scheduler-toggle.ts --enable --all",
"cloud:schedule:enable:apply": "ts-node scripts/cloud/scheduler-toggle.ts --enable --all --apply",
"migrate:dry": "ts-node scripts/migrate-legacy-to-prod.ts",
"migrate:apply": "ts-node scripts/migrate-legacy-to-prod.ts --apply --limit 100000"
```

## Technical Improvements

1. **Batch Insert Safety**: Implemented recursive batch splitting to prevent JSON size limits
2. **Human Auth Switching**: All deployment scripts now automatically switch to human user for IAM operations
3. **Idempotency**: All operations are idempotent and safe to re-run
4. **Error Handling**: Enhanced error logging throughout with explicit error messages
5. **ADC Fallback**: Code already supports Application Default Credentials (no key file needed in Cloud)

## Next Steps (When Ready)

1. Gmail ingest Cloud Run Jobs (currently manual due to browser OAuth)
2. Monitor hourly scheduled jobs
3. Review and optimize batch sizes if needed
4. Consider adding more comprehensive monitoring/alerting

---

**Status**: All systems operational, 100% embedding coverage achieved, legacy migration complete.

**Last Updated**: 2025-11-05 03:21:43 ET
</file>

<file path="docs/STATUS_SNAPSHOT.md">
# Newsletter Control Center - Status Snapshot

## TL;DR

- **Gmail → BigQuery ingest**: Working, dual inbox support (me/other), idempotent inserts, READONLY default prevents label/mark-read
- **Chunking**: Working (latest run: 237 chunks from 20 emails via `pipeline:chunks:small`)
- **Embeddings**: Working (latest run: 50 embeddings inserted), Vertex AI region mapping fixed (US → us-central1)
- **Smoke test**: 20 emails ingested (last_24h=20, all_time=20), 100% chunk coverage (20 raw_ids → 20 chunked_ids)
- **Labels**: Safe/idempotent when READONLY=false (checks existing labels before applying; currently default true)
- **Auth**: Gmail via local OAuth tokens (.tokens/token.{me|other}.json); BigQuery via service account at secrets/gcp/ncc-local-dev.json

## Architecture

```
Gmail (me/other)
  → scripts/ingest-gmail.ts
  → BigQuery: ncc_production.raw_emails + email_labels
  → scripts/chunk-new.ts
  → ncc_production.chunks
  → scripts/embed-new-chunks.ts
  → ncc_production.chunk_embeddings
```

## Key Configs

- **BigQuery**: BQ_PROJECT_ID, BQ_DATASET, BQ_LOCATION, GOOGLE_APPLICATION_CREDENTIALS
- **Gmail**: GMAIL_QUERY, GMAIL_PROCESSED_LABEL, GMAIL_PAID_LABEL, GMAIL_MARK_READ, GMAIL_READONLY
- **Embeddings**: EMB_MODEL (default: text-embedding-004), EMB_LOCATION (maps BQ_LOCATION: US→us-central1), EMB_BATCH_SIZE (default: 32)

## Safeguards

- **READONLY default**: Gmail operations disabled by default (GMAIL_READONLY=true), prevents accidental label/mark-read
- **Idempotency**: BigQuery inserts check for existing rows; Gmail labeling checks existing labels before applying; embeddings skip existing chunk_id
- **Dry-run flags**: All chunk/embedding scripts support --dry-run (default true)

## Known Gaps / Parked Items

- **sent_date backfill**: Script added (backfill:sentdate:run) but not executed yet; current rows show N/A dates
- **Scheduling**: No launchd/cron setup yet; all runs are manual
- **Dashboards/metrics**: Not set up; monitoring via smoke test only

## "When We Resume" Checklist

- [ ] Flip GMAIL_READONLY=false only for tiny label test (ingest:label-test)
- [ ] Run backfill: `npm run backfill:sentdate:run` to fix NULL sent_date rows
- [ ] Run embeddings on backlog: `npm run process:embeddings:run` (current limit: 100)
- [ ] Optional: Add schedule (launchd/cron) for daily ingest → chunk → embed pipeline
</file>

<file path="docs/STRATEGIC_STATUS.md">
# Newsletter Control Center - COMPLETE STRATEGIC STATUS

**Date:** 2025-01-27  
**Purpose:** Understand what's ACTUALLY built and working at scale, not just test runs.

---

## 1. DATA SCALE REALITY CHECK

### Production Data (ncc_production - NEW PIPELINE)
- **Total emails in raw_emails**: 20 (from recent test runs)
- **Chunked**: 20 (100% coverage)
- **Embedded**: 50 chunks (partial coverage)
- **Date range**: Recent (last 24 hours)
- **Inboxes**: Both "me" and "other" tested, but only recent test data

### Legacy Data (ncc_newsletters - OLD PIPELINE)
- **Total newsletters**: ~69,673 (referenced in frontend)
- **Total chunks**: ~938,601 (referenced in frontend)
- **Dataset**: `ncc_newsletters.messages` (legacy schema)
- **Status**: This is what the search frontend currently queries

**CRITICAL DISCONNECT**: The new ingestion pipeline (`ncc_production`) and the search frontend (`ncc_newsletters`) are using DIFFERENT datasets. Search is working on old data, new pipeline is writing to new schema.

---

## 2. SEARCH CAPABILITIES - WHAT EXISTS?

### ✅ Semantic Search API (RAG-Powered)
**Location:** `newsletter-search/src/app/api/intelligence/query/route.ts`

**Features:**
- ✅ Vector similarity search on embeddings (cosine distance)
- ✅ Keyword/full-text search (LIKE queries)
- ✅ Hybrid search (combines vector + keyword with 70/30 weighting)
- ✅ Gemini 2.5 Pro integration for:
  - Fact extraction from chunks
  - Answer synthesis with citations
- ✅ Publisher relevance rankings
- ✅ Freshness bias (boosts recent content)
- ✅ Daily budget tracking ($10/day limit)
- ✅ Cost logging (tokens in/out, USD cost)

**Data Source:** `ncc_newsletters.messages` + `ncc_newsletters.chunks` (LEGACY)

### ⚠️ Legacy Keyword Search API
**Location:** `newsletter-search/src/app/api/search/route.ts`

**Features:**
- ✅ Full-text keyword search (body_text, subject, sender)
- ✅ Date range filtering
- ✅ Publisher filtering
- ✅ VIP-only filtering
- ✅ Pagination

**Data Source:** `ncc_newsletters.messages` (LEGACY)

### ❌ Placeholder APIs (Not Implemented)
- `src/api/intelligence.ts` - placeholder only
- `src/api/search.ts` - placeholder only

---

## 3. FRONTEND STATUS

### ✅ Next.js Web Application
**Location:** `newsletter-search/`

**Components:**
- ✅ **Search Interface** (`src/app/page.tsx`)
  - Semantic query input
  - Loading states
  - Error handling
  - Results display

- ✅ **AI Answer Display**
  - Formatted answer with citations
  - Inline citation format: "Publisher · Date · Subject"
  - Clickable citations linking to newsletter detail

- ✅ **Citations Section**
  - Lists all sources used
  - Links to newsletter detail page with chunk highlighting
  - Date formatting

- ✅ **Publisher Rankings**
  - Top 5 publishers by relevance
  - Relevance score percentage
  - Chunk count per publisher

- ✅ **Top Chunks Display**
  - Shows relevant newsletters with match scores
  - Links to full newsletter view

- ✅ **Newsletter Detail Page** (`src/app/newsletter/[id]/page.tsx`)
  - Full newsletter content display
  - Chunk highlighting support (via `?highlight_chunk=` query param)
  - Read time calculation
  - Word count
  - Publisher name, VIP badge
  - Date formatting
  - Clean HTML rendering

- ✅ **Newsletter API** (`src/app/api/newsletter/[id]/route.ts`)
  - Fetches full newsletter by ID
  - Returns all metadata

**What's Missing:**
- ❌ No admin interface
- ❌ No analytics dashboard
- ❌ No user authentication
- ❌ No search history
- ❌ No saved/bookmarked newsletters

---

## 4. WORKING END-TO-END FLOWS

### ✅ Flow 1: Semantic Search with RAG Answers
**User Journey:**
1. User enters query on homepage
2. Frontend calls `/api/intelligence/query`
3. API generates query embedding
4. API performs hybrid search (vector + keyword)
5. API extracts facts using Gemini 2.5 Pro
6. API synthesizes answer with citations
7. User sees:
   - AI-generated answer
   - Clickable citations (Publisher · Date · Subject)
   - Top publishers ranked by relevance
   - List of relevant newsletters with match scores

**Status:** ✅ FULLY WORKING (but queries OLD dataset)

### ✅ Flow 2: Newsletter Detail View
**User Journey:**
1. User clicks citation or newsletter link
2. Frontend navigates to `/newsletter/[id]`
3. API fetches full newsletter from `ncc_newsletters.messages`
4. User sees full newsletter content with metadata
5. If `?highlight_chunk=` param exists, specific chunk is highlighted

**Status:** ✅ FULLY WORKING (but queries OLD dataset)

### ⚠️ Flow 3: New Newsletter Ingestion
**User Journey:**
1. Run `npm run ingest:today` (ingests from both inboxes)
2. Emails written to `ncc_production.raw_emails`
3. Run `npm run process:chunks:run` (chunks emails)
4. Chunks written to `ncc_production.chunks`
5. Run `npm run process:embeddings:run` (generates embeddings)
6. Embeddings written to `ncc_production.chunk_embeddings`

**Status:** ✅ PIPELINE WORKING (but writes to NEW dataset, not searchable by frontend)

### ❌ Flow 4: Automatic Daily Ingestion
**Status:** ❌ NOT IMPLEMENTED
- No scheduling (launchd/cron)
- No Cloud Run jobs for automation
- All runs are manual

---

## 5. THE GAP TO "FINDING ANY NEWSLETTER IN 10 SECONDS"

### What's Preventing This Goal TODAY:

#### 🔴 CRITICAL BLOCKER #1: Dataset Mismatch
**Problem:** Search frontend queries `ncc_newsletters.messages` (legacy), but new pipeline writes to `ncc_production.raw_emails` (new).

**Impact:** 
- New emails ingested via `ingest-gmail.ts` are NOT searchable
- Only old data (69K newsletters) is searchable
- New pipeline is effectively invisible to users

**Fix Required:** 
- Update search APIs to query `ncc_production.*` tables
- OR migrate/merge legacy data into new schema
- OR run both pipelines in parallel

#### 🔴 CRITICAL BLOCKER #2: Scale Mismatch
**Problem:** New pipeline has only 20 emails vs 69K in legacy.

**Impact:**
- Even if dataset mismatch is fixed, new pipeline has minimal data
- Need to backfill historical emails into new pipeline OR migrate legacy data

#### 🟡 MEDIUM BLOCKER #3: No Automation
**Problem:** All ingestion is manual.

**Impact:**
- New newsletters don't appear automatically
- User must manually run scripts
- No real-time updates

**Fix Required:** 
- Schedule daily runs (launchd/cron or Cloud Scheduler)
- Or set up Cloud Run jobs with triggers

#### 🟡 MEDIUM BLOCKER #4: Embedding Coverage Gap
**Problem:** Only 50 embeddings generated vs 237 chunks created.

**Impact:**
- Vector search will have incomplete results
- Some chunks won't be findable via semantic search

**Fix Required:** 
- Run `process:embeddings:run` on full backlog
- Ensure embedding job runs after each chunking batch

#### 🟢 MINOR GAPS:
- No user authentication (currently public/widely accessible)
- No search filters (date range, publisher, etc.) in semantic search UI
- No search history or saved searches
- No analytics on search patterns

---

## SUMMARY: What Actually Works vs What Doesn't

### ✅ FULLY WORKING (but uses old data):
- Semantic search with RAG answers
- Newsletter detail pages
- Citation linking
- Publisher rankings
- Hybrid search (vector + keyword)

### ✅ WORKING (but not connected to search):
- New email ingestion pipeline
- Chunking pipeline
- Embedding generation pipeline
- Dual inbox support

### ❌ NOT WORKING:
- Automatic daily ingestion
- Search of newly ingested emails
- Complete embedding coverage
- Admin interface
- Analytics dashboard

### 🎯 TO ACHIEVE "FIND ANY NEWSLETTER IN 10 SECONDS":
1. **Fix dataset mismatch** (connect new pipeline to search OR migrate legacy data)
2. **Scale up new pipeline** (backfill historical emails OR migrate 69K legacy newsletters)
3. **Automate ingestion** (schedule daily runs)
4. **Complete embedding coverage** (run embeddings on all chunks)

---

## ARCHITECTURAL DECISIONS NEEDED

1. **Dataset Strategy:**
   - Option A: Migrate all legacy data (`ncc_newsletters`) → `ncc_production`
   - Option B: Update search APIs to query `ncc_production` directly
   - Option C: Run both in parallel (dual-write)

2. **Schema Alignment:**
   - Legacy: `ncc_newsletters.messages` (has `newsletter_id`, different column names)
   - New: `ncc_production.raw_emails` (has `gmail_message_id`, `inbox`, `content_hash`)
   - Search expects `newsletter_id` but new pipeline uses `gmail_message_id`

3. **Publisher Canonicalization:**
   - New pipeline has `publishers` table with canonicalization
   - Legacy may not have this
   - Search uses `publisher_name` field directly

---

**Next Action:** Choose dataset strategy, then update search APIs to use chosen dataset.
</file>

<file path="docs/TODO_BACKLOG.md">
# Newsletter Control Center - TODO Backlog

## P0 (Ship First)

- **Run sent_date backfill when ready** (script exists)
  - Execute: `npm run backfill:sentdate:run`
  - File: `scripts/backfill-sent-date.ts`

- **Verify chunk schema alignment** after recent fixes
  - Compare: `scripts/chunk-new.ts` row structure vs `scripts/setup-bigquery.ts` DDL
  - Ensure all column names/types match exactly

- **Add embedding coverage % to smoke test**
  - Query: `SELECT COUNT(DISTINCT ch.chunk_id) AS chunked, COUNT(DISTINCT ce.chunk_id) AS embedded FROM chunks ch LEFT JOIN chunk_embeddings ce ON ch.chunk_id = ce.chunk_id`
  - File: `scripts/smoke.ts` (add to output after chunk coverage line)

## P1 (Quality)

- **Minimal README: credential rotation**
  - Create: `docs/keys-and-rotation.md` (or section in existing README)
  - Cover: Gmail token refresh (`--reauth`), service account key rotation path

- **Light retry/backoff for transient errors**
  - Add: 3 attempts with exponential backoff (1s, 2s, 4s)
  - Files: `src/bq/client.ts` (query wrapper), `src/embeddings/vertex.ts` (API calls)

- **One-line metrics at script end**
  - Print: `run_id=<uuid> | inserted=<N> | duration=<ms>`
  - Files: `scripts/ingest-gmail.ts`, `scripts/chunk-new.ts`, `scripts/embed-new-chunks.ts`

## P2 (Nice to Have)

- **Optional daily scheduling**
  - Option A: macOS launchd plist for `npm run pipeline:today`
  - Option B: Cloud Scheduler + Cloud Run job
  - Files: New `scripts/schedule-launchd.sh` or Cloud Build config

- **Simple Looker Studio dashboard**
  - Queries: 7-day ingest counts, chunk/embedding coverage trends
  - Data source: BigQuery `ncc_production` dataset

- **Unit tests for core utilities**
  - Test: `splitIntoChunks()` (edge cases: overlap > targetSize, empty text)
  - Test: `htmlToText()` (entity decoding, script/style removal)
  - Files: `src/lib/parseMessage.ts` → `src/lib/__tests__/parseMessage.test.ts`, `scripts/chunk-new.ts` → `scripts/__tests__/chunk-new.test.ts`
</file>

<file path="docs/UNIFIED_VIEWS.sql">
-- ============================================================================
-- Unified Views for Legacy + Production Data
-- ============================================================================
-- These views combine data from ncc_newsletters (legacy) and ncc_production
-- (new pipeline) to provide a unified query interface. All columns match
-- the Phase 0 canonical schema exactly (see docs/PHASE0_SPEC.md).

-- ============================================================================
-- View 1: Unified Raw Emails
-- ============================================================================
CREATE OR REPLACE VIEW `newsletter-control-center.ncc_production.v_all_raw_emails` AS
SELECT
  -- Legacy data (ncc_newsletters.messages)
  id AS gmail_message_id,
  CAST(NULL AS STRING) AS inbox,                    -- legacy doesn't track inbox
  CAST(NULL AS STRING) AS history_id,               -- legacy doesn't track history_id
  CAST(NULL AS STRING) AS message_id_header,        -- legacy doesn't store this
  subject,
  sender AS from_email,
  CAST(NULL AS STRING) AS from_name,                -- legacy doesn't parse display name
  CAST(NULL AS STRING) AS reply_to,                 -- legacy doesn't store reply-to
  list_id,
  CAST(sent_date AS TIMESTAMP) AS sent_date,
  body_html,
  body_text,
  CAST(NULL AS STRING) AS content_hash,             -- legacy doesn't compute hash
  COALESCE(is_paid, FALSE) AS is_paid,
  CAST(COALESCE(received_date, processed_at, sent_date) AS TIMESTAMP) AS ingested_at,
  'legacy' AS source
FROM `newsletter-control-center.ncc_newsletters.messages`
UNION ALL
SELECT
  -- Production data (ncc_production.raw_emails)
  gmail_message_id,
  inbox,
  history_id,
  message_id_header,
  subject,
  from_email,
  from_name,
  reply_to,
  list_id,
  sent_date,
  body_html,
  body_text,
  content_hash,
  is_paid,
  ingested_at,
  'prod' AS source
FROM `newsletter-control-center.ncc_production.raw_emails`;

-- ============================================================================
-- View 2: Unified Chunks
-- ============================================================================
CREATE OR REPLACE VIEW `newsletter-control-center.ncc_production.v_all_chunks` AS
SELECT
  -- Legacy data (ncc_newsletters.chunks)
  chunk_id,
  newsletter_id AS gmail_message_id,
  CAST(NULL AS STRING) AS publisher_id,             -- legacy doesn't link publishers
  CAST(NULL AS STRING) AS source_part,              -- not tracked in legacy
  CAST(NULL AS INT64)  AS char_start,               -- not tracked in legacy
  CAST(NULL AS INT64)  AS char_end,                 -- not tracked in legacy
  chunk_index,
  chunk_text,
  created_at,                                       -- legacy has created_at
  'legacy' AS source
FROM `newsletter-control-center.ncc_newsletters.chunks`
UNION ALL
SELECT
  -- Production data (ncc_production.chunks)
  chunk_id,
  gmail_message_id,
  publisher_id,
  source_part,
  char_start,
  char_end,
  chunk_index,
  chunk_text,
  created_at,
  'prod' AS source
FROM `newsletter-control-center.ncc_production.chunks`;

-- ============================================================================
-- View 3: Unified Chunk Embeddings
-- ============================================================================
-- Legacy stores embeddings in chunks table (chunk_embedding column), not separate table.
-- Production has dedicated chunk_embeddings table. For now, return only production.
-- TODO: Future backfill can extract legacy chunk_embedding arrays into this view.
-- NOTE: Legacy embeddings are denormalized in ncc_newsletters.chunks.chunk_embedding
-- and will need extraction during migration (see docs/MIGRATION_PLAYBOOK.md).
CREATE OR REPLACE VIEW `newsletter-control-center.ncc_production.v_all_chunk_embeddings` AS
SELECT
  -- Production data (ncc_production.chunk_embeddings)
  chunk_id,
  model,
  dim,
  embedding,
  created_at,
  'prod' AS source
FROM `newsletter-control-center.ncc_production.chunk_embeddings`
-- TODO: Future UNION ALL for legacy embeddings once extracted from ncc_newsletters.chunks.chunk_embedding
</file>

<file path="docs/VALIDATION_SQL.sql">
-- ============================================================================
-- VALIDATION SQL PACK (Phase 0 Spec)
-- ============================================================================
-- Purpose: Validate data integrity, uniqueness, and referential integrity
-- during migration from legacy (ncc_newsletters) to production (ncc_production).

-- ============================================================================
-- 1. ROW COUNTS (Prod vs Legacy vs Unified Views)
-- ============================================================================

-- Raw emails: Production vs Legacy vs Unified
SELECT 
  'raw_emails' AS table_name,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.raw_emails`) AS prod_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_newsletters.messages`) AS legacy_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_raw_emails`) AS unified_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_raw_emails` WHERE source = 'prod') AS unified_prod,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_raw_emails` WHERE source = 'legacy') AS unified_legacy;

-- Chunks: Production vs Legacy vs Unified
SELECT 
  'chunks' AS table_name,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.chunks`) AS prod_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_newsletters.chunks`) AS legacy_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_chunks`) AS unified_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_chunks` WHERE source = 'prod') AS unified_prod,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_chunks` WHERE source = 'legacy') AS unified_legacy;

-- Chunk embeddings: Production vs Unified
SELECT 
  'chunk_embeddings' AS table_name,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.chunk_embeddings`) AS prod_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_chunk_embeddings`) AS unified_count,
  (SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.v_all_chunk_embeddings` WHERE source = 'prod') AS unified_prod;

-- ============================================================================
-- 2. UNIQUENESS CHECKS
-- ============================================================================

-- raw_emails: gmail_message_id uniqueness (app-level primary key)
-- Should return 0 rows if no duplicates
SELECT 
  gmail_message_id, 
  COUNT(*) AS duplicate_count,
  ARRAY_AGG(DISTINCT inbox) AS inboxes,
  ARRAY_AGG(DISTINCT source) AS sources
FROM `newsletter-control-center.ncc_production.v_all_raw_emails`
GROUP BY gmail_message_id
HAVING COUNT(*) > 1;

-- raw_emails: gmail_message_id uniqueness per source (legacy vs prod)
-- Should return 0 rows if no duplicates within each source
SELECT 
  source,
  gmail_message_id, 
  COUNT(*) AS duplicate_count
FROM `newsletter-control-center.ncc_production.v_all_raw_emails`
GROUP BY source, gmail_message_id
HAVING COUNT(*) > 1;

-- chunks: (gmail_message_id, chunk_index) uniqueness (app-level composite key)
-- Should return 0 rows if no duplicates
SELECT 
  gmail_message_id,
  chunk_index,
  COUNT(*) AS duplicate_count,
  ARRAY_AGG(DISTINCT chunk_id) AS chunk_ids,
  ARRAY_AGG(DISTINCT source) AS sources
FROM `newsletter-control-center.ncc_production.v_all_chunks`
GROUP BY gmail_message_id, chunk_index
HAVING COUNT(*) > 1;

-- chunks: chunk_id uniqueness (database-level unique identifier)
-- Should return 0 rows if no duplicates
SELECT 
  chunk_id,
  COUNT(*) AS duplicate_count,
  ARRAY_AGG(DISTINCT gmail_message_id) AS gmail_message_ids,
  ARRAY_AGG(DISTINCT source) AS sources
FROM `newsletter-control-center.ncc_production.v_all_chunks`
GROUP BY chunk_id
HAVING COUNT(*) > 1;

-- chunk_embeddings: chunk_id uniqueness (app-level primary key, one-to-one with chunks)
-- Should return 0 rows if no duplicates
SELECT 
  chunk_id,
  COUNT(*) AS duplicate_count
FROM `newsletter-control-center.ncc_production.chunk_embeddings`
GROUP BY chunk_id
HAVING COUNT(*) > 1;

-- email_labels: (gmail_message_id, label_name) uniqueness (app-level composite key)
-- Should return 0 rows if no duplicates
SELECT 
  gmail_message_id,
  label_name,
  COUNT(*) AS duplicate_count
FROM `newsletter-control-center.ncc_production.email_labels`
GROUP BY gmail_message_id, label_name
HAVING COUNT(*) > 1;

-- ============================================================================
-- 3. REFERENTIAL INTEGRITY CHECKS
-- ============================================================================

-- Orphaned chunks: chunks without parent raw_emails
-- Should return 0 rows if referential integrity is maintained
SELECT 
  ch.gmail_message_id,
  ch.chunk_id,
  ch.chunk_index,
  ch.source AS chunk_source,
  COUNT(*) AS orphan_count
FROM `newsletter-control-center.ncc_production.v_all_chunks` ch
LEFT JOIN `newsletter-control-center.ncc_production.v_all_raw_emails` re
  ON ch.gmail_message_id = re.gmail_message_id
WHERE re.gmail_message_id IS NULL
GROUP BY ch.gmail_message_id, ch.chunk_id, ch.chunk_index, ch.source;

-- Orphaned embeddings: embeddings without parent chunks
-- Should return 0 rows if referential integrity is maintained
SELECT 
  ce.chunk_id,
  ce.model,
  COUNT(*) AS orphan_count
FROM `newsletter-control-center.ncc_production.chunk_embeddings` ce
LEFT JOIN `newsletter-control-center.ncc_production.v_all_chunks` ch
  ON ce.chunk_id = ch.chunk_id
WHERE ch.chunk_id IS NULL
GROUP BY ce.chunk_id, ce.model;

-- Orphaned email_labels: labels without parent raw_emails
-- Should return 0 rows if referential integrity is maintained
SELECT 
  el.gmail_message_id,
  el.label_name,
  COUNT(*) AS orphan_count
FROM `newsletter-control-center.ncc_production.email_labels` el
LEFT JOIN `newsletter-control-center.ncc_production.raw_emails` re
  ON el.gmail_message_id = re.gmail_message_id
WHERE re.gmail_message_id IS NULL
GROUP BY el.gmail_message_id, el.label_name;

-- Chunks without embeddings (coverage gap, not a referential error)
-- Returns chunks that exist but don't have embeddings yet
SELECT 
  ch.source,
  COUNT(DISTINCT ch.chunk_id) AS chunks_without_embeddings,
  COUNT(DISTINCT ch.gmail_message_id) AS emails_affected
FROM `newsletter-control-center.ncc_production.v_all_chunks` ch
LEFT JOIN `newsletter-control-center.ncc_production.chunk_embeddings` ce
  ON ch.chunk_id = ce.chunk_id
WHERE ce.chunk_id IS NULL
GROUP BY ch.source;

-- ============================================================================
-- 4. SPOT-CHECK HELPERS
-- ============================================================================

-- Latest 10 emails by sent_date: Compare unified vs prod vs legacy
-- Use to verify data appears correctly across all views
SELECT 
  'unified' AS source,
  gmail_message_id,
  subject,
  from_email,
  sent_date,
  ingested_at
FROM `newsletter-control-center.ncc_production.v_all_raw_emails`
ORDER BY COALESCE(sent_date, ingested_at) DESC NULLS LAST
LIMIT 10;

SELECT 
  'prod' AS source,
  gmail_message_id,
  subject,
  from_email,
  sent_date,
  ingested_at
FROM `newsletter-control-center.ncc_production.raw_emails`
ORDER BY COALESCE(sent_date, ingested_at) DESC NULLS LAST
LIMIT 10;

SELECT 
  'legacy' AS source,
  id AS gmail_message_id,
  subject,
  sender AS from_email,
  sent_date,
  received_date AS ingested_at
FROM `newsletter-control-center.ncc_newsletters.messages`
ORDER BY COALESCE(sent_date, received_date) DESC NULLS LAST
LIMIT 10;

-- Random sample hash comparison: Compare chunk_text MD5 between legacy and prod
-- Use to verify transform correctness (chunking logic preserved text correctly)
WITH legacy_sample AS (
  SELECT 
    newsletter_id AS gmail_message_id,
    chunk_index,
    TO_HEX(MD5(chunk_text)) AS chunk_hash,
    chunk_text
  FROM `newsletter-control-center.ncc_newsletters.chunks`
  WHERE chunk_text IS NOT NULL
  ORDER BY RAND()
  LIMIT 100
),
prod_sample AS (
  SELECT 
    ch.gmail_message_id,
    ch.chunk_index,
    TO_HEX(MD5(ch.chunk_text)) AS chunk_hash,
    ch.chunk_text
  FROM `newsletter-control-center.ncc_production.chunks` ch
  WHERE ch.chunk_text IS NOT NULL
    AND ch.gmail_message_id IN (SELECT gmail_message_id FROM legacy_sample)
)
SELECT 
  ls.gmail_message_id,
  ls.chunk_index,
  ls.chunk_hash AS legacy_hash,
  ps.chunk_hash AS prod_hash,
  CASE 
    WHEN ls.chunk_hash = ps.chunk_hash THEN 'MATCH'
    WHEN ps.chunk_hash IS NULL THEN 'MISSING_IN_PROD'
    ELSE 'MISMATCH'
  END AS status
FROM legacy_sample ls
LEFT JOIN prod_sample ps
  ON ls.gmail_message_id = ps.gmail_message_id
  AND ls.chunk_index = ps.chunk_index
ORDER BY ls.gmail_message_id, ls.chunk_index;

-- Sample email transform validation: Compare specific gmail_message_id across datasets
-- Replace 'REPLACE_WITH_GMAIL_MESSAGE_ID' with actual ID to test
SELECT 
  'legacy' AS source,
  id AS gmail_message_id,
  sender AS from_email,
  subject,
  sent_date,
  LENGTH(body_text) AS body_text_length,
  LENGTH(body_html) AS body_html_length,
  TO_HEX(MD5(body_text || COALESCE(body_html, ''))) AS content_hash
FROM `newsletter-control-center.ncc_newsletters.messages`
WHERE id = 'REPLACE_WITH_GMAIL_MESSAGE_ID';

SELECT 
  'prod' AS source,
  gmail_message_id,
  from_email,
  subject,
  sent_date,
  LENGTH(body_text) AS body_text_length,
  LENGTH(body_html) AS body_html_length,
  content_hash
FROM `newsletter-control-center.ncc_production.raw_emails`
WHERE gmail_message_id = 'REPLACE_WITH_GMAIL_MESSAGE_ID';

-- Chunk count comparison: Verify chunk counts match for migrated emails
-- Returns emails where chunk counts differ between legacy and prod
WITH legacy_chunk_counts AS (
  SELECT 
    newsletter_id AS gmail_message_id,
    COUNT(*) AS legacy_chunk_count
  FROM `newsletter-control-center.ncc_newsletters.chunks`
  GROUP BY newsletter_id
),
prod_chunk_counts AS (
  SELECT 
    gmail_message_id,
    COUNT(*) AS prod_chunk_count
  FROM `newsletter-control-center.ncc_production.chunks`
  GROUP BY gmail_message_id
)
SELECT 
  COALESCE(l.gmail_message_id, p.gmail_message_id) AS gmail_message_id,
  COALESCE(l.legacy_chunk_count, 0) AS legacy_count,
  COALESCE(p.prod_chunk_count, 0) AS prod_count,
  ABS(COALESCE(l.legacy_chunk_count, 0) - COALESCE(p.prod_chunk_count, 0)) AS difference
FROM legacy_chunk_counts l
FULL OUTER JOIN prod_chunk_counts p
  ON l.gmail_message_id = p.gmail_message_id
WHERE COALESCE(l.legacy_chunk_count, 0) != COALESCE(p.prod_chunk_count, 0)
ORDER BY difference DESC
LIMIT 50;

-- ============================================================================
-- 5. DATA QUALITY CHECKS
-- ============================================================================

-- NULL sent_date count (should be minimal after backfill)
SELECT 
  source,
  COUNT(*) AS null_sent_date_count,
  ROUND(COUNT(*) * 100.0 / NULLIF(COUNT(*) OVER (PARTITION BY source), 0), 2) AS pct_null
FROM `newsletter-control-center.ncc_production.v_all_raw_emails`
WHERE sent_date IS NULL
GROUP BY source;

-- Empty chunk_text (should be 0 or very low)
SELECT 
  source,
  COUNT(*) AS empty_chunk_text_count
FROM `newsletter-control-center.ncc_production.v_all_chunks`
WHERE chunk_text IS NULL OR LENGTH(chunk_text) = 0
GROUP BY source;

-- Embeddings with NULL or empty arrays
SELECT 
  COUNT(*) AS null_embedding_count,
  COUNT(*) * 100.0 / NULLIF((SELECT COUNT(*) FROM `newsletter-control-center.ncc_production.chunk_embeddings`), 0) AS pct_null
FROM `newsletter-control-center.ncc_production.chunk_embeddings`
WHERE embedding IS NULL OR ARRAY_LENGTH(embedding) = 0;

-- Embedding dimension consistency (should all be same dimension for text-embedding-004)
SELECT 
  model,
  dim,
  COUNT(*) AS count,
  COUNT(*) * 100.0 / NULLIF(SUM(COUNT(*)) OVER (PARTITION BY model), 0) AS pct
FROM `newsletter-control-center.ncc_production.chunk_embeddings`
GROUP BY model, dim
ORDER BY model, count DESC;
</file>

<file path="scripts/cloud/auth-sa.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

interface ServiceAccountKey {
  client_email?: string;
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Execute the command instead of previewing',
    })
    .parse();

  // Robust project resolution
  const resolve = (cmd: string) => {
    try {
      return execSync(cmd, { stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
    } catch {
      return '';
    }
  };

  const project = process.env.BQ_PROJECT_ID || (() => {
    try {
      return resolve('gcloud config get-value project');
    } catch {
      return '';
    }
  })();

  if (!project) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Key file resolution
  const keyPath = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  const resolvedKey = path.resolve(keyPath);

  // Validate key file exists
  const fsSync = require('fs');
  if (!fsSync.existsSync(resolvedKey)) {
    throw new Error(`Key file does not exist: ${resolvedKey}`);
  }

  // Parse client_email from key JSON
  let client_email = '';
  try {
    const content = await fs.readFile(resolvedKey, 'utf8');
    const key = JSON.parse(content) as ServiceAccountKey;
    if (!key.client_email) {
      throw new Error('client_email not found in key file');
    }
    client_email = key.client_email;
  } catch (error: any) {
    throw new Error(`Failed to read service account from ${resolvedKey}: ${error.message}`);
  }

  const command = `gcloud auth activate-service-account ${client_email} --key-file ${resolvedKey} --project ${project}`;

  if (!argv.apply) {
    // Preview mode: PRINT the exact command
    console.log('Command that would be executed:');
    console.log('');
    console.log(command);
    console.log('');
    console.log('To execute, run with --apply');
  } else {
    // Apply mode: execute with stdio: 'inherit'
    try {
      execSync(command, { stdio: 'inherit' });
      console.log(`\ngcloud is now authenticated as: ${client_email}`);
    } catch (error: any) {
      console.error(`\nError: ${error.message || 'Command failed'}`);
      process.exit(1);
    }
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/bootstrap-cloud-access.ts">
import 'dotenv/config';
import * as fs from 'fs';
import { execSync, spawnSync } from 'child_process';

const APIS = [
  'cloudresourcemanager.googleapis.com',
  'run.googleapis.com',
  'cloudscheduler.googleapis.com',
  'secretmanager.googleapis.com',
  'artifactregistry.googleapis.com',
  'containerregistry.googleapis.com',
  'cloudbuild.googleapis.com',
];

const ROLES = [
  'roles/viewer',
  'roles/run.viewer',
  'roles/cloudscheduler.viewer',
  'roles/secretmanager.viewer',
  'roles/iam.serviceAccountViewer',
  'roles/logging.viewer',
  'roles/cloudbuild.builds.editor',
  'roles/storage.objectCreator',
];

async function main(): Promise<void> {
  const argv = process.argv.includes('--apply');

  // Resolve project
  const PROJECT = process.env.BQ_PROJECT_ID || execSync('gcloud config get-value project', { stdio: ['ignore', 'pipe', 'ignore'] }).toString().trim();
  if (!PROJECT) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve key file
  const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  if (!fs.existsSync(KEY)) {
    throw new Error(`Key file not found: ${KEY}`);
  }

  // Parse SA from key
  const SA = JSON.parse(fs.readFileSync(KEY, 'utf8')).client_email;
  if (!SA) {
    throw new Error('client_email missing in key');
  }

  // Build commands
  const commands = [
    { step: 1, cmd: 'gcloud auth login --update-adc', description: 'Authenticate as human user' },
    { step: 2, cmd: `gcloud config set project ${PROJECT}`, description: 'Set project' },
    ...APIS.map((api) => ({
      step: 3,
      cmd: `gcloud services enable ${api} --project ${PROJECT}`,
      description: `Enable ${api}`,
    })),
    ...ROLES.map((role) => ({
      step: 4,
      cmd: `gcloud projects add-iam-policy-binding ${PROJECT} --member serviceAccount:${SA} --role ${role}`,
      description: `Grant ${role}`,
    })),
    { step: 5, cmd: `gcloud auth activate-service-account ${SA} --key-file ${KEY} --project ${PROJECT}`, description: 'Switch back to service account' },
    { step: 6, cmd: 'echo "Now run: npm run cloud:discover:apply && npm run cloud:issues"', description: 'Next steps' },
  ];

  if (!argv) {
    // Preview mode: PRINT the exact commands
    console.log('---');
    console.log('CLOUD BOOTSTRAP PLAN (Preview)');
    console.log('');
    console.log(`Project: ${PROJECT}`);
    console.log(`Service Account: ${SA}`);
    console.log('');
    console.log('Commands to run:');
    console.log('');
    for (const cmd of commands) {
      console.log(`${cmd.step}) ${cmd.cmd}`);
    }
    console.log('');
    console.log('To execute, run with --apply');
    console.log('---');
    return;
  }

  // Apply mode: execute commands
  console.log('---');
  console.log('CLOUD BOOTSTRAP (Applying)');
  console.log(`Project: ${PROJECT}`);
  console.log(`Service Account: ${SA}`);
  console.log('');

  for (const cmd of commands) {
    console.log(`Step ${cmd.step}: ${cmd.description}`);
    console.log(`Running: ${cmd.cmd}`);

    try {
      if (cmd.step === 1) {
        // Step 1: auth login must use stdio: 'inherit' (opens browser)
        spawnSync('gcloud', ['auth', 'login', '--update-adc'], { stdio: 'inherit' });
      } else if (cmd.step === 6) {
        // Step 6: echo command
        console.log('Now run: npm run cloud:discover:apply && npm run cloud:issues');
      } else {
        // Steps 2-5: use execSync
        execSync(cmd.cmd, { stdio: 'inherit' });
      }
      console.log(`✅ Step ${cmd.step} completed`);
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      // Steps 3 and 4: continue on "already enabled" / "already has"
      if ((cmd.step === 3 || cmd.step === 4) && (errorMsg.includes('already enabled') || errorMsg.includes('already has') || errorMsg.includes('already exists'))) {
        console.log(`⚠️  Step ${cmd.step} skipped (already applied)`);
      } else {
        console.error(`❌ Step ${cmd.step} failed: ${cmd.cmd}`);
        console.error(`Error: ${errorMsg}`);
        process.exit(1);
      }
    }
    console.log('');
  }

  console.log('---');
  console.log('Bootstrap complete!');
  console.log('Now run: npm run cloud:discover:apply && npm run cloud:issues');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/build-image.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function sleep(ms: number): Promise<void> {
  return new Promise((resolve) => setTimeout(resolve, ms));
}

async function main(): Promise<void> {
  // Resolve project
  const PROJECT = process.env.BQ_PROJECT_ID || shell('gcloud config get-value project');
  if (!PROJECT) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve region
  const REGION = process.env.NCC_REGION || 'us-central1';

  // Image configuration
  const REPO = 'ncc';
  const IMAGE = `us-central1-docker.pkg.dev/${PROJECT}/${REPO}/ncc-worker`;
  const TAG = shell('git rev-parse --short HEAD') || 'local';
  const FULL = `${IMAGE}:${TAG}`;

  console.log('---');
  console.log('BUILD IMAGE');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Repository: ${REPO}`);
  console.log(`Image: ${FULL}`);
  console.log('');

  // Ensure Artifact Registry repo exists
  console.log(`Ensuring Artifact Registry repository exists: ${REPO}...`);
  try {
    execSync(`gcloud artifacts repositories describe ${REPO} --location=${REGION} --project=${PROJECT}`, {
      stdio: 'ignore',
    });
    console.log(`✅ Repository ${REPO} already exists`);
  } catch (error: any) {
    const errorMsg = error.message || String(error);
    if (errorMsg.includes('404') || errorMsg.includes('not found')) {
      console.log(`Creating repository ${REPO}...`);
      execSync(
        `gcloud artifacts repositories create ${REPO} --repository-format=docker --location=${REGION} --project=${PROJECT}`,
        { stdio: 'inherit' },
      );
      console.log(`✅ Repository ${REPO} created`);
    } else {
      throw error;
    }
  }
  console.log('');

  // Submit build asynchronously
  console.log(`Submitting build: ${FULL}...`);
  try {
    execSync(`gcloud builds submit --tag ${FULL} --async --timeout=1200s .`, { stdio: 'inherit' });
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Build submission failed: ${stderr}`);
  }
  console.log('✅ Build submitted');
  console.log('');

  // Poll for build ID
  console.log('Polling for build ID...');
  let BUILD_ID = '';
  const maxRetries = 20;
  const pollInterval = 3000; // 3 seconds

  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      BUILD_ID = shell(
        `gcloud builds list --filter="images:${FULL}" --sort-by="~createTime" --limit=1 --format="value(ID)" --project=${PROJECT}`,
      );
      if (BUILD_ID) {
        break;
      }
    } catch (error: any) {
      // Continue polling
    }

    if (attempt < maxRetries) {
      console.log(`  Attempt ${attempt}/${maxRetries}: waiting for build ID...`);
      await sleep(pollInterval);
    }
  }

  if (!BUILD_ID) {
    throw new Error(`Failed to get build ID after ${maxRetries} attempts`);
  }

  console.log(`✅ Build ID: ${BUILD_ID}`);
  console.log('');

  // Stream logs
  console.log('Streaming build logs...');
  console.log('---');
  const logResult = spawnSync(
    'gcloud',
    ['builds', 'log', '--stream', '--project', PROJECT, BUILD_ID],
    { stdio: 'inherit' },
  );
  console.log('---');
  console.log('');

  // Verify status
  console.log('Verifying build status...');
  const status = shell(`gcloud builds describe --project ${PROJECT} ${BUILD_ID} --format="value(status)"`);

  if (status !== 'SUCCESS') {
    console.error(`❌ Build failed with status: ${status}`);
    process.exit(1);
  }

  console.log(`✅ Build status: ${status}`);
  console.log('');

  // Print digest
  console.log('Fetching image digest...');
  const digest = shell(
    `gcloud builds describe --project ${PROJECT} ${BUILD_ID} --format="value(results.images[0].digest)"`,
  );

  console.log('');
  console.log('---');
  console.log(`✅ Build complete!`);
  console.log(`Image URI: ${FULL}`);
  if (digest) {
    console.log(`Digest: ${digest}`);
  }
  console.log('---');

  // Write image URI to docs/LATEST_IMAGE.txt for deployment scripts
  const fs = require('fs');
  const path = require('path');
  const docsDir = path.resolve(__dirname, '../../docs');
  if (!fs.existsSync(docsDir)) {
    fs.mkdirSync(docsDir, { recursive: true });
  }
  const imagePath = path.join(docsDir, 'LATEST_IMAGE.txt');
  fs.writeFileSync(imagePath, `${FULL}\n`);
  console.log(`Written image URI to: ${imagePath}`);
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/deploy-jobs.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import * as fs from 'fs';
import * as path from 'path';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string): T | null {
  try {
    const output = shell(cmd);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

interface JobConfig {
  name: string;
  command: string;
  args: string[];
}

interface IngestJobConfig extends JobConfig {
  inbox: 'me' | 'other';
  secrets?: string[]; // Secret bindings for Cloud Run
}

const JOBS: JobConfig[] = [
  {
    name: 'ncc-chunks',
    command: 'node',
    args: ['dist/scripts/chunk-new.js', '--limit', '800', '--no-dry-run'],
  },
  {
    name: 'ncc-embeddings',
    command: 'node',
    args: ['dist/scripts/embed-new-chunks.js', '--limit', '800', '--no-dry-run'],
  },
  {
    name: 'ncc-smoke',
    command: 'node',
    args: ['dist/scripts/smoke.js'],
  },
];

const INGEST_JOBS: IngestJobConfig[] = [
  {
    name: 'ncc-ingest-me',
    command: 'node',
    args: ['dist/scripts/ingest-gmail.js', '--inbox', 'me', '--limit', '500', '--no-dry-run'],
    inbox: 'me',
    secrets: [
      'GMAIL_CLIENT_ID=GMAIL_CLIENT_ID:latest',
      'GMAIL_CLIENT_SECRET=GMAIL_CLIENT_SECRET:latest',
      'GMAIL_REFRESH_TOKEN_ME=GMAIL_REFRESH_TOKEN_ME:latest',
    ],
  },
  {
    name: 'ncc-ingest-other',
    command: 'node',
    args: ['dist/scripts/ingest-gmail.js', '--inbox', 'other', '--limit', '500', '--no-dry-run'],
    inbox: 'other',
    secrets: [
      'GMAIL_CLIENT_ID=GMAIL_CLIENT_ID:latest',
      'GMAIL_CLIENT_SECRET=GMAIL_CLIENT_SECRET:latest',
      'GMAIL_REFRESH_TOKEN_OTHER=GMAIL_REFRESH_TOKEN_OTHER:latest',
    ],
  },
];

async function resolveImage(override?: string): Promise<string> {
  if (override) {
    return override;
  }

  // Try reading from docs/LATEST_IMAGE.txt
  const latestImagePath = path.resolve(__dirname, '../../docs/LATEST_IMAGE.txt');
  if (fs.existsSync(latestImagePath)) {
    const content = fs.readFileSync(latestImagePath, 'utf8').trim();
    if (content) {
      return content;
    }
  }

  // Fall back to querying Artifact Registry
  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REPO = 'us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc-worker';

  try {
    const tags = shellJSON<Array<{ name?: string; createTime?: string }>>(
      `gcloud artifacts docker tags list ${REPO} --format=json --project=${PROJECT}`,
    );

    if (tags && tags.length > 0) {
      // Sort by createTime descending and pick the newest
      const sorted = tags
        .filter((t) => t.name && t.createTime)
        .sort((a, b) => (b.createTime || '').localeCompare(a.createTime || ''));
      if (sorted.length > 0) {
        const latestTag = sorted[0].name || '';
        return `${REPO}:${latestTag}`;
      }
    }
  } catch (error: any) {
    console.warn(`⚠️  Could not query Artifact Registry: ${error.message}`);
  }

  throw new Error(
    'Could not resolve image. Either provide --image or ensure docs/LATEST_IMAGE.txt exists or Artifact Registry is accessible.',
  );
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually create/update jobs (default: preview)',
    })
    .option('image', {
      type: 'string',
      description: 'Override image URI',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';

  console.log('---');
  console.log('DEPLOY CLOUD RUN JOBS');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Service Account: ${SA}`);
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'PREVIEW'}`);
  console.log('');

  // Resolve image
  const IMAGE = await resolveImage(argv.image);
  console.log(`Image: ${IMAGE}`);
  console.log('');

  // Build commands for each job
  const commands: Array<{ job: JobConfig | IngestJobConfig; createCmd: string; updateCmd: string }> = [];

  // Regular jobs (chunks, embeddings, smoke)
  for (const job of JOBS) {
    // Don't set GOOGLE_APPLICATION_CREDENTIALS - let jobs use ADC (metadata server)
    const envVars = [
      `BQ_PROJECT_ID=${PROJECT}`,
      `BQ_DATASET=ncc_production`,
      `BQ_LOCATION=US`,
    ].join(',');

    const argsStr = job.args.map((a) => `"${a}"`).join(',');

    const createCmd = `gcloud run jobs create ${job.name} \\
  --image=${IMAGE} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  --service-account=${SA} \\
  --set-env-vars=${envVars} \\
  --command=${job.command} \\
  --args=${argsStr}`;

    const updateCmd = `gcloud run jobs update ${job.name} \\
  --image=${IMAGE} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  --service-account=${SA} \\
  --set-env-vars=${envVars} \\
  --command=${job.command} \\
  --args=${argsStr}`;

    commands.push({ job, createCmd, updateCmd });
  }

  // Ingest jobs (with secrets and Gmail env vars)
  for (const job of INGEST_JOBS) {
    // Note: GMAIL_QUERY contains spaces - quote the value in the env var string
    const envVars = [
      `BQ_PROJECT_ID=${PROJECT}`,
      `BQ_DATASET=ncc_production`,
      `BQ_LOCATION=US`,
      `GMAIL_READONLY=false`,
      `GMAIL_PROCESSED_LABEL=Ingested`,
      `GMAIL_PAID_LABEL=Paid\\ $`,
      `GMAIL_MARK_READ=true`,
      `GMAIL_QUERY=is:unread\\ -label:Ingested`,
    ].join(',');

    const secretsStr = job.secrets?.join(',') || '';
    const setSecrets = secretsStr ? `--set-secrets=${secretsStr}` : '';

    const argsStr = job.args.map((a) => `"${a}"`).join(',');

    const createCmd = `gcloud run jobs create ${job.name} \\
  --image=${IMAGE} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  --service-account=${SA} \\
  --set-env-vars=${envVars} \\
  ${setSecrets} \\
  --command=${job.command} \\
  --args=${argsStr}`;

    const updateCmd = `gcloud run jobs update ${job.name} \\
  --image=${IMAGE} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  --service-account=${SA} \\
  --set-env-vars=${envVars} \\
  ${setSecrets} \\
  --command=${job.command} \\
  --args=${argsStr}`;

    commands.push({ job, createCmd, updateCmd });
  }

  if (!argv.apply) {
    // Preview mode
    console.log('PREVIEW: Would execute the following:');
    console.log('');
    for (const { job, createCmd, updateCmd } of commands) {
      console.log(`Job: ${job.name}`);
      console.log(`  1. Try: ${createCmd.split('\\')[0]}...`);
      console.log(`  2. If exists, run: ${updateCmd.split('\\')[0]}...`);
      console.log('');
    }
    console.log('Run with --apply to execute these commands.');
    console.log('---');
    return;
  }

  // Apply mode
  console.log('APPLY MODE: Creating/updating jobs...');
  console.log('');

  // Check if we need to switch to human user for deployment
  let currentAccount = '';
  try {
    currentAccount = execSync('gcloud config get-value account', { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch {
    currentAccount = '';
  }

  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com');

  if (needsHumanAuth) {
    console.log('Authenticated as service account. Deployment requires human user.');
    console.log('Switching to human user...');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    execSync(`gcloud config set project ${PROJECT}`, { stdio: 'ignore' });
    console.log('✅ Switched to human user');
    console.log('');
  }

  for (const { job, createCmd, updateCmd } of commands) {
    console.log(`Processing job: ${job.name}...`);

    try {
      // Build command properly
      const isIngestJob = 'inbox' in job;
      const ingestJob = isIngestJob ? job as IngestJobConfig : null;

      let envVars: string;
      if (isIngestJob && ingestJob) {
        envVars = [
          `BQ_PROJECT_ID=${PROJECT}`,
          `BQ_DATASET=ncc_production`,
          `BQ_LOCATION=US`,
          `GMAIL_READONLY=false`,
          `GMAIL_PROCESSED_LABEL=Ingested`,
          `GMAIL_PAID_LABEL=Paid\\ $`,
          `GMAIL_MARK_READ=true`,
          `GMAIL_QUERY=is:unread\\ -label:Ingested`,
        ].join(',');
      } else {
        envVars = `BQ_PROJECT_ID=${PROJECT},BQ_DATASET=ncc_production,BQ_LOCATION=US`;
      }

      const createArgs = [
        'run', 'jobs', 'create', job.name,
        `--image=${IMAGE}`,
        `--region=${REGION}`,
        `--project=${PROJECT}`,
        `--service-account=${SA}`,
        `--set-env-vars=${envVars}`,
        `--command=${job.command}`,
        `--args=${job.args.join(',')}`,
      ];

      // Add secrets for ingest jobs
      if (isIngestJob && ingestJob?.secrets) {
        createArgs.push(`--set-secrets=${ingestJob.secrets.join(',')}`);
      }
      
      execSync(`gcloud ${createArgs.join(' ')}`, { stdio: 'pipe' });
      console.log(`✅ Created job: ${job.name}`);
    } catch (error: any) {
      const errorMsg = (error.stderr?.toString() || error.stdout?.toString() || error.message || String(error)).toLowerCase();
      if (errorMsg.includes('already exists') || errorMsg.includes('409')) {
        // Job exists, update it
        console.log(`   Job exists, updating...`);
        try {
          const isIngestJob = 'inbox' in job;
          const ingestJob = isIngestJob ? job as IngestJobConfig : null;

          let envVars: string;
          if (isIngestJob && ingestJob) {
            envVars = [
              `BQ_PROJECT_ID=${PROJECT}`,
              `BQ_DATASET=ncc_production`,
              `BQ_LOCATION=US`,
              `GMAIL_READONLY=false`,
              `GMAIL_PROCESSED_LABEL=Ingested`,
              `GMAIL_PAID_LABEL=Paid\\ $`,
              `GMAIL_MARK_READ=true`,
              `GMAIL_QUERY=is:unread\\ -label:Ingested`,
            ].join(',');
          } else {
            envVars = `BQ_PROJECT_ID=${PROJECT},BQ_DATASET=ncc_production,BQ_LOCATION=US`;
          }

          const updateArgs = [
            'run', 'jobs', 'update', job.name,
            `--image=${IMAGE}`,
            `--region=${REGION}`,
            `--project=${PROJECT}`,
            `--service-account=${SA}`,
            `--set-env-vars=${envVars}`,
            `--command=${job.command}`,
            `--args=${job.args.join(',')}`,
          ];

          // Add secrets for ingest jobs
          if (isIngestJob && ingestJob?.secrets) {
            updateArgs.push(`--set-secrets=${ingestJob.secrets.join(',')}`);
          }

          execSync(`gcloud ${updateArgs.join(' ')}`, { stdio: 'inherit' });
          console.log(`✅ Updated job: ${job.name}`);
        } catch (updateError: any) {
          console.error(`❌ Failed to update job ${job.name}: ${updateError.message}`);
        }
      } else {
        // Print the error for debugging
        console.error(`❌ Failed to create job ${job.name}`);
        console.error(error.stderr?.toString() || error.message || String(error));
      }
    }
    console.log('');
  }

  // Switch back to service account if we switched
  if (needsHumanAuth) {
    const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
    console.log('Switching back to service account...');
    execSync(`gcloud auth activate-service-account ${SA} --key-file "${KEY}" --project ${PROJECT}`, { stdio: 'inherit' });
    console.log('✅ Switched back to service account');
    console.log('');
  }

  console.log('---');
  console.log('✅ Job deployment complete!');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/deploy-runner.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

async function resolveImage(override?: string): Promise<string> {
  if (override) {
    return override;
  }

  // Try reading from docs/LATEST_IMAGE.txt
  const fs = require('fs');
  const path = require('path');
  const latestImagePath = path.resolve(__dirname, '../../docs/LATEST_IMAGE.txt');
  if (fs.existsSync(latestImagePath)) {
    const content = fs.readFileSync(latestImagePath, 'utf8').trim();
    if (content) {
      return content;
    }
  }

  // Fall back to querying Artifact Registry
  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REPO = 'us-central1-docker.pkg.dev/newsletter-control-center/ncc/ncc-worker';

  try {
    const tags = JSON.parse(
      shell(`gcloud artifacts docker tags list ${REPO} --format=json --project=${PROJECT}`),
    );

    if (tags && tags.length > 0) {
      const sorted = tags
        .filter((t: any) => t.name && t.createTime)
        .sort((a: any, b: any) => (b.createTime || '').localeCompare(a.createTime || ''));
      if (sorted.length > 0) {
        const latestTag = sorted[0].name || '';
        return `${REPO}:${latestTag}`;
      }
    }
  } catch (error: any) {
    console.warn(`⚠️  Could not query Artifact Registry: ${error.message}`);
  }

  throw new Error(
    'Could not resolve image. Either provide --image or ensure docs/LATEST_IMAGE.txt exists or Artifact Registry is accessible.',
  );
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually deploy the service (default: preview)',
    })
    .option('image', {
      type: 'string',
      description: 'Override image URI',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';
  const SERVICE_NAME = 'ncc-jobs-runner';
  const HEALTH_PUBLIC = process.env.RUNNER_HEALTH_PUBLIC === 'true';

  console.log('---');
  console.log('DEPLOY JOBS RUNNER SERVICE');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Service Account: ${SA}`);
  console.log(`Service Name: ${SERVICE_NAME}`);
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'PREVIEW'}`);
  if (HEALTH_PUBLIC) {
    console.log(`⚠️  WARNING: RUNNER_HEALTH_PUBLIC=true`);
    console.log(`   Service will be publicly invokable at IAM level (--allow-unauthenticated)`);
    console.log(`   App-level guards protect /run and other endpoints (only GET /health-check is unauthenticated)`);
  }
  console.log('');

  // Resolve image
  const IMAGE = await resolveImage(argv.image);
  console.log(`Image: ${IMAGE}`);
  console.log('');

  const authFlag = HEALTH_PUBLIC ? '--allow-unauthenticated' : '--no-allow-unauthenticated';
  const deployCmd = `gcloud run deploy ${SERVICE_NAME} \\
  --image=${IMAGE} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  ${authFlag} \\
  --service-account=${SA} \\
  --port=8080 \\
  --set-env-vars=BQ_PROJECT_ID=${PROJECT},NCC_REGION=${REGION} \\
  --command=node \\
  --args=dist/src/api/jobs-runner.js`;

  if (!argv.apply) {
    // Preview mode
    console.log('PREVIEW: Would execute the following:');
    console.log('');
    console.log(deployCmd);
    console.log('');
    console.log('Run with --apply to execute this command.');
    console.log('---');
    return;
  }

  // Apply mode
  console.log('APPLY MODE: Deploying service...');
  console.log('');

  // Check if we need to switch to human user for deployment
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }

  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com');

  if (needsHumanAuth) {
    console.log('Authenticated as service account. Deployment requires human user.');
    console.log('Switching to human user...');
    const { spawnSync } = require('child_process');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    shell(`gcloud config set project ${PROJECT}`);
    console.log('✅ Switched to human user');
    console.log('');
  }

  try {
    // Build command array properly
    const cmdArgs = [
      'run', 'deploy', SERVICE_NAME,
      `--image=${IMAGE}`,
      `--region=${REGION}`,
      `--project=${PROJECT}`,
      ...(HEALTH_PUBLIC ? ['--allow-unauthenticated'] : ['--no-allow-unauthenticated']),
      `--service-account=${SA}`,
      '--port=8080',
      `--set-env-vars=BQ_PROJECT_ID=${PROJECT},NCC_REGION=${REGION},BQ_DATASET=ncc_production,BQ_LOCATION=US`,
      '--command=node',
      `--args=dist/src/api/jobs-runner.js`,
    ];
    
    execSync(`gcloud ${cmdArgs.join(' ')}`, { stdio: 'inherit' });
    console.log('');
    console.log('✅ Service deployed successfully!');
    console.log('');

    // Get the service URL
    const url = shell(
      `gcloud run services describe ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format="value(status.url)"`,
    );
    console.log(`Service URL: ${url}`);
    console.log('---');

    // Switch back to service account if we switched
    if (needsHumanAuth) {
      const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
      console.log('Switching back to service account...');
      execSync(`gcloud auth activate-service-account ${SA} --key-file "${KEY}" --project ${PROJECT}`, { stdio: 'inherit' });
      console.log('✅ Switched back to service account');
    }
  } catch (error: any) {
    console.error('❌ Deployment failed:', error.message);
    process.exit(1);
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/diagnose-build-perms.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs';

interface ServiceAccount {
  email?: string;
  uniqueId?: string;
  name?: string;
}

interface IAMBinding {
  role?: string;
  members?: string[];
}

interface IAMPolicy {
  bindings?: IAMBinding[];
}

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch {
    return '';
  }
}

function shellOrFail(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string): T | null {
  try {
    const output = shellOrFail(cmd);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

async function main(): Promise<void> {
  // Resolve project ID
  let PROJECT_ID = process.env.BQ_PROJECT_ID;
  if (!PROJECT_ID) {
    PROJECT_ID = shell('gcloud config get-value project');
  }
  if (!PROJECT_ID) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve project number
  let PROJECT_NUMBER = '';
  try {
    const projectInfo = shellJSON<{ projectNumber?: string }>(
      `gcloud projects describe ${PROJECT_ID} --format=json`,
    );
    PROJECT_NUMBER = projectInfo?.projectNumber || '';
  } catch (error: any) {
    console.error(`⚠️  Failed to get project number: ${error.message}`);
  }

  // Resolve caller SA from credentials
  let CALLER_SA = '';
  const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  if (fs.existsSync(KEY)) {
    try {
      const keyContent = JSON.parse(fs.readFileSync(KEY, 'utf8'));
      CALLER_SA = keyContent.client_email || '';
    } catch (error: any) {
      console.error(`⚠️  Failed to read service account from ${KEY}: ${error.message}`);
    }
  }

  // Compute expected SA emails
  const CLOUDBUILD_SA = PROJECT_NUMBER ? `${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com` : '';
  const COMPUTE_DEFAULT_SA = PROJECT_NUMBER ? `${PROJECT_NUMBER}-compute@developer.gserviceaccount.com` : '';

  console.log('---');
  console.log('CLOUD BUILD PERMISSIONS DIAGNOSIS');
  console.log(`Project ID: ${PROJECT_ID}`);
  console.log(`Project Number: ${PROJECT_NUMBER || '(unknown)'}`);
  console.log(`Caller SA: ${CALLER_SA || '(unknown)'}`);
  console.log(`Cloud Build SA: ${CLOUDBUILD_SA || '(unknown)'}`);
  console.log(`Compute Default SA: ${COMPUTE_DEFAULT_SA || '(unknown)'}`);
  console.log('');

  // List all service accounts to map uniqueId → email
  console.log('1. SERVICE ACCOUNTS:');
  let allSAs: ServiceAccount[] = [];
  try {
    const output = shellOrFail(`gcloud iam service-accounts list --project=${PROJECT_ID} --format=json`);
    allSAs = output ? JSON.parse(output) : [];
    console.log(`   Found ${allSAs.length} service accounts:`);
    for (const sa of allSAs) {
      console.log(`   - ${sa.email || '(no email)'} (uniqueId: ${sa.uniqueId || 'unknown'})`);
    }
  } catch (error: any) {
    console.error(`   ❌ Failed to list service accounts: ${error.message}`);
  }
  console.log('');

  // Resolve the target SA with uniqueId 117326338887774071653
  const TARGET_UNIQUE_ID = '117326338887774071653';
  const targetSA = allSAs.find((sa) => sa.uniqueId === TARGET_UNIQUE_ID);
  const TARGET_SA_EMAIL = targetSA?.email || '';

  console.log('2. ACT-AS TARGET:');
  if (TARGET_SA_EMAIL) {
    console.log(`   ✅ Resolved uniqueId ${TARGET_UNIQUE_ID} → ${TARGET_SA_EMAIL}`);
  } else {
    console.log(`   ❌ Could not resolve uniqueId ${TARGET_UNIQUE_ID} to any service account`);
  }
  console.log('');

  // Get project IAM policy
  console.log('3. PROJECT IAM POLICY (relevant bindings):');
  let projectPolicy: IAMPolicy | null = null;
  try {
    projectPolicy = shellJSON<IAMPolicy>(`gcloud projects get-iam-policy ${PROJECT_ID} --format=json`);
  } catch (error: any) {
    console.error(`   ❌ Failed to get IAM policy: ${error.message}`);
  }

  const relevantSAs = [CALLER_SA, CLOUDBUILD_SA, COMPUTE_DEFAULT_SA, TARGET_SA_EMAIL].filter(Boolean);
  const callerRoles: string[] = [];
  const cloudbuildRoles: string[] = [];
  const targetRoles: string[] = [];

  if (projectPolicy?.bindings) {
    for (const binding of projectPolicy.bindings) {
      if (!binding.role || !binding.members) continue;
      for (const member of binding.members) {
        if (member.includes(CALLER_SA)) {
          callerRoles.push(binding.role);
        }
        if (member.includes(CLOUDBUILD_SA)) {
          cloudbuildRoles.push(binding.role);
        }
        if (member.includes(TARGET_SA_EMAIL)) {
          targetRoles.push(binding.role);
        }
      }
    }
  }

  console.log(`   Caller SA (${CALLER_SA}):`);
  if (callerRoles.length > 0) {
    callerRoles.forEach((role) => console.log(`     - ${role}`));
  } else {
    console.log('     (no bindings found)');
  }

  console.log(`   Cloud Build SA (${CLOUDBUILD_SA}):`);
  if (cloudbuildRoles.length > 0) {
    cloudbuildRoles.forEach((role) => console.log(`     - ${role}`));
  } else {
    console.log('     (no bindings found)');
  }

  if (TARGET_SA_EMAIL) {
    console.log(`   Target SA (${TARGET_SA_EMAIL}):`);
    if (targetRoles.length > 0) {
      targetRoles.forEach((role) => console.log(`     - ${role}`));
    } else {
      console.log('     (no bindings found)');
    }
  }
  console.log('');

  // Check if CALLER_SA has serviceAccountUser on TARGET_SA
  console.log('4. SERVICE ACCOUNT USER PERMISSION:');
  let hasServiceAccountUser = false;
  if (TARGET_SA_EMAIL && CALLER_SA) {
    try {
      const testPolicy = shellJSON<IAMPolicy>(
        `gcloud iam service-accounts get-iam-policy ${TARGET_SA_EMAIL} --format=json`,
      );
      if (testPolicy?.bindings) {
        for (const binding of testPolicy.bindings) {
          if (binding.role === 'roles/iam.serviceAccountUser' && binding.members) {
            if (binding.members.includes(`serviceAccount:${CALLER_SA}`)) {
              hasServiceAccountUser = true;
              break;
            }
          }
        }
      }
    } catch (error: any) {
      console.error(`   ⚠️  Failed to check SA IAM policy: ${error.message}`);
    }
  }
  const targetLabel = TARGET_SA_EMAIL || 'target';
  const serviceAccountUserStatus = hasServiceAccountUser ? '✅ Yes' : '❌ No';
  console.log(`   Does ${CALLER_SA} have roles/iam.serviceAccountUser on ${targetLabel}: ${serviceAccountUserStatus}`);
  console.log('');

  // Check storage.objectCreator permission
  console.log('5. STORAGE PERMISSIONS:');
  const hasStorageObjectCreator = callerRoles.some((r) => r.includes('storage.objectCreator'));
  const storageStatus = hasStorageObjectCreator ? '✅ Yes' : '❌ No';
  console.log(`   Does ${CALLER_SA} have roles/storage.objectCreator (project level): ${storageStatus}`);

  // Check Cloud Build staging bucket
  const stagingBucket = `gs://${PROJECT_ID}_cloudbuild`;
  try {
    shellOrFail(`gsutil ls -L ${stagingBucket}`);
    console.log(`   ✅ Staging bucket exists: ${stagingBucket}`);
    try {
      const bucketIAM = shellJSON<IAMPolicy>(`gsutil iam get ${stagingBucket}`);
      console.log(`   Bucket IAM bindings: ${bucketIAM?.bindings?.length || 0} bindings`);
    } catch (error: any) {
      console.error(`   ⚠️  Cannot read bucket IAM: ${error.message}`);
    }
  } catch (error: any) {
    console.log(`   ⚠️  Staging bucket may not exist or is inaccessible: ${stagingBucket}`);
  }
  console.log('');

  // Check enabled APIs
  console.log('6. ENABLED APIs:');
  const requiredAPIs = [
    'cloudbuild.googleapis.com',
    'artifactregistry.googleapis.com',
    'run.googleapis.com',
    'cloudscheduler.googleapis.com',
    'secretmanager.googleapis.com',
    'cloudresourcemanager.googleapis.com',
  ];
  let enabledAPIs: string[] = [];
  try {
    const services = shellJSON<Array<{ name?: string; state?: string }>>(
      `gcloud services list --project=${PROJECT_ID} --format=json`,
    );
    enabledAPIs = (services || []).filter((s) => s.state === 'ENABLED').map((s) => s.name || '');
  } catch (error: any) {
    console.error(`   ⚠️  Failed to list services: ${error.message}`);
  }

  const missingAPIs = requiredAPIs.filter((api) => !enabledAPIs.includes(api));
  for (const api of requiredAPIs) {
    const enabled = enabledAPIs.includes(api);
    console.log(`   ${enabled ? '✅' : '❌'} ${api}`);
  }
  if (missingAPIs.length > 0) {
    console.log(`   Missing APIs: ${missingAPIs.join(', ')}`);
  }
  console.log('');

  // Check Artifact Registry repo IAM
  console.log('7. ARTIFACT REGISTRY PERMISSIONS:');
  const repoPath = `projects/${PROJECT_ID}/locations/us-central1/repositories/ncc`;
  try {
    const repoIAM = shellJSON<IAMPolicy>(
      `gcloud artifacts repositories get-iam-policy ncc --location=us-central1 --project=${PROJECT_ID} --format=json`,
    );
    if (repoIAM?.bindings) {
      console.log(`   Repository IAM bindings: ${repoIAM.bindings.length} bindings`);
      for (const binding of repoIAM.bindings) {
        if (binding.members && binding.members.some((m) => m.includes(CLOUDBUILD_SA))) {
          console.log(`     Cloud Build SA has: ${binding.role}`);
        }
      }
    }
  } catch (error: any) {
    console.error(`   ⚠️  Failed to check Artifact Registry IAM: ${error.message}`);
  }
  console.log('');

  // Summary
  console.log('---');
  console.log('SUMMARY:');
  console.log(`ACT-AS target: ${TARGET_SA_EMAIL || `unknown (uniqueId: ${TARGET_UNIQUE_ID})`}`);
  console.log(
    `ServiceAccountUser on target: ${hasServiceAccountUser ? '✅ Yes' : '❌ No'} ${TARGET_SA_EMAIL ? `(on ${TARGET_SA_EMAIL})` : ''}`,
  );
  console.log(`Storage.objectCreator: ${hasStorageObjectCreator ? '✅ Yes' : '❌ No'} (project level)`);
  console.log(`Missing APIs: ${missingAPIs.length > 0 ? `❌ ${missingAPIs.join(', ')}` : '✅ None'}`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/discover-inventory.ts">
import 'dotenv/config';
import { getBigQuery } from '../../src/bq/client';
import { execSync } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

interface CloudRunJob {
  name?: string;
  spec?: {
    template?: {
      spec?: {
        containers?: Array<{
          image?: string;
        }>;
      };
    };
  };
  metadata?: {
    creationTimestamp?: string;
    annotations?: {
      'run.googleapis.com/launch-stage'?: string;
    };
  };
}

interface CloudRunService {
  metadata?: {
    name?: string;
    creationTimestamp?: string;
  };
  spec?: {
    template?: {
      spec?: {
        containers?: Array<{
          image?: string;
        }>;
      };
    };
  };
}

interface SchedulerJob {
  name?: string;
  schedule?: string;
  httpTarget?: {
    uri?: string;
  };
  pubsubTarget?: {
    topic?: string;
  };
}

interface ServiceAccount {
  email?: string;
  displayName?: string;
}

interface Secret {
  name?: string;
}

interface BigQueryTable {
  tableReference?: {
    tableId?: string;
  };
}

async function runCommand(cmd: string, silent = false, issues?: string[]): Promise<string | null> {
  try {
    if (silent) {
      // Capture stdout, stderr will be in error object if command fails
      const result = execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] });
      return result.trim();
    } else {
      const result = execSync(cmd, { encoding: 'utf8', stdio: 'inherit' });
      return result.trim();
    }
  } catch (error: any) {
    // Capture stderr - extract first error line
    let errorLine = '';
    if (error.stderr) {
      const stderrLines = error.stderr.toString().split('\n').filter((line: string) => line.trim());
      if (stderrLines.length > 0) {
        errorLine = stderrLines[0];
      }
    }
    if (!errorLine && error.message) {
      // Fallback: use error message if stderr not available
      const errorLines = error.message.split('\n').filter((line: string) => line.trim());
      if (errorLines.length > 0) {
        errorLine = errorLines[0];
      }
    }
    if (issues && errorLine) {
      issues.push(errorLine);
    }
    if (!silent) {
      console.error(`Command failed: ${cmd}`);
      console.error(`Error: ${error.message}`);
    }
    return null;
  }
}

async function queryBigQuery(query: string, timeout = 10000): Promise<any[]> {
  const bq = getBigQuery();
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const location = process.env.BQ_LOCATION || 'US';

  try {
    const [rows] = await Promise.race([
      bq.query({ query, location }),
      new Promise<any[]>((_, reject) =>
        setTimeout(() => reject(new Error('Query timeout')), timeout)
      ),
    ]);
    return rows as any[];
  } catch (error: any) {
    console.error(`BigQuery query failed: ${error.message}`);
    return [];
  }
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Execute commands and generate inventory doc',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID;
  const BQ_LOC = process.env.BQ_LOCATION || 'US';
  const RUN_REGION = process.env.NCC_REGION || (BQ_LOC.toUpperCase() === 'US' ? 'us-central1' : BQ_LOC);

  if (!PROJECT) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const isApply = argv.apply;

  // Build impersonation flags if NCC_IMPERSONATE_SA is set
  const impersonateFlags = process.env.NCC_IMPERSONATE_SA
    ? ['--impersonate-service-account', process.env.NCC_IMPERSONATE_SA]
    : [];
  const impersonateStr = impersonateFlags.length > 0 ? impersonateFlags.join(' ') + ' ' : '';

  // Define commands with impersonation support
  const commands = {
    cloudRunJobs: `gcloud ${impersonateStr}run jobs list --region ${RUN_REGION} --project ${PROJECT} --format=json`,
    cloudRunServices: `gcloud ${impersonateStr}run services list --region ${RUN_REGION} --project ${PROJECT} --format=json`,
    scheduler: `gcloud ${impersonateStr}scheduler jobs list --location ${RUN_REGION} --project ${PROJECT} --format=json`,
    gcrImages: `gcloud ${impersonateStr}container images list --repository=gcr.io/${PROJECT} --format=json || true`,
    artifactRepos: `gcloud ${impersonateStr}artifacts repositories list --project ${PROJECT} --format=json || true`,
    serviceAccounts: `gcloud ${impersonateStr}iam service-accounts list --project ${PROJECT} --format=json`,
    iamPolicy: `gcloud ${impersonateStr}projects get-iam-policy ${PROJECT} --format=json`,
    secrets: `gcloud ${impersonateStr}secrets list --project ${PROJECT} --format=json`,
  };

  if (!isApply) {
    // Preview mode: print commands
    console.log('---');
    console.log('CLOUD INVENTORY DISCOVERY (PREVIEW)');
    console.log('');
    console.log('Project:', PROJECT);
    console.log('BigQuery Location:', BQ_LOC);
    console.log('Cloud Run Region:', RUN_REGION);
    console.log('');
    console.log('Commands that would be executed:');
    console.log('');
    for (const [name, cmd] of Object.entries(commands)) {
      console.log(`  ${name}:`);
      console.log(`    ${cmd}`);
    }
    console.log('');
    console.log('BigQuery queries:');
    console.log('  - List datasets matching /ncc/i');
    console.log('  - For ncc_production and ncc_newsletters: list tables and row counts');
    console.log('');
    console.log('To execute and generate docs/CLOUD_INVENTORY.md, run with --apply');
    console.log('---');
    return;
  }

  // Apply mode: execute commands and generate doc
  console.log('Discovering cloud inventory...');
  console.log('Project:', PROJECT);
  console.log('BigQuery Location:', BQ_LOC);
  console.log('Cloud Run Region:', RUN_REGION);
  console.log('');

  const issues: string[] = [];
  const inventory: any = {
    project: PROJECT,
    region: RUN_REGION,
    cloudRunJobs: [],
    cloudRunServices: [],
    scheduler: [],
    images: { gcr: [], artifacts: [] },
    serviceAccounts: [],
    iamRoles: {},
    secrets: [],
    bigquery: {},
  };

  // 1. Cloud Run Jobs
  console.log('Fetching Cloud Run Jobs...');
  const jobsJson = await runCommand(commands.cloudRunJobs, true, issues);
  if (jobsJson) {
    try {
      inventory.cloudRunJobs = JSON.parse(jobsJson);
    } catch (e) {
      issues.push('Failed to parse Cloud Run Jobs JSON');
    }
  } else {
    issues.push('Failed to fetch Cloud Run Jobs');
  }

  // 2. Cloud Run Services
  console.log('Fetching Cloud Run Services...');
  const servicesJson = await runCommand(commands.cloudRunServices, true, issues);
  if (servicesJson) {
    try {
      inventory.cloudRunServices = JSON.parse(servicesJson);
    } catch (e) {
      issues.push('Failed to parse Cloud Run Services JSON');
    }
  } else {
    issues.push('Failed to fetch Cloud Run Services');
  }

  // 3. Cloud Scheduler
  console.log('Fetching Cloud Scheduler jobs...');
  const schedulerJson = await runCommand(commands.scheduler, true, issues);
  if (schedulerJson) {
    try {
      inventory.scheduler = JSON.parse(schedulerJson);
    } catch (e) {
      issues.push('Failed to parse Cloud Scheduler JSON');
    }
  } else {
    issues.push('Failed to fetch Cloud Scheduler jobs');
  }

  // 4. Images
  console.log('Fetching container images...');
  const gcrJson = await runCommand(commands.gcrImages, true);
  if (gcrJson) {
    try {
      inventory.images.gcr = JSON.parse(gcrJson);
    } catch (e) {
      // Ignore errors for optional commands
    }
  }

  const artifactsJson = await runCommand(commands.artifactRepos, true);
  if (artifactsJson) {
    try {
      inventory.images.artifacts = JSON.parse(artifactsJson);
    } catch (e) {
      // Ignore errors for optional commands
    }
  }

  // 5. Service Accounts & IAM
  console.log('Fetching Service Accounts...');
  const saJson = await runCommand(commands.serviceAccounts, true, issues);
  if (saJson) {
    try {
      inventory.serviceAccounts = JSON.parse(saJson);
    } catch (e) {
      issues.push('Failed to parse Service Accounts JSON');
    }
  } else {
    issues.push('Failed to fetch Service Accounts');
  }

  console.log('Fetching IAM policy...');
  const iamJson = await runCommand(commands.iamPolicy, true, issues);
  if (iamJson) {
    try {
      const policy = JSON.parse(iamJson);
      // Summarize roles by service account
      const roleMap: Record<string, string[]> = {};
      if (policy.bindings) {
        for (const binding of policy.bindings) {
          const role = binding.role || '';
          const members = binding.members || [];
          for (const member of members) {
            if (member.startsWith('serviceAccount:')) {
              const saEmail = member.replace('serviceAccount:', '');
              if (!roleMap[saEmail]) {
                roleMap[saEmail] = [];
              }
              roleMap[saEmail].push(role);
            }
          }
        }
      }
      inventory.iamRoles = roleMap;
    } catch (e) {
      issues.push('Failed to parse IAM policy JSON');
    }
  } else {
    issues.push('Failed to fetch IAM policy');
  }

  // 6. Secrets
  console.log('Fetching Secrets...');
  const secretsJson = await runCommand(commands.secrets, true, issues);
  if (secretsJson) {
    try {
      inventory.secrets = JSON.parse(secretsJson);
    } catch (e) {
      issues.push('Failed to parse Secrets JSON');
    }
  } else {
    issues.push('Failed to fetch Secrets');
  }

  // 7. BigQuery
  console.log('Querying BigQuery...');
  const bq = getBigQuery();
  try {
    const [datasets] = await bq.getDatasets();
    const nccDatasets = datasets.filter((ds) => /ncc/i.test(ds.id || ''));
    inventory.bigquery.datasets = nccDatasets.map((ds) => ds.id);

    // For key datasets, get tables and row counts
    for (const datasetId of ['ncc_production', 'ncc_newsletters']) {
      if (!nccDatasets.find((d) => d.id === datasetId)) {
        continue;
      }

      const dataset = bq.dataset(datasetId);
      const [tables] = await dataset.getTables();

      inventory.bigquery[datasetId] = {
        tables: [],
      };

      for (const table of tables) {
        const tableId = table.id || '';
        let rowCount: string | number = 'unknown';

        try {
          const countQuery = `SELECT COUNT(*) AS cnt FROM \`${PROJECT}.${datasetId}.${tableId}\``;
          const rows = await queryBigQuery(countQuery, 10000);
          if (rows.length > 0) {
            const count = (rows[0] as { cnt: number }).cnt;
            rowCount = count > 100000 ? '>100k rows' : count;
          }
        } catch (e) {
          rowCount = 'error';
        }

        inventory.bigquery[datasetId].tables.push({
          name: tableId,
          rowCount,
        });
      }
    }
  } catch (e: any) {
    issues.push(`BigQuery discovery failed: ${e.message}`);
  }

  // Generate markdown document
  const docPath = path.resolve(__dirname, '../../docs/CLOUD_INVENTORY.md');
  let markdown = `# Cloud Inventory (Generated)

**Generated:** ${new Date().toISOString()}
**Project:** ${PROJECT}
**BigQuery Location:** ${BQ_LOC}
**Cloud Run Region:** ${RUN_REGION}

`;

  if (issues.length > 0) {
    markdown += `## ⚠️ Issues Encountered

`;
    for (const issue of issues) {
      markdown += `- ${issue}\n`;
    }
    markdown += `\n`;
  }

  markdown += `## Cloud Run Jobs

| Name | Image | Created | Updated |
|------|-------|---------|---------|
`;
  for (const job of inventory.cloudRunJobs as CloudRunJob[]) {
    const name = job.name?.split('/').pop() || 'unknown';
    const image =
      job.spec?.template?.spec?.containers?.[0]?.image || 'unknown';
    const created = job.metadata?.creationTimestamp || 'unknown';
    const updated = job.metadata?.annotations?.['run.googleapis.com/launch-stage'] || 'unknown';
    markdown += `| ${name} | ${image} | ${created} | ${updated} |\n`;
  }

  markdown += `\n## Cloud Run Services

| Name | Image | Created |
|------|-------|---------|
`;
  for (const service of inventory.cloudRunServices as CloudRunService[]) {
    const name = service.metadata?.name || 'unknown';
    const image =
      service.spec?.template?.spec?.containers?.[0]?.image || 'unknown';
    const created = service.metadata?.creationTimestamp || 'unknown';
    markdown += `| ${name} | ${image} | ${created} |\n`;
  }

  markdown += `\n## Cloud Scheduler

| Name | Schedule | Target |
|------|----------|--------|
`;
  for (const job of inventory.scheduler as SchedulerJob[]) {
    const name = job.name?.split('/').pop() || 'unknown';
    const schedule = job.schedule || 'unknown';
    const target = job.httpTarget?.uri || job.pubsubTarget?.topic || 'unknown';
    markdown += `| ${name} | ${schedule} | ${target} |\n`;
  }

  markdown += `\n## Container Images

### GCR Images (gcr.io/${PROJECT})

`;
  if (inventory.images.gcr.length > 0) {
    for (const img of inventory.images.gcr as any[]) {
      markdown += `- ${img.name || 'unknown'}\n`;
    }
  } else {
    markdown += `_None found_\n`;
  }

  markdown += `\n### Artifact Registry Repositories

`;
  if (inventory.images.artifacts.length > 0) {
    for (const repo of inventory.images.artifacts as any[]) {
      markdown += `- ${repo.name || 'unknown'}\n`;
    }
  } else {
    markdown += `_None found_\n`;
  }

  markdown += `\n## Service Accounts

| Email | Display Name | Key Roles |
|-------|--------------|-----------|
`;
  for (const sa of inventory.serviceAccounts as ServiceAccount[]) {
    const email = sa.email || 'unknown';
    const displayName = sa.displayName || '-';
    const roles = inventory.iamRoles[email] || [];
    const keyRoles = roles.slice(0, 3).join(', ') + (roles.length > 3 ? '...' : '');
    markdown += `| ${email} | ${displayName} | ${keyRoles || '-'} |\n`;
  }

  markdown += `\n## Secrets

`;
  if (inventory.secrets.length > 0) {
    for (const secret of inventory.secrets as Secret[]) {
      markdown += `- ${secret.name || 'unknown'}\n`;
    }
  } else {
    markdown += `_None found_\n`;
  }

  markdown += `\n## BigQuery

### Datasets (matching /ncc/i)

`;
  if (inventory.bigquery.datasets) {
    for (const dataset of inventory.bigquery.datasets) {
      markdown += `- ${dataset}\n`;
    }
  }

  for (const datasetId of ['ncc_production', 'ncc_newsletters']) {
    if (!inventory.bigquery[datasetId]) {
      continue;
    }

    markdown += `\n### ${datasetId}

| Table | Row Count |
|-------|-----------|
`;
    for (const table of inventory.bigquery[datasetId].tables) {
      markdown += `| ${table.name} | ${table.rowCount} |\n`;
    }
  }

  markdown += `\n## Notes

`;
  const notes: string[] = [];
  if (issues.length > 0) {
    notes.push(`Some discovery operations encountered errors (see Issues section above).`);
  }
  if (inventory.cloudRunJobs.length === 0 && inventory.cloudRunServices.length === 0) {
    notes.push(`No Cloud Run resources found in region ${RUN_REGION}.`);
  }
  if (notes.length === 0) {
    notes.push(`No anomalies detected.`);
  }
  for (const note of notes) {
    markdown += `- ${note}\n`;
  }

  // Write file
  await fs.writeFile(docPath, markdown, 'utf8');
  console.log(`\nInventory written to: ${docPath}`);
  if (issues.length > 0) {
    console.log(`\n⚠️  ${issues.length} issue(s) encountered during discovery.`);
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/ensure-iam.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string): T | null {
  try {
    const output = shell(cmd);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

interface IAMBinding {
  role?: string;
  members?: string[];
}

interface IAMPolicy {
  bindings?: IAMBinding[];
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually apply IAM bindings (default: preview)',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';
  const SERVICE_NAME = 'ncc-jobs-runner';

  console.log('---');
  console.log('ENSURE IAM PERMISSIONS');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Service Account: ${SA}`);
  console.log(`Runner Service: ${SERVICE_NAME}`);
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'PREVIEW'}`);
  console.log('');

  // Check 1: Project-level role for running Cloud Run Jobs
  console.log('1. Checking project-level IAM for Cloud Run Jobs execution...');
  let projectPolicy: IAMPolicy | null = null;
  try {
    projectPolicy = shellJSON<IAMPolicy>(`gcloud projects get-iam-policy ${PROJECT} --format=json`);
  } catch (error: any) {
    console.error(`   ❌ Failed to get project IAM policy: ${error.message}`);
    process.exit(1);
  }

  let hasRunDeveloper = false;
  if (projectPolicy?.bindings) {
    for (const binding of projectPolicy.bindings) {
      if (binding.role === 'roles/run.developer' && binding.members) {
        const member = `serviceAccount:${SA}`;
        if (binding.members.includes(member)) {
          hasRunDeveloper = true;
          break;
        }
      }
    }
  }

  const projectCmd = `gcloud projects add-iam-policy-binding ${PROJECT} \\
  --member=serviceAccount:${SA} \\
  --role=roles/run.developer`;

  if (hasRunDeveloper) {
    console.log(`   ✅ PASS: Service account has roles/run.developer at project level`);
  } else {
    console.log(`   ❌ FAIL: Service account missing roles/run.developer at project level`);
    if (!argv.apply) {
      console.log(`   Would run: ${projectCmd.split('\\')[0]}...`);
    }
  }
  console.log('');

  // Check 2: Service-level invoker role for Scheduler
  console.log('2. Checking service-level IAM for runner service invocation...');

  let servicePolicy: IAMPolicy | null = null;
  try {
    servicePolicy = shellJSON<IAMPolicy>(
      `gcloud run services get-iam-policy ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format=json`,
    );
  } catch (error: any) {
    const errorMsg = error.message || String(error);
    if (errorMsg.includes('not found') || errorMsg.includes('404')) {
      console.log(`   ⚠️  Service ${SERVICE_NAME} does not exist yet. Deploy it first with: npm run cloud:runner:apply`);
      console.log(`   Will skip this check until service exists.`);
    } else {
      console.error(`   ❌ Failed to get service IAM policy: ${errorMsg}`);
    }
    servicePolicy = null;
  }

  let hasRunInvoker = false;
  if (servicePolicy?.bindings) {
    for (const binding of servicePolicy.bindings) {
      if (binding.role === 'roles/run.invoker' && binding.members) {
        const member = `serviceAccount:${SA}`;
        if (binding.members.includes(member)) {
          hasRunInvoker = true;
          break;
        }
      }
    }
  }

  const serviceCmd = `gcloud run services add-iam-policy-binding ${SERVICE_NAME} \\
  --region=${REGION} \\
  --project=${PROJECT} \\
  --member=serviceAccount:${SA} \\
  --role=roles/run.invoker`;

  if (servicePolicy === null) {
    // Service doesn't exist, skip
    console.log(`   ⚠️  SKIP: Service does not exist yet`);
  } else if (hasRunInvoker) {
    console.log(`   ✅ PASS: Service account has roles/run.invoker on ${SERVICE_NAME}`);
  } else {
    console.log(`   ❌ FAIL: Service account missing roles/run.invoker on ${SERVICE_NAME}`);
    if (!argv.apply) {
      console.log(`   Would run: ${serviceCmd.split('\\')[0]}...`);
    }
  }
  console.log('');

  // Summary and apply
  const needsFix = !hasRunDeveloper || (servicePolicy !== null && !hasRunInvoker);

  if (!needsFix) {
    console.log('---');
    console.log('✅ All IAM permissions are correct!');
    console.log('---');
    return;
  }

  if (!argv.apply) {
    console.log('---');
    console.log('PREVIEW: The following commands would be executed:');
    console.log('');
    if (!hasRunDeveloper) {
      console.log(projectCmd);
      console.log('');
    }
    if (servicePolicy !== null && !hasRunInvoker) {
      console.log(serviceCmd);
      console.log('');
    }
    console.log('Run with --apply to execute these commands.');
    console.log('---');
    return;
  }

  // Apply mode
  console.log('---');
  console.log('APPLY MODE: Fixing IAM permissions...');
  console.log('');

  // Check if we need to switch to human user for IAM modifications
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }

  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com') && (!hasRunDeveloper || (servicePolicy !== null && !hasRunInvoker));

  if (needsHumanAuth) {
    console.log('Authenticated as service account. IAM modifications require human user.');
    console.log('Switching to human user...');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    shell(`gcloud config set project ${PROJECT}`);
    console.log('✅ Switched to human user');
    console.log('');
  }

  if (!hasRunDeveloper) {
    console.log('Granting roles/run.developer at project level...');
    try {
      execSync(`gcloud projects add-iam-policy-binding ${PROJECT} --member=serviceAccount:${SA} --role=roles/run.developer`, { stdio: 'inherit' });
      console.log('✅ Granted roles/run.developer');
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      if (errorMsg.includes('already has') || errorMsg.includes('already exists')) {
        console.log('✅ Permission already present');
      } else {
        console.error(`❌ Failed: ${errorMsg}`);
        process.exit(1);
      }
    }
    console.log('');
  }

  if (servicePolicy !== null && !hasRunInvoker) {
    console.log(`Granting roles/run.invoker on ${SERVICE_NAME}...`);
    try {
      execSync(`gcloud run services add-iam-policy-binding ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --member=serviceAccount:${SA} --role=roles/run.invoker`, { stdio: 'inherit' });
      console.log('✅ Granted roles/run.invoker');
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      if (errorMsg.includes('already has') || errorMsg.includes('already exists')) {
        console.log('✅ Permission already present');
      } else {
        console.error(`❌ Failed: ${errorMsg}`);
        process.exit(1);
      }
    }
    console.log('');
  }

  // Switch back to service account if we switched
  if (needsHumanAuth) {
    const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
    console.log('Switching back to service account...');
    execSync(`gcloud auth activate-service-account ${SA} --key-file "${KEY}" --project ${PROJECT}`, { stdio: 'inherit' });
    console.log('✅ Switched back to service account');
    console.log('');
  }

  console.log('---');
  console.log('✅ IAM permissions check complete!');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/fix-build-perms.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import * as fs from 'fs';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string): T | null {
  try {
    const output = shell(cmd);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

interface IAMBinding {
  role?: string;
  members?: string[];
}

interface IAMPolicy {
  bindings?: IAMBinding[];
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually apply the changes (default: dry-run)',
    })
    .parse();

  // Resolve project ID
  let PROJECT_ID = process.env.BQ_PROJECT_ID;
  if (!PROJECT_ID) {
    PROJECT_ID = shell('gcloud config get-value project');
  }
  if (!PROJECT_ID) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve project number
  const PROJECT_NUMBER = shell(`gcloud projects describe ${PROJECT_ID} --format='value(projectNumber)'`);
  if (!PROJECT_NUMBER) {
    throw new Error('Failed to get project number');
  }

  // Resolve caller SA from credentials
  const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  if (!fs.existsSync(KEY)) {
    throw new Error(`Key file not found: ${KEY}`);
  }

  const keyContent = JSON.parse(fs.readFileSync(KEY, 'utf8'));
  const CALLER_SA = keyContent.client_email;
  if (!CALLER_SA) {
    throw new Error('client_email missing in key file');
  }

  // Compute target SA
  const TARGET_SA = `${PROJECT_NUMBER}-compute@developer.gserviceaccount.com`;

  console.log('---');
  console.log('FIX BUILD PERMISSIONS');
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'DRY-RUN'}`);
  console.log(`Project ID: ${PROJECT_ID}`);
  console.log(`Project Number: ${PROJECT_NUMBER}`);
  console.log(`Caller SA: ${CALLER_SA}`);
  console.log(`Target SA: ${TARGET_SA}`);
  console.log('');

  // Validate all values
  if (!PROJECT_ID || !PROJECT_NUMBER || !CALLER_SA || !TARGET_SA) {
    throw new Error('Missing required values');
  }

  // Check current IAM policy on target SA
  console.log(`Checking IAM policy on ${TARGET_SA}...`);
  const policy = shellJSON<IAMPolicy>(`gcloud iam service-accounts get-iam-policy ${TARGET_SA} --format=json`);

  let hasPermission = false;
  if (policy?.bindings) {
    for (const binding of policy.bindings) {
      if (binding.role === 'roles/iam.serviceAccountUser' && binding.members) {
        const member = `serviceAccount:${CALLER_SA}`;
        if (binding.members.includes(member)) {
          hasPermission = true;
          break;
        }
      }
    }
  }

  if (hasPermission) {
    console.log('✅ Permission already present');
    console.log('---');
    return;
  }

  console.log('❌ Permission missing');
  console.log('');

  if (!argv.apply) {
    // Dry-run mode: print what would be done
    console.log('DRY-RUN: Would execute the following:');
    console.log('');
    console.log('1. Check current gcloud account');
    console.log('2. If authenticated as service account, switch to human user:');
    console.log('   gcloud auth login --update-adc');
    console.log(`   gcloud config set project ${PROJECT_ID}`);
    console.log('');
    console.log('3. Grant permission:');
    console.log(`   gcloud iam service-accounts add-iam-policy-binding ${TARGET_SA} \\`);
    console.log(`     --member=serviceAccount:${CALLER_SA} \\`);
    console.log(`     --role=roles/iam.serviceAccountUser \\`);
    console.log(`     --project=${PROJECT_ID}`);
    console.log('');
    console.log('4. Switch back to service account:');
    console.log(`   gcloud auth activate-service-account ${CALLER_SA} --key-file "${KEY}" --project ${PROJECT_ID}`);
    console.log('');
    console.log('5. Verify permission was granted');
    console.log('');
    console.log('Run with --apply to execute these steps.');
    console.log('---');
    return;
  }

  // Apply mode: execute the fix
  console.log('APPLY MODE: Executing fix...');
  console.log('');

  // Step 1: Check current account
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }

  console.log(`Current gcloud account: ${currentAccount || '(unknown)'}`);

  // Step 2: Switch to human user if needed
  if (currentAccount.endsWith('.iam.gserviceaccount.com')) {
    console.log('Authenticated as service account, switching to human user...');
    console.log('(A browser window will open for OAuth login)');
    const loginResult = spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], {
      stdio: 'inherit',
    });
    if (loginResult.status !== 0) {
      console.error('');
      console.error('❌ OAuth login failed or was canceled.');
      console.error('Please complete the login in your browser, then run this script again with --apply.');
      process.exit(1);
    }
    shell(`gcloud config set project ${PROJECT_ID}`);
    
    // Verify we switched
    const newAccount = shell('gcloud config get-value account');
    if (!newAccount || newAccount.endsWith('.iam.gserviceaccount.com')) {
      console.error('❌ Still authenticated as service account after login.');
      console.error('Please ensure you completed the browser login, then run this script again.');
      process.exit(1);
    }
    console.log(`✅ Switched to human user: ${newAccount}`);
  } else {
    console.log('✅ Already authenticated as human user');
    // Ensure project is set
    shell(`gcloud config set project ${PROJECT_ID}`);
  }
  console.log('');

  // Step 3: Grant permission
  console.log('Granting permission...');
  try {
    shell(
      `gcloud iam service-accounts add-iam-policy-binding ${TARGET_SA} --member=serviceAccount:${CALLER_SA} --role=roles/iam.serviceAccountUser --project=${PROJECT_ID}`,
    );
    console.log('✅ Permission granted');
  } catch (error: any) {
    const errorMsg = error.message || String(error);
    if (errorMsg.includes('already has') || errorMsg.includes('already exists')) {
      console.log('✅ Permission already present (from previous run)');
    } else {
      throw error;
    }
  }
  console.log('');

  // Step 4: Switch back to service account
  console.log('Switching back to service account...');
  shell(`gcloud auth activate-service-account ${CALLER_SA} --key-file "${KEY}" --project ${PROJECT_ID}`);
  console.log('✅ Switched back to service account');
  console.log('');

  // Step 5: Verify
  console.log('Verifying permission...');
  const verifyPolicy = shellJSON<IAMPolicy>(`gcloud iam service-accounts get-iam-policy ${TARGET_SA} --format=json`);

  let verified = false;
  if (verifyPolicy?.bindings) {
    for (const binding of verifyPolicy.bindings) {
      if (binding.role === 'roles/iam.serviceAccountUser' && binding.members) {
        const member = `serviceAccount:${CALLER_SA}`;
        if (binding.members.includes(member)) {
          verified = true;
          break;
        }
      }
    }
  }

  if (verified) {
    console.log('✅ SA User granted and verified. OK to retry build.');
  } else {
    console.error('❌ Permission still missing after grant - this should not happen');
    process.exit(1);
  }

  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/gcloud-doctor.ts">
import 'dotenv/config';
import { execSync } from 'child_process';

interface AuthAccount {
  account?: string;
  status?: string;
}

function runCommand(cmd: string, silent = true): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: silent ? 'pipe' : 'inherit' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    return { success: false, output: error.message || String(error) };
  }
}

async function main(): Promise<void> {
  console.log('---');
  console.log('GCLOUD DOCTOR');
  console.log('');

  // Check if gcloud is installed
  const whichResult = runCommand('which gcloud');
  const isInstalled = whichResult.success && whichResult.output.length > 0;

  if (!isInstalled) {
    console.log('❌ gcloud: NOT INSTALLED');
    console.log('');
    console.log('Install gcloud CLI: https://cloud.google.com/sdk/docs/install');
    console.log('---');
    process.exit(1);
  }

  console.log('✅ gcloud: INSTALLED');
  console.log('');

  // Get version
  const versionResult = runCommand('gcloud --version', true);
  if (versionResult.success) {
    const firstLine = versionResult.output.split('\n')[0];
    console.log(`Version: ${firstLine}`);
  } else {
    console.log('Version: (error)');
  }
  console.log('');

  // Get current project
  const projectResult = runCommand('gcloud config get-value project', true);
  const currentProject = projectResult.success && projectResult.output.length > 0
    ? projectResult.output
    : '(unset)';
  console.log(`Project: ${currentProject}`);
  console.log('');

  // Get run/region
  const regionResult = runCommand('gcloud config get-value run/region', true);
  const currentRegion = regionResult.success && regionResult.output.length > 0
    ? regionResult.output
    : '(unset)';
  console.log(`Run Region: ${currentRegion}`);
  console.log('');

  // Get active account
  const authResult = runCommand('gcloud auth list --format=json', true);
  let activeAccount = 'none';
  let hasActiveAccount = false;

  if (authResult.success) {
    try {
      const accounts = JSON.parse(authResult.output) as AuthAccount[];
      const active = accounts.find((acc) => acc.status === 'ACTIVE');
      if (active && active.account) {
        activeAccount = active.account;
        hasActiveAccount = true;
      }
    } catch (e) {
      // Parse failed, try simple grep
      const lines = authResult.output.split('\n');
      for (const line of lines) {
        if (line.includes('ACTIVE') && line.includes('@')) {
          const match = line.match(/([^\s@]+@[^\s@]+)/);
          if (match) {
            activeAccount = match[1];
            hasActiveAccount = true;
            break;
          }
        }
      }
    }
  }

  console.log(`Active Account: ${activeAccount}`);
  console.log('');

  // Provide next steps
  if (!hasActiveAccount) {
    console.log('---');
    console.log('NEXT STEPS (no active account):');
    console.log('');
    console.log('1) gcloud auth login --update-adc');
    console.log('2) gcloud config set project newsletter-control-center');
    console.log('3) gcloud config set run/region us-central1');
    console.log('---');
  } else {
    console.log('---');
    console.log('✅ Active account found');
    console.log('');
    console.log('If discovery still fails, you may need service account impersonation:');
    console.log('');
    console.log('export NCC_IMPERSONATE_SA=\'<service-account>@newsletter-control-center.iam.gserviceaccount.com\'');
    console.log('');
    console.log('Then use: gcloud config set auth/impersonate_service_account $NCC_IMPERSONATE_SA');
    console.log('---');
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/grant-view-roles.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

interface ServiceAccountKey {
  client_email?: string;
}

const ROLES = [
  'roles/viewer',
  'roles/run.viewer',
  'roles/cloudscheduler.viewer',
  'roles/secretmanager.viewer',
  'roles/iam.serviceAccountViewer',
  'roles/logging.viewer',
];

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Execute the commands instead of previewing',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID;
  if (!PROJECT) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  // Get SA from env or parse from key
  let SA = process.env.NCC_IMPERSONATE_SA;
  if (!SA) {
    const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
    const resolvedKey = path.resolve(KEY);
    try {
      const content = await fs.readFile(resolvedKey, 'utf8');
      const key = JSON.parse(content) as ServiceAccountKey;
      if (key.client_email) {
        SA = key.client_email;
      } else {
        throw new Error('client_email not found in key file');
      }
    } catch (error: any) {
      throw new Error(`Failed to read service account: ${error.message}`);
    }
  }

  const commands = ROLES.map((role) => {
    return `gcloud projects add-iam-policy-binding ${PROJECT} \\\n  --member serviceAccount:${SA} \\\n  --role ${role}`;
  });

  if (!argv.apply) {
    // Preview mode
    console.log('Commands that would be executed:');
    console.log('');
    for (const cmd of commands) {
      console.log(cmd);
      console.log('');
    }
    console.log('To execute, run with --apply');
  } else {
    // Apply mode
    console.log(`Granting viewer roles to: ${SA}`);
    console.log('');

    for (let i = 0; i < commands.length; i++) {
      const role = ROLES[i];
      const command = `gcloud projects add-iam-policy-binding ${PROJECT} --member serviceAccount:${SA} --role ${role}`;

      try {
        execSync(command, { stdio: 'inherit' });
        console.log(`✅ Granted: ${role}`);
      } catch (error: any) {
        // Check if error is because role is already bound
        const errorMsg = error.message || String(error);
        if (errorMsg.includes('already exists') || errorMsg.includes('already bound')) {
          console.log(`⚠️  Already bound: ${role} (skipping)`);
        } else {
          console.error(`❌ Failed to grant ${role}: ${errorMsg}`);
          // Continue with next role
        }
      }
    }

    console.log('');
    console.log('Done.');
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/plan.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';

interface CloudRunJob {
  name?: string;
  metadata?: {
    name?: string;
  };
}

interface SchedulerJob {
  name?: string;
  schedule?: string;
}

async function main(): Promise<void> {
  // Resolve project
  let PROJECT = process.env.BQ_PROJECT_ID;
  if (!PROJECT) {
    try {
      PROJECT = execSync('gcloud config get-value project', { stdio: ['ignore', 'pipe', 'ignore'] }).toString().trim();
    } catch {
      PROJECT = '';
    }
  }
  if (!PROJECT) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve region
  const REGION = process.env.NCC_REGION || 'us-central1';

  // Fetch existing Cloud Run Jobs
  let existingRunJobs: CloudRunJob[] = [];
  let runJobsError: string | null = null;
  try {
    const cmd = `gcloud run jobs list --region ${REGION} --project ${PROJECT} --format=json`;
    const output = execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
    if (output) {
      existingRunJobs = JSON.parse(output);
    }
  } catch (error: any) {
    runJobsError = error.message || String(error);
    const stderr = error.stderr?.toString() || '';
    if (stderr) {
      const lines = stderr.split('\n').filter((l: string) => l.trim());
      if (lines.length > 0) {
        runJobsError = lines[0];
      }
    }
  }

  // Fetch existing Scheduler Jobs
  let existingSchedulerJobs: SchedulerJob[] = [];
  let schedulerError: string | null = null;
  try {
    const cmd = `gcloud scheduler jobs list --location ${REGION} --project ${PROJECT} --format=json`;
    const output = execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
    if (output) {
      existingSchedulerJobs = JSON.parse(output);
    }
  } catch (error: any) {
    schedulerError = error.message || String(error);
    const stderr = error.stderr?.toString() || '';
    if (stderr) {
      const lines = stderr.split('\n').filter((l: string) => l.trim());
      if (lines.length > 0) {
        schedulerError = lines[0];
      }
    }
  }

  // Build sets of existing names
  const existingRunJobNames = new Set<string>();
  for (const job of existingRunJobs) {
    const name = job.name || job.metadata?.name || '';
    if (name) {
      // Extract just the job name (remove path)
      const parts = name.split('/');
      existingRunJobNames.add(parts[parts.length - 1]);
    }
  }

  const existingSchedulerJobNames = new Set<string>();
  for (const job of existingSchedulerJobs) {
    const name = job.name || '';
    if (name) {
      // Extract just the job name (remove path)
      const parts = name.split('/');
      existingSchedulerJobNames.add(parts[parts.length - 1]);
    }
  }

  // Read SCHEDULING_PLAN.md
  const planPath = path.resolve(__dirname, '../../docs/SCHEDULING_PLAN.md');
  let planContent = '';
  try {
    planContent = await fs.readFile(planPath, 'utf8');
  } catch (error: any) {
    // Continue with defaults if file doesn't exist
  }

  // Extract desired jobs from plan (heuristic: look for job names or use defaults)
  const desiredRunJobs = [
    { name: 'ncc-ingest-me', cron: null as string | null },
    { name: 'ncc-ingest-other', cron: null as string | null },
    { name: 'ncc-chunks', cron: null as string | null },
    { name: 'ncc-embeddings', cron: null as string | null },
    { name: 'ncc-smoke', cron: null as string | null },
  ];

  const desiredSchedulerJobs = [
    { name: 'schedule-ncc-ingest-me', cron: '(cron TBD)' },
    { name: 'schedule-ncc-ingest-other', cron: '(cron TBD)' },
    { name: 'schedule-ncc-chunks', cron: '(cron TBD)' },
    { name: 'schedule-ncc-embeddings', cron: '(cron TBD)' },
    { name: 'schedule-ncc-smoke', cron: '(cron TBD)' },
  ];

  // Try to extract cron info from plan doc
  if (planContent) {
    // Look for ingest schedules (3x daily at 07:10, 12:10, 17:10 ET)
    if (/07:10.*?12:10.*?17:10/i.test(planContent) || /Schedule:.*?07:10.*?12:10.*?17:10/i.test(planContent)) {
      desiredSchedulerJobs[0].cron = '07:10,12:10,17:10 ET';
      // Assume other ingest is offset (could be 5 minutes later, but doc doesn't specify - mark as TBD)
      desiredSchedulerJobs[1].cron = '(cron TBD)';
    }

    // Look for chunk schedule (hourly :20)
    if (/hourly.*?:20/i.test(planContent) || /Schedule:.*?hourly.*?:20/i.test(planContent)) {
      desiredSchedulerJobs[2].cron = 'hourly :20';
    }

    // Look for embeddings schedule (hourly :35)
    if (/hourly.*?:35/i.test(planContent) || /Schedule:.*?hourly.*?:35/i.test(planContent)) {
      desiredSchedulerJobs[3].cron = 'hourly :35';
    }

    // Look for smoke schedule (18:00 ET)
    if (/18:00/i.test(planContent) && /smoke|Smoke/i.test(planContent)) {
      desiredSchedulerJobs[4].cron = '18:00 ET';
    }
  }

  // Print plan
  console.log('---');
  console.log('CLOUD PLAN (read-only)');
  console.log(`Project: ${PROJECT}  Region: ${REGION}`);
  console.log('');

  if (runJobsError) {
    console.log(`⚠️  Cloud Run Jobs listing failed: ${runJobsError}`);
    console.log('');
  }

  if (schedulerError) {
    console.log(`⚠️  Scheduler Jobs listing failed: ${schedulerError}`);
    console.log('');
  }

  console.log('Cloud Run Jobs:');
  for (const job of desiredRunJobs) {
    const status = existingRunJobNames.has(job.name) ? 'KEEP' : 'CREATE';
    const dots = '.'.repeat(Math.max(1, 30 - job.name.length));
    console.log(`  - ${job.name} ${dots} ${status}`);
  }
  console.log('');

  console.log('Cloud Scheduler Jobs:');
  for (const job of desiredSchedulerJobs) {
    const status = existingSchedulerJobNames.has(job.name) ? 'KEEP' : 'CREATE';
    const dots = '.'.repeat(Math.max(1, 35 - job.name.length - job.cron.length));
    console.log(`  - ${job.name} (cron: ${job.cron}) ${dots} ${status}`);
  }
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/print-key-sa.ts">
import 'dotenv/config';
import * as fs from 'fs/promises';
import * as path from 'path';

interface ServiceAccountKey {
  client_email?: string;
}

async function main(): Promise<void> {
  const keyPath = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  const resolvedPath = path.resolve(keyPath);

  let clientEmail = 'not found';

  try {
    const content = await fs.readFile(resolvedPath, 'utf8');
    const key = JSON.parse(content) as ServiceAccountKey;
    if (key.client_email) {
      clientEmail = key.client_email;
    }
  } catch (error: any) {
    // File not found or parse error - keep "not found"
  }

  console.log('---');
  console.log('SERVICE ACCOUNT FROM KEY');
  console.log(`client_email: ${clientEmail}`);
  console.log(`key_file: ${resolvedPath}`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/remediate-from-issues.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs/promises';
import * as path from 'path';
import * as fsSync from 'fs';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

interface ServiceAccountKey {
  client_email?: string;
}

function resolve(cmd: string): string {
  try {
    return execSync(cmd, { stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch {
    return '';
  }
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Execute the commands instead of previewing',
    })
    .parse();

  // Resolve project
  const PROJECT = process.env.BQ_PROJECT_ID || resolve('gcloud config get-value project');
  if (!PROJECT) {
    throw new Error('No project: set BQ_PROJECT_ID or run `gcloud config set project <id>`');
  }

  // Resolve SA from key file
  const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  const resolvedKey = path.resolve(KEY);
  
  if (!fsSync.existsSync(resolvedKey)) {
    throw new Error(`Key file does not exist: ${resolvedKey}`);
  }

  let SA = '';
  try {
    const content = await fs.readFile(resolvedKey, 'utf8');
    const key = JSON.parse(content) as ServiceAccountKey;
    if (!key.client_email) {
      throw new Error('client_email not found in key file');
    }
    SA = key.client_email;
  } catch (error: any) {
    throw new Error(`Failed to read service account from ${resolvedKey}: ${error.message}`);
  }

  // Load and parse issues from CLOUD_INVENTORY.md
  const docPath = path.resolve(__dirname, '../../docs/CLOUD_INVENTORY.md');
  let content: string;
  try {
    content = await fs.readFile(docPath, 'utf8');
  } catch (error: any) {
    throw new Error(`Failed to read ${docPath}: ${error.message}`);
  }

  const lines = content.split('\n');
  const issues: string[] = [];

  let inIssuesSection = false;
  let inNotesSection = false;

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];

    // Check for issues section header
    if (line.match(/^##\s+⚠️\s+Issues?\s+Encountered/i)) {
      inIssuesSection = true;
      inNotesSection = false;
      continue;
    }

    // Check for notes section header
    if (line.match(/^##\s+Notes?$/i)) {
      inNotesSection = true;
      inIssuesSection = false;
      continue;
    }

    // Stop at next section
    if (line.match(/^##\s+/) && (inIssuesSection || inNotesSection)) {
      break;
    }

    // Collect bullet points
    if ((inIssuesSection || inNotesSection) && line.trim().startsWith('- ')) {
      issues.push(line.trim().substring(2)); // Remove '- ' prefix
    }
  }

  if (issues.length === 0) {
    console.log('---');
    console.log('CLOUD REMEDIATION PLAN');
    console.log('No issues found in cloud inventory.');
    console.log('---');
    return;
  }

  // Build remediation commands
  const commands: string[] = [];
  const servicesToEnable: Set<string> = new Set();
  let needsAuth = false;
  let needsRoles = false;
  let needsCloudBuildRoles = false;

  for (const issue of issues) {
    // Check for permission errors
    if (/PERMISSION|not authorized|does not have permission/i.test(issue)) {
      needsRoles = true;
    }

    // Check for Cloud Build or storage object create errors
    if (/storage\.objects\.create|gcloud\.builds\.submit|Cloud Build/i.test(issue)) {
      needsCloudBuildRoles = true;
      needsRoles = true;
    }

    // Check for API not enabled errors
    if (/API .* not found|API .* has not been used|not enabled|UNIMPLEMENTED/i.test(issue)) {
      // Map to services based on context
      if (/run|Cloud Run/i.test(issue)) {
        servicesToEnable.add('run.googleapis.com');
      }
      if (/scheduler|Cloud Scheduler/i.test(issue)) {
        servicesToEnable.add('cloudscheduler.googleapis.com');
      }
      if (/secret|Secret Manager/i.test(issue)) {
        servicesToEnable.add('secretmanager.googleapis.com');
      }
      if (/iam|IAM|Admin/i.test(issue)) {
        servicesToEnable.add('iam.googleapis.com');
      }
      if (/artifact|Artifact Registry/i.test(issue)) {
        servicesToEnable.add('artifactregistry.googleapis.com');
      }
      if (/container|Container Registry|gcr/i.test(issue)) {
        servicesToEnable.add('containerregistry.googleapis.com');
      }
      if (/build|Cloud Build|gcloud builds/i.test(issue)) {
        servicesToEnable.add('cloudbuild.googleapis.com');
      }
    }

    // Check for authentication errors
    if (/UNAUTHENTICATED|needs login/i.test(issue)) {
      needsAuth = true;
    }
  }

  // Add auth command if needed
  if (needsAuth) {
    commands.push(`gcloud auth activate-service-account ${SA} --key-file ${resolvedKey} --project ${PROJECT}`);
  }

  // Add role binding commands if needed
  if (needsRoles) {
    const roles = [
      'roles/viewer',
      'roles/run.viewer',
      'roles/cloudscheduler.viewer',
      'roles/secretmanager.viewer',
      'roles/iam.serviceAccountViewer',
      'roles/logging.viewer',
      'roles/cloudbuild.builds.editor',
      'roles/storage.objectCreator',
    ];
    for (const role of roles) {
      commands.push(`gcloud projects add-iam-policy-binding ${PROJECT} --member serviceAccount:${SA} --role ${role}`);
    }
  }

  // Add service enable commands
  for (const service of Array.from(servicesToEnable).sort()) {
    commands.push(`gcloud services enable ${service} --project ${PROJECT}`);
  }

  // De-duplicate commands
  const uniqueCommands = Array.from(new Set(commands));

  if (!argv.apply) {
    // Preview mode
    console.log('---');
    console.log('CLOUD REMEDIATION PLAN (Preview)');
    console.log('');
    console.log(`Project: ${PROJECT}`);
    console.log(`Service Account: ${SA}`);
    console.log('');
    console.log('Issues detected:');
    for (const issue of issues) {
      console.log(`- ${issue}`);
    }
    console.log('');
    if (uniqueCommands.length === 0) {
      console.log('No remediation commands needed (issues may not be fixable via this script).');
    } else {
      console.log('Commands to run:');
      for (const cmd of uniqueCommands) {
        console.log(cmd);
      }
    }
    console.log('---');
  } else {
    // Apply mode
    console.log('---');
    console.log('CLOUD REMEDIATION (Applying)');
    console.log(`Project: ${PROJECT}`);
    console.log(`Service Account: ${SA}`);
    console.log('');

    let successCount = 0;
    let skipCount = 0;
    let errorCount = 0;

    for (const cmd of uniqueCommands) {
      try {
        execSync(cmd, { stdio: 'inherit' });
        successCount++;
      } catch (error: any) {
        const errorMsg = error.message || String(error);
        if (errorMsg.includes('already has') || errorMsg.includes('already enabled') || errorMsg.includes('already exists')) {
          console.log(`⚠️  Skipped (already applied): ${cmd.split(' ').slice(0, 3).join(' ')}...`);
          skipCount++;
        } else {
          console.error(`❌ Failed: ${cmd}`);
          errorCount++;
          // Continue with next command
        }
      }
    }

    console.log('');
    console.log(`Results: ${successCount} applied, ${skipCount} skipped, ${errorCount} errors`);
    console.log('');
    console.log('Now re-run: npm run cloud:discover:apply && npm run cloud:issues');
    console.log('---');
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/schedule-jobs.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

// Convert ET time to UTC cron
function etToUtcCron(hour: number, minute: number): string {
  // ET is UTC-5 (EST) or UTC-4 (EDT). We'll use UTC-5 for simplicity.
  // For EDT, adjust accordingly.
  let utcHour = hour + 5;
  if (utcHour >= 24) {
    utcHour -= 24;
  }
  return `${minute} ${utcHour} * * *`; // minute hour * * *
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually create scheduler jobs (default: preview)',
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SERVICE_NAME = 'ncc-jobs-runner';

  // Get runner service URL
  let runnerUrl = '';
  try {
    runnerUrl = shell(
      `gcloud run services describe ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format="value(status.url)"`,
    );
  } catch (error: any) {
    throw new Error(`Could not get runner service URL. Deploy the runner first with: npm run cloud:runner:apply`);
  }

  console.log('---');
  console.log('SCHEDULE CLOUD RUN JOBS');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Runner URL: ${runnerUrl}`);
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'PREVIEW'}`);
  console.log('');

  const schedules = [
    {
      name: 'schedule-ncc-chunks',
      job: 'ncc-chunks',
      description: 'Hourly at :20 ET',
      cron: '20 * * * *', // Every hour at :20
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-embeddings',
      job: 'ncc-embeddings',
      description: 'Hourly at :35 ET',
      cron: '35 * * * *', // Every hour at :35
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-smoke',
      job: 'ncc-smoke',
      description: 'Daily at 18:00 ET',
      cron: '0 18 * * *', // 18:00 ET (timezone handles DST automatically)
      timeZone: 'America/New_York',
    },
    // Ingest schedules (3x daily at 07:10, 12:10, 17:10 ET)
    {
      name: 'schedule-ncc-ingest-me-0710',
      job: 'ncc-ingest-me',
      description: 'Daily at 07:10 ET',
      cron: '10 12 * * *', // 07:10 ET = 12:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-ingest-me-1210',
      job: 'ncc-ingest-me',
      description: 'Daily at 12:10 ET',
      cron: '10 17 * * *', // 12:10 ET = 17:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-ingest-me-1710',
      job: 'ncc-ingest-me',
      description: 'Daily at 17:10 ET',
      cron: '10 22 * * *', // 17:10 ET = 22:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-ingest-other-0710',
      job: 'ncc-ingest-other',
      description: 'Daily at 07:10 ET',
      cron: '10 12 * * *', // 07:10 ET = 12:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-ingest-other-1210',
      job: 'ncc-ingest-other',
      description: 'Daily at 12:10 ET',
      cron: '10 17 * * *', // 12:10 ET = 17:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
    {
      name: 'schedule-ncc-ingest-other-1710',
      job: 'ncc-ingest-other',
      description: 'Daily at 17:10 ET',
      cron: '10 22 * * *', // 17:10 ET = 22:10 UTC (timezone handles DST)
      timeZone: 'America/New_York',
    },
  ];

  const commands: string[] = [];

  for (const schedule of schedules) {
    const payload = JSON.stringify({ job: schedule.job });

    const createCmd = `gcloud scheduler jobs create http ${schedule.name} \\
  --location=${REGION} \\
  --project=${PROJECT} \\
  --schedule="${schedule.cron}" \\
  --time-zone="${schedule.timeZone}" \\
  --uri="${runnerUrl}/run" \\
  --http-method=POST \\
  --headers="Content-Type=application/json" \\
  --message-body='${payload}' \\
  --oidc-service-account-email=newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com`;

    commands.push(createCmd);
  }

  if (!argv.apply) {
    // Preview mode
    console.log('PREVIEW: Would create the following scheduler jobs:');
    console.log('');
    for (let i = 0; i < schedules.length; i++) {
      const schedule = schedules[i];
      const cmd = commands[i];
      console.log(`${i + 1}. ${schedule.name} (${schedule.description})`);
      console.log(`   Command: ${cmd.split('\\')[0]}...`);
      console.log('');
    }
    console.log('Run with --apply to create these scheduler jobs.');
    console.log('---');
    return;
  }

  // Apply mode
  console.log('APPLY MODE: Creating scheduler jobs...');
  console.log('');

  // Check if we need to switch to human user for deployment
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }

  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com');

  if (needsHumanAuth) {
    console.log('Authenticated as service account. Deployment requires human user.');
    console.log('Switching to human user...');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    shell(`gcloud config set project ${PROJECT}`);
    console.log('✅ Switched to human user');
    console.log('');
  }

  for (let i = 0; i < schedules.length; i++) {
    const schedule = schedules[i];

    console.log(`Creating scheduler job: ${schedule.name}...`);

    const payload = JSON.stringify({ job: schedule.job });
    const createArgs = [
      'scheduler', 'jobs', 'create', 'http', schedule.name,
      `--location=${REGION}`,
      `--project=${PROJECT}`,
      `--schedule="${schedule.cron}"`,
      `--time-zone="${schedule.timeZone}"`,
      `--uri="${runnerUrl}/run"`,
      '--http-method=POST',
      '--headers=Content-Type=application/json',
      `--message-body='${payload}'`,
      `--oidc-service-account-email=newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com`,
    ];

    try {
      execSync(`gcloud ${createArgs.join(' ')}`, { stdio: 'inherit' });
      console.log(`✅ Created scheduler job: ${schedule.name}`);
    } catch (error: any) {
      const errorMsg = (error.stderr?.toString() || error.stdout?.toString() || error.message || String(error)).toLowerCase();
      if (errorMsg.includes('already exists') || errorMsg.includes('409')) {
        console.log(`   Scheduler job exists, updating...`);
        const updateArgs = createArgs.map(arg => arg.replace('create', 'update'));
        updateArgs[3] = 'update'; // Fix the command name
        try {
          execSync(`gcloud ${updateArgs.join(' ')}`, { stdio: 'inherit' });
          console.log(`✅ Updated scheduler job: ${schedule.name}`);
        } catch (updateError: any) {
          console.error(`❌ Failed to update scheduler job ${schedule.name}: ${updateError.message}`);
        }
      } else {
        console.error(`❌ Failed to create scheduler job ${schedule.name}`);
        console.error(error.stderr?.toString() || error.message || String(error));
      }
    }
    console.log('');
  }

  // Switch back to service account if we switched
  if (needsHumanAuth) {
    const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';
    const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
    console.log('Switching back to service account...');
    execSync(`gcloud auth activate-service-account ${SA} --key-file "${KEY}" --project ${PROJECT}`, { stdio: 'inherit' });
    console.log('✅ Switched back to service account');
    console.log('');
  }

  console.log('---');
  console.log('✅ Scheduler deployment complete!');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/scheduler-toggle.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string): T | null {
  try {
    const output = shell(cmd);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

// Job names matching schedule-jobs.ts
const SCHEDULER_JOBS = ['schedule-ncc-chunks', 'schedule-ncc-embeddings', 'schedule-ncc-smoke'];

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('enable', {
      type: 'boolean',
      default: false,
      description: 'Enable scheduler jobs',
    })
    .option('disable', {
      type: 'boolean',
      default: false,
      description: 'Disable scheduler jobs',
    })
    .option('all', {
      type: 'boolean',
      default: false,
      description: 'Apply to all scheduler jobs',
    })
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually apply changes (default: preview)',
    })
    .option('jobs', {
      type: 'array',
      string: true,
      description: 'Specific job names to enable/disable',
    })
    .check((argv) => {
      if (!argv.enable && !argv.disable) {
        throw new Error('Must specify either --enable or --disable');
      }
      if (argv.enable && argv.disable) {
        throw new Error('Cannot specify both --enable and --disable');
      }
      if (!argv.all && (!argv.jobs || argv.jobs.length === 0)) {
        throw new Error('Must specify either --all or --jobs <name1> <name2> ...');
      }
      return true;
    })
    .parse();

  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const action = argv.enable ? 'resume' : 'pause';
  const isEnable = argv.enable;

  // Determine which jobs to process
  let targetJobs: string[] = [];
  if (argv.all) {
    targetJobs = [...SCHEDULER_JOBS];
  } else if (argv.jobs) {
    // Validate job names
    for (const job of argv.jobs) {
      if (!SCHEDULER_JOBS.includes(job)) {
        throw new Error(`Invalid job name: ${job}. Valid names: ${SCHEDULER_JOBS.join(', ')}`);
      }
    }
    targetJobs = argv.jobs as string[];
  }

  console.log('---');
  console.log(`SCHEDULER TOGGLE: ${action.toUpperCase()}`);
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Jobs: ${targetJobs.join(', ')}`);
  console.log(`Mode: ${argv.apply ? 'APPLY' : 'PREVIEW'}`);
  console.log('');

  // Check current state of jobs
  let existingJobs: any[] = [];
  try {
    const listOutput = shell(`gcloud scheduler jobs list --location=${REGION} --project=${PROJECT} --format=json`);
    existingJobs = JSON.parse(listOutput);
  } catch (error: any) {
    console.warn('⚠️  Could not list existing jobs:', error.message);
  }

  // Build commands for each job
  const commands: Array<{ job: string; cmd: string; currentState: string }> = [];

  for (const jobName of targetJobs) {
    // Find existing job to check current state
    const existing = existingJobs.find((j: any) => j.name?.includes(jobName) || j.name?.endsWith(jobName));
    const currentState = existing?.state || 'UNKNOWN';

    const cmd = `gcloud scheduler jobs ${action} ${jobName} --location=${REGION} --project=${PROJECT}`;
    commands.push({ job: jobName, cmd, currentState });
  }

  if (!argv.apply) {
    // Preview mode
    console.log('PREVIEW: Would execute the following commands:');
    console.log('');
    for (const { job, cmd, currentState } of commands) {
      const status = currentState === 'ENABLED' ? 'ENABLED' : currentState === 'PAUSED' ? 'PAUSED' : 'UNKNOWN';
      console.log(`Job: ${job} (current state: ${status})`);
      console.log(`  ${cmd}`);
      console.log('');
    }
    console.log('Run with --apply to execute these commands.');
    console.log('---');
    return;
  }

  // Apply mode
  console.log('APPLY MODE: Applying changes...');
  console.log('');

  // Check if we need to switch to human user for scheduler operations
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }

  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com');

  if (needsHumanAuth) {
    console.log('Authenticated as service account. Scheduler operations require human user.');
    console.log('Switching to human user...');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    shell(`gcloud config set project ${PROJECT}`);
    console.log('✅ Switched to human user');
    console.log('');
  }

  for (const { job, cmd, currentState } of commands) {
    console.log(`${isEnable ? 'Enabling' : 'Disabling'} ${job}...`);
    
    try {
      execSync(cmd, { stdio: 'inherit' });
      console.log(`✅ ${isEnable ? 'Enabled' : 'Disabled'} ${job}`);
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      // Handle idempotent cases
      if (
        (isEnable && (errorMsg.includes('already enabled') || errorMsg.includes('already ENABLED') || errorMsg.includes('already RESUMED'))) ||
        (!isEnable && (errorMsg.includes('already paused') || errorMsg.includes('already PAUSED')))
      ) {
        console.log(`✅ ${job} is already ${isEnable ? 'enabled' : 'paused'}`);
      } else {
        console.error(`❌ Failed to ${action} ${job}: ${errorMsg}`);
      }
    }
    console.log('');
  }

  // Switch back to service account if we switched
  if (needsHumanAuth) {
    const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';
    const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
    console.log('Switching back to service account...');
    execSync(`gcloud auth activate-service-account ${SA} --key-file "${KEY}" --project ${PROJECT}`, { stdio: 'inherit' });
    console.log('✅ Switched back to service account');
    console.log('');
  }

  console.log('---');
  console.log(`✅ Scheduler ${action} complete!`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/show-issues.ts">
import 'dotenv/config';
import * as fs from 'fs/promises';
import * as path from 'path';

async function main(): Promise<void> {
  const docPath = path.resolve(__dirname, '../../docs/CLOUD_INVENTORY.md');

  let content: string;
  try {
    content = await fs.readFile(docPath, 'utf8');
  } catch (error: any) {
    console.log('---');
    console.log('CLOUD INVENTORY ISSUES');
    console.log('(file not found)');
    console.log('---');
    process.exit(1);
  }

  const lines = content.split('\n');
  const issues: string[] = [];

  let inIssuesSection = false;
  let inNotesSection = false;

  for (let i = 0; i < lines.length; i++) {
    const line = lines[i];

    // Check for issues section header
    if (line.match(/^##\s+⚠️\s+Issues?\s+Encountered/i)) {
      inIssuesSection = true;
      inNotesSection = false;
      continue;
    }

    // Check for notes section header
    if (line.match(/^##\s+Notes?$/i)) {
      inNotesSection = true;
      inIssuesSection = false;
      continue;
    }

    // Check for warning emoji at start of line
    if (line.trim().startsWith('⚠️')) {
      issues.push(line.trim());
      continue;
    }

    // Collect lines in issues section (bullet points)
    if (inIssuesSection) {
      // Stop at next section (##)
      if (line.match(/^##\s+/)) {
        inIssuesSection = false;
        continue;
      }
      // Collect bullet points
      if (line.trim().startsWith('- ')) {
        issues.push(line.trim().substring(2)); // Remove '- ' prefix
      }
    }

    // Collect lines in notes section (bullet points)
    if (inNotesSection) {
      // Stop at next section (##) or end of file
      if (line.match(/^##\s+/)) {
        inNotesSection = false;
        continue;
      }
      // Collect bullet points
      if (line.trim().startsWith('- ')) {
        issues.push(line.trim().substring(2)); // Remove '- ' prefix
      }
    }
  }

  console.log('---');
  console.log('CLOUD INVENTORY ISSUES');
  if (issues.length === 0) {
    console.log('(none found)');
  } else {
    for (const issue of issues) {
      console.log(issue);
    }
  }
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/cloud/snapshot.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import * as fs from 'fs';
import * as path from 'path';

function shell(cmd: string, allowFail = false): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    if (allowFail) {
      return '';
    }
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function shellJSON<T>(cmd: string, allowFail = false): T | null {
  try {
    const output = shell(cmd, allowFail);
    return output ? JSON.parse(output) : null;
  } catch {
    return null;
  }
}

function formatTimestamp(ts: string | null | undefined): string {
  if (!ts) return 'N/A';
  try {
    const date = new Date(ts);
    return date.toISOString().replace('T', ' ').replace('Z', ' UTC');
  } catch {
    return ts;
  }
}

function getETTimestamp(): string {
  const now = new Date();
  const et = new Date(now.toLocaleString('en-US', { timeZone: 'America/New_York' }));
  return et.toISOString().replace('T', ' ').substring(0, 19) + ' ET';
}

async function main(): Promise<void> {
  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SERVICE_NAME = 'ncc-jobs-runner';
  const JOBS = ['ncc-chunks', 'ncc-embeddings', 'ncc-smoke', 'ncc-ingest-me', 'ncc-ingest-other'];
  const SCHEDULER_JOBS = [
    'schedule-ncc-chunks',
    'schedule-ncc-embeddings',
    'schedule-ncc-smoke',
    'schedule-ncc-ingest-me-0710',
    'schedule-ncc-ingest-me-1210',
    'schedule-ncc-ingest-me-1710',
    'schedule-ncc-ingest-other-0710',
    'schedule-ncc-ingest-other-1210',
    'schedule-ncc-ingest-other-1710',
  ];

  console.log('---');
  console.log('DEPLOYMENT SNAPSHOT');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log('');

  const snapshot: string[] = [];
  snapshot.push(`# Deploy Snapshot (${getETTimestamp()})`);
  snapshot.push('');

  // 1. Image URI
  snapshot.push('## Image');
  snapshot.push('');
  let imageUri = 'unknown';
  const imagePath = path.join(process.cwd(), 'docs', 'LATEST_IMAGE.txt');
  if (fs.existsSync(imagePath)) {
    try {
      imageUri = fs.readFileSync(imagePath, 'utf8').trim();
    } catch {
      // Keep as unknown
    }
  }
  snapshot.push(`- Latest image URI: ${imageUri}`);
  snapshot.push('');

  // 2. Runner Service
  snapshot.push('## Runner Service');
  snapshot.push('');
  let serviceInfo: any = null;
  try {
    serviceInfo = shell(`gcloud run services describe ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format=json`);
    serviceInfo = JSON.parse(serviceInfo);
  } catch (error: any) {
    snapshot.push('- Status: NOT FOUND or ERROR');
    snapshot.push(`- Error: ${error.message || String(error)}`);
  }

  if (serviceInfo) {
    const url = serviceInfo.status?.url || 'N/A';
    const latestRevision = serviceInfo.status?.latestReadyRevisionName || 'N/A';
    const image = serviceInfo.spec?.template?.spec?.containers?.[0]?.image || 'N/A';
    const status = serviceInfo.status?.conditions?.[0]?.status || 'UNKNOWN';
    snapshot.push(`- URL: ${url}`);
    snapshot.push(`- Latest revision: ${latestRevision}`);
    snapshot.push(`- Image: ${image}`);
    snapshot.push(`- Status: ${status}`);
  }
  snapshot.push('');

  // 3. Cloud Run Jobs
  snapshot.push('## Cloud Run Jobs');
  snapshot.push('');
  snapshot.push('| Job | Last Status | Last Started | Last Completed |');
  snapshot.push('|-----|-------------|--------------|----------------|');

  for (const jobName of JOBS) {
    let jobInfo: any = null;
    let lastExecution: any = null;

    try {
      const jobDesc = shell(`gcloud run jobs describe ${jobName} --region=${REGION} --project=${PROJECT} --format=json`, true);
      if (jobDesc) {
        jobInfo = JSON.parse(jobDesc);
      }
    } catch {
      // Job might not exist
    }

    try {
      const execList = shell(`gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --format=json --limit=1`, true);
      if (execList) {
        const executions = JSON.parse(execList);
        if (Array.isArray(executions) && executions.length > 0) {
          lastExecution = executions[0];
        }
      }
    } catch {
      // No executions yet
    }

    const status = lastExecution?.status?.conditions?.[0]?.status || 'N/A';
    const startTime = formatTimestamp(lastExecution?.status?.startTime);
    const completionTime = formatTimestamp(lastExecution?.status?.completionTime);

    snapshot.push(`| ${jobName} | ${status} | ${startTime} | ${completionTime} |`);
  }
  snapshot.push('');

  // 4. Cloud Scheduler
  snapshot.push('## Cloud Scheduler');
  snapshot.push('');
  snapshot.push('| Job | Cron | Time Zone | Target | Next Run |');
  snapshot.push('|-----|------|-----------|--------|----------|');

  let schedulerJobs: any[] = [];
  try {
    const schedulerList = shell(`gcloud scheduler jobs list --location=${REGION} --project=${PROJECT} --format=json`, true);
    if (schedulerList) {
      schedulerJobs = JSON.parse(schedulerList);
    }
  } catch {
    // No scheduler jobs or error
  }

  for (const schedulerName of SCHEDULER_JOBS) {
    const scheduler = schedulerJobs.find((j: any) => j.name?.includes(schedulerName) || j.name?.endsWith(schedulerName));
    
    if (!scheduler) {
      snapshot.push(`| ${schedulerName} | NOT FOUND | - | - | - |`);
      continue;
    }

    const cron = scheduler.schedule || 'N/A';
    const timeZone = scheduler.timeZone || 'N/A';
    const target = scheduler.httpTarget?.uri || scheduler.oidcToken?.serviceAccountEmail || 'N/A';
    
    // Get detailed info to fetch nextRunTime via describe command
    let nextRunTime: string | null = null;
    try {
      // Extract job name from full path (e.g., "projects/.../locations/.../jobs/schedule-ncc-chunks" -> "schedule-ncc-chunks")
      const jobNameFromPath = scheduler.name?.split('/').pop() || schedulerName;
      const describeOutput = shell(`gcloud scheduler jobs describe ${jobNameFromPath} --location=${REGION} --project=${PROJECT} --format=json`, true);
      if (describeOutput) {
        const detailed = JSON.parse(describeOutput);
        // scheduleTime is the field that contains the next run time
        nextRunTime = detailed.scheduleTime || null;
      }
    } catch (error) {
      // Failed to describe, will use N/A
    }

    let nextRunFormatted = 'N/A';
    if (scheduler.state === 'PAUSED') {
      nextRunFormatted = 'PAUSED';
    } else if (scheduler.state !== 'ENABLED') {
      nextRunFormatted = scheduler.state || 'UNKNOWN';
    } else if (nextRunTime) {
      nextRunFormatted = formatTimestamp(nextRunTime);
    } else {
      nextRunFormatted = 'N/A (not available)';
    }

    snapshot.push(`| ${schedulerName} | ${cron} | ${timeZone} | ${target.substring(0, 60)}... | ${nextRunFormatted} |`);
  }
  snapshot.push('');

  // 5. Reconcile Report
  snapshot.push('## Reconcile');
  snapshot.push('');
  snapshot.push('```');
  try {
    const reconcileOutput = shell('npm run report:reconcile', true);
    snapshot.push(reconcileOutput || 'Reconcile report unavailable');
  } catch (error: any) {
    snapshot.push(`Error running reconcile: ${error.message || String(error)}`);
  }
  snapshot.push('```');
  snapshot.push('');

  // 6. How to Resume
  snapshot.push('## How to Resume');
  snapshot.push('');
  snapshot.push('1. `npm run cloud:build:stream`');
  snapshot.push('2. `npm run cloud:runner:apply`');
  snapshot.push('3. `npm run cloud:jobs:apply`');
  snapshot.push('4. `npm run cloud:schedule:apply`');
  snapshot.push('5. `npm run report:reconcile`');
  snapshot.push('');

  // Write to file
  const outputPath = path.join(process.cwd(), 'docs', 'DEPLOY_SNAPSHOT.md');
  const output = snapshot.join('\n');
  fs.writeFileSync(outputPath, output, 'utf8');

  console.log('✅ Snapshot written to:', outputPath);
  console.log('');
  console.log('---');
  console.log('SNAPSHOT PREVIEW:');
  console.log('---');
  console.log(output);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/dev/doctor-env.ts">
import 'dotenv/config';
import * as fs from 'fs';
import * as path from 'path';

async function main(): Promise<void> {
  const BQ_PROJECT_ID = process.env.BQ_PROJECT_ID || '(unset)';
  const BQ_LOCATION = process.env.BQ_LOCATION || '(unset)';
  
  // Compute RUN_REGION using same mapping as discovery
  const BQ_LOC = process.env.BQ_LOCATION || 'US';
  const RUN_REGION = process.env.NCC_REGION || (BQ_LOC.toUpperCase() === 'US' ? 'us-central1' : BQ_LOC);
  
  const GOOGLE_APPLICATION_CREDENTIALS = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
  const resolvedKeyPath = path.resolve(GOOGLE_APPLICATION_CREDENTIALS);
  const keyExists = fs.existsSync(resolvedKeyPath) ? 'exists' : 'does-not-exist';
  
  const NCC_IMPERSONATE_SA = process.env.NCC_IMPERSONATE_SA || '(unset)';

  console.log('---');
  console.log('ENV DOCTOR');
  console.log(`BQ_PROJECT_ID: ${BQ_PROJECT_ID}`);
  console.log(`BQ_LOCATION: ${BQ_LOCATION}`);
  console.log(`NCC_REGION (Cloud Run): ${RUN_REGION}`);
  console.log(`GOOGLE_APPLICATION_CREDENTIALS: ${resolvedKeyPath} (${keyExists})`);
  console.log(`NCC_IMPERSONATE_SA: ${NCC_IMPERSONATE_SA}`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/gmail/mint-refresh-token.ts">
import 'dotenv/config';
import { google } from 'googleapis';
import * as readline from 'readline';
import { execSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';

interface Args {
  inbox: 'me' | 'other';
  code?: string;
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

async function getSecretValue(secretName: string): Promise<string | null> {
  const cmd = `gcloud secrets versions access latest --secret=${secretName} --project=${PROJECT}`;
  const result = shell(cmd, true);
  return result.success ? result.output.trim() : null;
}

async function getClientCredentials(): Promise<{ clientId: string; clientSecret: string }> {
  // Try Secret Manager first
  const clientId = await getSecretValue('GMAIL_CLIENT_ID');
  const clientSecret = await getSecretValue('GMAIL_CLIENT_SECRET');
  
  if (clientId && clientSecret) {
    return { clientId, clientSecret };
  }
  
  // Fall back to .env
  const envClientId = process.env.GMAIL_CLIENT_ID;
  const envClientSecret = process.env.GMAIL_CLIENT_SECRET;
  
  if (envClientId && envClientSecret) {
    return { clientId: envClientId, clientSecret: envClientSecret };
  }
  
  throw new Error(
    'Missing Gmail OAuth credentials. Set GMAIL_CLIENT_ID and GMAIL_CLIENT_SECRET in:\n' +
    '  - Secret Manager (GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET), or\n' +
    '  - .env file (GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET)'
  );
}

async function promptForCode(): Promise<string> {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });
  
  return new Promise((resolve) => {
    rl.question('\nPaste the code here and press Enter: ', (code) => {
      rl.close();
      resolve(code.trim());
    });
  });
}

async function verifyToken(clientId: string, clientSecret: string, refreshToken: string): Promise<string> {
  const oAuth2Client = new google.auth.OAuth2(
    clientId,
    clientSecret,
    'urn:ietf:wg:oauth:2.0:oob'
  );
  oAuth2Client.setCredentials({ refresh_token: refreshToken });
  
  const gmail = google.gmail({ version: 'v1', auth: oAuth2Client });
  const profile = await gmail.users.getProfile({ userId: 'me' });
  
  if (!profile.data.emailAddress) {
    throw new Error('Token verification failed: no email address in profile');
  }
  
  return profile.data.emailAddress;
}

async function main() {
  const argv = await yargs(hideBin(process.argv))
    .option('inbox', {
      type: 'string',
      choices: ['me', 'other'],
      demandOption: true,
      describe: 'Inbox type (me or other)',
    })
    .option('code', {
      type: 'string',
      describe: 'OAuth code (if not provided, will prompt)',
    })
    .parseAsync() as Args;
  
  const inboxLabel = argv.inbox.toUpperCase();
  
  console.log(`\n=== Minting Gmail Refresh Token (${inboxLabel}) ===\n`);
  
  // Get client credentials
  const { clientId, clientSecret } = await getClientCredentials();
  console.log('✓ Retrieved OAuth credentials\n');
  
  // Set up OAuth2 client
  const REDIRECT_URI = 'http://localhost';
  const oAuth2Client = new google.auth.OAuth2(
    clientId,
    clientSecret,
    REDIRECT_URI
  );
  
  // Request modify scope (includes labels)
  const scopes = [
    'https://www.googleapis.com/auth/gmail.modify',
    'https://www.googleapis.com/auth/gmail.labels',
  ];
  
  // Generate auth URL
  const authUrl = oAuth2Client.generateAuthUrl({
    access_type: 'offline',
    prompt: 'consent',
    scope: scopes,
    redirect_uri: REDIRECT_URI,
  });
  
  // Get code from argument or prompt
  let code = argv.code;
  if (!code) {
    console.log('1) Open this URL in your browser and approve access:\n');
    console.log(authUrl);
    console.log('\n2) After approving, your browser will try to open http://localhost and show a connection error.');
    console.log('   That is expected. COPY the value after "code=" from the address bar,');
    console.log('   up to but not including any "&".\n');
    code = await promptForCode();
  }
  
  if (!code) {
    console.error('\n❌ No code provided. Exiting.');
    process.exit(1);
  }
  
  console.log('\nExchanging code for tokens...');
  
  try {
    const { tokens } = await oAuth2Client.getToken(code);
    
    if (!tokens.refresh_token) {
      console.error('\n❌ No refresh_token returned. Make sure you:');
      console.error('   - Added your email as a Test User on the OAuth consent screen');
      console.error('   - Used prompt=consent (this script does)');
      console.error('   - Selected the account and clicked Allow');
      console.error('\nThen try again.');
      process.exit(1);
    }
    
    // Verify token works
    console.log('Verifying token...');
    const emailAddress = await verifyToken(clientId, clientSecret, tokens.refresh_token);
    console.log(`✓ Token verified for: ${emailAddress}\n`);
    
    // Print ONLY the refresh token with clear label
    console.log('---');
    console.log(`${inboxLabel} REFRESH TOKEN:`);
    console.log('---');
    console.log(tokens.refresh_token);
    console.log('---');
    console.log('\n✓ Copy the token above and use it with:');
    console.log(`  npm run gmail:secret:${argv.inbox} -- --token="<paste_token_here>"`);
    console.log('');
    
  } catch (error: any) {
    console.error('\n❌ Token exchange failed:');
    console.error(error.response?.data || error.message);
    process.exit(1);
  }
}

main().catch((error) => {
  console.error('Fatal error:', error);
  process.exit(1);
});
</file>

<file path="scripts/gmail/run-live-test.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';

interface Args {
  inbox: 'me' | 'other';
  limit?: number;
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

function extractMetrics(logText: string): {
  inserted_messages: number;
  already_present: number;
  labels_inserted: number;
  already_labeled: number;
  marked_read: number;
  errors: string[];
} {
  const metrics = {
    inserted_messages: 0,
    already_present: 0,
    labels_inserted: 0,
    already_labeled: 0,
    marked_read: 0,
    errors: [] as string[],
  };

  // Extract from RECONCILE SUMMARY or similar patterns
  const insertedMatch = logText.match(/New emails ingested:\s*(\d+)/i);
  if (insertedMatch) {
    metrics.inserted_messages = parseInt(insertedMatch[1], 10);
  }

  const skippedMatch = logText.match(/Existing emails skipped:\s*(\d+)/i);
  if (skippedMatch) {
    metrics.already_present = parseInt(skippedMatch[1], 10);
  }

  const labelsMatch = logText.match(/New labels applied:\s*(\d+)/i);
  if (labelsMatch) {
    metrics.labels_inserted = parseInt(labelsMatch[1], 10);
  }

  // Extract Gmail API results
  const labeledMatch = logText.match(/Gmail labels applied:\s*(\d+)/i);
  if (labeledMatch) {
    metrics.labels_inserted = parseInt(labeledMatch[1], 10);
  }

  const alreadyLabeledMatch = logText.match(/\((\d+)\s+already had label\)/i);
  if (alreadyLabeledMatch) {
    metrics.already_labeled = parseInt(alreadyLabeledMatch[1], 10);
  }

  const markedReadMatch = logText.match(/Messages marked read:\s*(\d+)/i);
  if (markedReadMatch) {
    metrics.marked_read = parseInt(markedReadMatch[1], 10);
  }

  // Extract errors
  const errorLines = logText.match(/Error[^:]*:\s*[^\n]+/gi) || [];
  metrics.errors = errorLines.filter((e: string) => 
    !e.includes('dry-run') && 
    !e.includes('dry run') &&
    !e.includes('DRY RUN')
  );

  return metrics;
}

async function main() {
  const argv = await yargs(hideBin(process.argv))
    .option('inbox', {
      type: 'string',
      choices: ['me', 'other'],
      demandOption: true,
      describe: 'Inbox type (me or other)',
    })
    .option('limit', {
      type: 'number',
      default: 25,
      describe: 'Message limit (default: 25)',
    })
    .parseAsync() as Args;

  const jobName = `ncc-ingest-${argv.inbox}`;
  const limit = argv.limit || 25;

  console.log(`\n=== Running Live Test: ${argv.inbox.toUpperCase()} ===\n`);
  console.log(`Job: ${jobName}`);
  console.log(`Limit: ${limit}\n`);

  // Execute job
  console.log('Executing job...');
  const execCmd = `gcloud run jobs execute ${jobName} --region=${REGION} --project=${PROJECT} --args="dist/scripts/ingest-gmail.js,--inbox,${argv.inbox},--limit,${limit},--no-dry-run" --wait --format=json`;
  const execResult = shell(execCmd, true);

  if (!execResult.success) {
    console.error('❌ FAIL: Job execution failed');
    console.error(execResult.output);
    process.exit(1);
  }

  // Parse execution result
  let execName: string | null = null;
  try {
    const execData = JSON.parse(execResult.output);
    if (execData.metadata?.name) {
      execName = execData.metadata.name.split('/').pop() || null;
    }
  } catch {
    // Try to get from list
    const listCmd = `gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --format=json --limit=1`;
    const listResult = shell(listCmd, true);
    if (listResult.success) {
      try {
        const executions = JSON.parse(listResult.output);
        if (executions && executions.length > 0 && executions[0].name) {
          execName = executions[0].name.split('/').pop() || null;
        }
      } catch {}
    }
  }

  if (!execName) {
    console.error('❌ FAIL: Cannot find execution name');
    process.exit(1);
  }

  console.log(`Execution: ${execName}\n`);

  // Wait for logs
  console.log('Waiting for logs...');
  await new Promise(resolve => setTimeout(resolve, 5000));

  // Fetch logs
  const logCmd = `gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=${jobName} AND resource.labels.execution_name=${execName}" --limit=500 --format="value(textPayload)" --project=${PROJECT}`;
  const logResult = shell(logCmd, true);

  if (!logResult.success || !logResult.output) {
    console.error('❌ FAIL: Cannot fetch logs');
    console.error('Try: gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=' + jobName + '" --limit=100 --project=' + PROJECT);
    process.exit(1);
  }

  // Extract metrics
  const metrics = extractMetrics(logResult.output);

  // Print summary
  console.log('---');
  console.log('METRICS SUMMARY:');
  console.log('---');
  console.log(`inserted_messages: ${metrics.inserted_messages}`);
  console.log(`already_present: ${metrics.already_present}`);
  console.log(`labels_inserted: ${metrics.labels_inserted}`);
  console.log(`already_labeled: ${metrics.already_labeled}`);
  console.log(`marked_read: ${metrics.marked_read}`);
  
  if (metrics.errors.length > 0) {
    console.log(`\nerrors: ${metrics.errors.length}`);
    metrics.errors.slice(0, 5).forEach((err: string) => {
      console.log(`  - ${err.substring(0, 100)}`);
    });
  } else {
    console.log(`\nerrors: 0`);
  }
  console.log('---\n');

  // Check for permission errors
  const hasPermissionError = logResult.output.includes('Insufficient Permission') || 
                             logResult.output.includes('403') ||
                             logResult.output.includes('Forbidden');

  if (hasPermissionError) {
    console.log('❌ FAIL: Gmail permission errors detected');
    console.log('Check token scopes - tokens need gmail.modify scope');
    process.exit(1);
  }

  if (metrics.inserted_messages === 0 && metrics.already_present === 0) {
    console.log('⚠ WARNING: No messages processed (may be no unread emails)');
  }

  console.log('✅ PASS: Job completed successfully');
}

main().catch((error) => {
  console.error('Fatal error:', error);
  process.exit(1);
});
</file>

<file path="scripts/gmail/spot-check.ts">
import 'dotenv/config';
import { getGmail } from '../../src/gmail/client';
import type { gmail_v1 } from 'googleapis';

async function spotCheck(inbox: 'me' | 'other'): Promise<void> {
  console.log(`\n=== Spot-checking ${inbox.toUpperCase()} inbox ===\n`);

  let gmail: gmail_v1.Gmail;
  try {
    gmail = await getGmail(inbox);
  } catch (error: any) {
    console.error(`❌ Failed to get Gmail client: ${error.message}`);
    return;
  }

  // Get labels map
  let labelsRes;
  try {
    labelsRes = await gmail.users.labels.list({ userId: 'me' });
  } catch (error: any) {
    console.error(`❌ Failed to list labels: ${error.message}`);
    return;
  }

  const labelsMap = new Map<string, string>();
  if (labelsRes.data.labels) {
    for (const label of labelsRes.data.labels) {
      if (label.id && label.name) {
        labelsMap.set(label.id, label.name);
      }
    }
  }

  // Get query from env (same as ingest job uses)
  const query = process.env.GMAIL_QUERY || 'is:unread';
  
  // List messages from last 60 minutes
  // Gmail query doesn't support time-based filtering directly, so we'll fetch recent and filter
  let listRes;
  try {
    listRes = await gmail.users.messages.list({
      userId: 'me',
      q: query,
      maxResults: 10,
    });
  } catch (error: any) {
    console.error(`❌ Failed to list messages: ${error.message}`);
    return;
  }

  const messageIds = (listRes.data.messages || []).map(m => m.id!).filter(Boolean);
  console.log(`Found ${messageIds.length} messages matching query: ${query}`);

  if (messageIds.length === 0) {
    console.log('No messages to check.\n');
    return;
  }

  // Get message details
  const now = Date.now();
  const sixtyMinutesAgo = now - 60 * 60 * 1000;
  let checked = 0;
  let recentCount = 0;

  for (const messageId of messageIds) {
    try {
      const msgRes = await gmail.users.messages.get({
        userId: 'me',
        id: messageId,
        format: 'metadata',
        metadataHeaders: ['Subject', 'Date'],
      });

      const msg = msgRes.data;
      const internalDate = msg.internalDate ? parseInt(msg.internalDate) : 0;
      
      // Only check messages from last 60 minutes
      if (internalDate < sixtyMinutesAgo) {
        continue;
      }

      recentCount++;
      checked++;

      const subjectHeader = msg.payload?.headers?.find(h => h.name === 'Subject');
      const subject = subjectHeader?.value || '(no subject)';
      const labelIds = msg.labelIds || [];
      const labelNames = labelIds.map(id => labelsMap.get(id) || id).filter(Boolean);

      console.log(`\nMessage ${checked}:`);
      console.log(`  ID: ${messageId}`);
      console.log(`  Subject: ${subject}`);
      console.log(`  Labels: ${labelNames.join(', ') || '(none)'}`);
      console.log(`  Label IDs: ${labelIds.join(', ')}`);

      // Check for processed label
      const processedLabel = process.env.GMAIL_PROCESSED_LABEL || 'processed';
      const hasProcessedLabel = labelNames.some(name => 
        name.toLowerCase().includes(processedLabel.toLowerCase())
      ) || labelIds.some(id => labelsMap.get(id)?.toLowerCase().includes(processedLabel.toLowerCase()));

      // Check for read state (UNREAD label absence)
      const isRead = !labelIds.includes('UNREAD');

      console.log(`  Has processed label: ${hasProcessedLabel ? '✅ YES' : '❌ NO'}`);
      console.log(`  Is read: ${isRead ? '✅ YES' : '❌ NO'}`);

      if (checked >= 10) break;
    } catch (error: any) {
      console.error(`  ⚠️  Failed to get message ${messageId}: ${error.message}`);
    }
  }

  if (recentCount === 0) {
    console.log('\n⚠️  No messages found in the last 60 minutes.');
    console.log('   Note: Gmail query may not return recent messages immediately.');
  }

  console.log(`\nChecked ${recentCount} recent message(s) out of ${messageIds.length} total.\n`);
}

async function main(): Promise<void> {
  const inboxes: Array<'me' | 'other'> = ['me', 'other'];

  for (const inbox of inboxes) {
    await spotCheck(inbox);
  }

  console.log('\n=== Summary ===');
  console.log('Expected behavior:');
  console.log('  - Messages should have the "processed" label (or label matching GMAIL_PROCESSED_LABEL)');
  console.log('  - Messages should be marked as read (UNREAD label removed)');
  console.log('\nNote: Marked-as-read messages are removed from Inbox view.');
  console.log('      Check "All Mail" or search by label: label:<processed_label_name>');
}

if (require.main === module) {
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}
</file>

<file path="scripts/gmail/update-refresh-secrets.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const RUNTIME_SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';

interface Args {
  inbox: 'me' | 'other';
  token: string;
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

function shellJSON<T>(cmd: string, allowFail = false): T | null {
  try {
    const result = shell(cmd, allowFail);
    return result.success ? JSON.parse(result.output) : null;
  } catch {
    return null;
  }
}

async function checkSecretAccess(secretName: string): Promise<boolean> {
  const cmd = `gcloud secrets get-iam-policy ${secretName} --project=${PROJECT} --format=json`;
  const policy = shellJSON<any>(cmd, true);
  
  if (!policy) {
    return false;
  }
  
  const bindings = policy.bindings || [];
  return bindings.some((b: any) => 
    b.role === 'roles/secretmanager.secretAccessor' &&
    b.members?.includes(`serviceAccount:${RUNTIME_SA}`)
  );
}

async function main() {
  const argv = await yargs(hideBin(process.argv))
    .option('inbox', {
      type: 'string',
      choices: ['me', 'other'],
      demandOption: true,
      describe: 'Inbox type (me or other)',
    })
    .option('token', {
      type: 'string',
      demandOption: true,
      describe: 'Refresh token value',
    })
    .parseAsync() as Args;
  
  const secretName = argv.inbox === 'me' ? 'GMAIL_REFRESH_TOKEN_ME' : 'GMAIL_REFRESH_TOKEN_OTHER';
  const inboxLabel = argv.inbox.toUpperCase();
  
  console.log(`\n=== Updating Secret Manager (${inboxLabel}) ===\n`);
  
  // Verify secret exists
  const checkCmd = `gcloud secrets describe ${secretName} --project=${PROJECT} --format=json`;
  const secretExists = shellJSON<any>(checkCmd, true);
  
  if (!secretExists) {
    console.error(`❌ Secret ${secretName} does not exist. Create it first:`);
    console.error(`   echo -n "${argv.token}" | gcloud secrets create ${secretName} \\`);
    console.error(`     --data-file=- --project=${PROJECT} --replication-policy="automatic"`);
    process.exit(1);
  }
  
  console.log(`✓ Secret ${secretName} exists\n`);
  
  // Check IAM access
  const hasAccess = await checkSecretAccess(secretName);
  if (!hasAccess) {
    console.warn(`⚠ Warning: ${RUNTIME_SA} may not have access to ${secretName}`);
    console.warn(`  Grant access with:`);
    console.warn(`    gcloud secrets add-iam-policy-binding ${secretName} \\`);
    console.warn(`      --member="serviceAccount:${RUNTIME_SA}" \\`);
    console.warn(`      --role="roles/secretmanager.secretAccessor" \\`);
    console.warn(`      --project=${PROJECT}`);
    console.warn('');
  } else {
    console.log(`✓ Service account has access\n`);
  }
  
  // Add new version
  console.log('Adding new secret version...');
  // Use printf to avoid shell interpretation issues with echo -n
  const addCmd = `printf '%s' "${argv.token}" | gcloud secrets versions add ${secretName} --data-file=- --project=${PROJECT}`;
  const result = shell(addCmd, false);
  
  if (!result.success) {
    console.error(`❌ Failed to add secret version: ${result.output}`);
    process.exit(1);
  }
  
  // Extract version number from output
  const versionMatch = result.output.match(/version (\d+)/i);
  const version = versionMatch ? versionMatch[1] : 'latest';
  
  console.log(`✓ Secret version added: ${version}\n`);
  
  // Verify we can read it back (confirms IAM is correct)
  const verifyCmd = `gcloud secrets versions access latest --secret=${secretName} --project=${PROJECT}`;
  const verifyResult = shell(verifyCmd, true);
  
  if (verifyResult.success && verifyResult.output === argv.token) {
    console.log('✓ Secret verified (can read back)\n');
  } else if (verifyResult.success) {
    console.warn('⚠ Secret added but verification read returned different value (may be IAM delay)');
    console.warn('   Cloud Run jobs will use latest version automatically\n');
  } else {
    console.warn('⚠ Secret added but cannot verify read (check IAM permissions)');
    console.warn('   Cloud Run jobs will use latest version automatically\n');
  }
  
  console.log('---');
  console.log(`✅ ${inboxLabel} refresh token updated in Secret Manager`);
  console.log(`   Secret: ${secretName}`);
  console.log(`   Version: ${version}`);
  console.log('---');
  console.log('\nNext steps:');
  console.log('  1. Run: npm run ingest:preflight -- --apply (verify modify capability)');
  console.log('  2. Test with a small live job:');
  console.log(`     gcloud run jobs execute ncc-ingest-${argv.inbox} --region=us-central1 \\`);
  console.log(`       --project=${PROJECT} --args="--limit=3","--no-dry-run","--mark-read=false"`);
  console.log('');
}

main().catch((error) => {
  console.error('Fatal error:', error);
  process.exit(1);
});
</file>

<file path="scripts/ingest/preflight.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import { google, gmail_v1 } from 'googleapis';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';
const RUNTIME_SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';

interface CheckResult {
  name: string;
  pass: boolean;
  message: string;
  remediation?: string[];
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

function shellJSON<T>(cmd: string, allowFail = false): T | null {
  try {
    const result = shell(cmd, allowFail);
    return result.success ? JSON.parse(result.output) : null;
  } catch {
    return null;
  }
}

async function checkSecretExists(secretName: string): Promise<CheckResult> {
  const cmd = `gcloud secrets describe ${secretName} --project=${PROJECT} --format=json`;
  const result = shellJSON<any>(cmd, true);
  
  if (result) {
    return {
      name: `Secret ${secretName} exists`,
      pass: true,
      message: `Secret found`,
    };
  }
  
  return {
    name: `Secret ${secretName} exists`,
    pass: false,
    message: `Secret not found`,
    remediation: [
      `gcloud secrets create ${secretName} --data-file=- --project=${PROJECT} --replication-policy="automatic"`,
      `# Then paste the secret value (client_id, client_secret, or refresh_token)`,
    ],
  };
}

async function checkSecretAccess(secretName: string): Promise<CheckResult> {
  // Check IAM policy for the secret
  const cmd = `gcloud secrets get-iam-policy ${secretName} --project=${PROJECT} --format=json`;
  const policy = shellJSON<any>(cmd, true);
  
  if (!policy) {
    return {
      name: `Secret ${secretName} accessible by ${RUNTIME_SA}`,
      pass: false,
      message: `Cannot read IAM policy (secret may not exist)`,
      remediation: [
        `Create secret: gcloud secrets create ${secretName} --data-file=- --project=${PROJECT}`,
        `Grant access: gcloud secrets add-iam-policy-binding ${secretName} \\`,
        `  --member="serviceAccount:${RUNTIME_SA}" \\`,
        `  --role="roles/secretmanager.secretAccessor" \\`,
        `  --project=${PROJECT}`,
      ],
    };
  }
  
  const bindings = policy.bindings || [];
  const hasAccess = bindings.some((b: any) => 
    b.role === 'roles/secretmanager.secretAccessor' &&
    b.members?.includes(`serviceAccount:${RUNTIME_SA}`)
  );
  
  if (hasAccess) {
    return {
      name: `Secret ${secretName} accessible by ${RUNTIME_SA}`,
      pass: true,
      message: `IAM policy grants access`,
    };
  }
  
  return {
    name: `Secret ${secretName} accessible by ${RUNTIME_SA}`,
    pass: false,
    message: `IAM policy does not grant access`,
    remediation: [
      `gcloud secrets add-iam-policy-binding ${secretName} \\`,
      `  --member="serviceAccount:${RUNTIME_SA}" \\`,
      `  --role="roles/secretmanager.secretAccessor" \\`,
      `  --project=${PROJECT}`,
    ],
  };
}

async function getSecretValue(secretName: string): Promise<string | null> {
  const cmd = `gcloud secrets versions access latest --secret=${secretName} --project=${PROJECT}`;
  const result = shell(cmd, true);
  return result.success ? result.output.trim() : null;
}

async function checkGmailAuth(inbox: 'me' | 'other'): Promise<CheckResult> {
  // Temporarily set env vars from secrets for auth test
  const originalClientId = process.env.GMAIL_CLIENT_ID;
  const originalClientSecret = process.env.GMAIL_CLIENT_SECRET;
  const originalRefreshTokenMe = process.env.GMAIL_REFRESH_TOKEN_ME;
  const originalRefreshTokenOther = process.env.GMAIL_REFRESH_TOKEN_OTHER;
  
  try {
    const clientId = await getSecretValue('GMAIL_CLIENT_ID');
    const clientSecret = await getSecretValue('GMAIL_CLIENT_SECRET');
    const refreshTokenName = inbox === 'me' ? 'GMAIL_REFRESH_TOKEN_ME' : 'GMAIL_REFRESH_TOKEN_OTHER';
    const refreshToken = await getSecretValue(refreshTokenName);
    
    if (!clientId || !clientSecret || !refreshToken) {
      return {
        name: `Gmail auth for ${inbox}`,
        pass: false,
        message: `Missing credentials (client_id=${!!clientId}, client_secret=${!!clientSecret}, refresh_token=${!!refreshToken})`,
        remediation: [
          `Verify secrets exist: gcloud secrets list --project=${PROJECT} | grep GMAIL`,
          `Check access: gcloud secrets versions access latest --secret=${refreshTokenName} --project=${PROJECT}`,
        ],
      };
    }
    
    // Set env vars for token provider
    process.env.GMAIL_CLIENT_ID = clientId;
    process.env.GMAIL_CLIENT_SECRET = clientSecret;
    if (inbox === 'me') {
      process.env.GMAIL_REFRESH_TOKEN_ME = refreshToken;
    } else {
      process.env.GMAIL_REFRESH_TOKEN_OTHER = refreshToken;
    }
    
    try {
      const oAuth2Client = new google.auth.OAuth2(
        clientId,
        clientSecret,
        'urn:ietf:wg:oauth:2.0:oob'
      );
      oAuth2Client.setCredentials({ refresh_token: refreshToken });
      
      const gmail = google.gmail({ version: 'v1', auth: oAuth2Client });
      const profile = await gmail.users.getProfile({ userId: 'me' });
      
      if (profile.data.emailAddress) {
        return {
          name: `Gmail auth for ${inbox}`,
          pass: true,
          message: `Profile: ${profile.data.emailAddress}`,
        };
      }
      
      return {
        name: `Gmail auth for ${inbox}`,
        pass: false,
        message: `No email address in profile`,
      };
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      return {
        name: `Gmail auth for ${inbox}`,
        pass: false,
        message: `Auth failed: ${errorMsg.substring(0, 100)}`,
        remediation: [
          `Check refresh token: gcloud secrets versions access latest --secret=${refreshTokenName} --project=${PROJECT}`,
          `If token is invalid, re-run OAuth flow and update secret`,
        ],
      };
    }
  } finally {
    // Restore original env vars
    if (originalClientId !== undefined) process.env.GMAIL_CLIENT_ID = originalClientId;
    if (originalClientSecret !== undefined) process.env.GMAIL_CLIENT_SECRET = originalClientSecret;
    if (originalRefreshTokenMe !== undefined) process.env.GMAIL_REFRESH_TOKEN_ME = originalRefreshTokenMe;
    if (originalRefreshTokenOther !== undefined) process.env.GMAIL_REFRESH_TOKEN_OTHER = originalRefreshTokenOther;
  }
}

async function checkModifyCapability(inbox: 'me' | 'other'): Promise<CheckResult> {
  // Skip in dry-run mode (this check requires actual API calls)
  if (process.argv.includes('--apply')) {
    // Temporarily set env vars from secrets for auth test
    const originalClientId = process.env.GMAIL_CLIENT_ID;
    const originalClientSecret = process.env.GMAIL_CLIENT_SECRET;
    const originalRefreshTokenMe = process.env.GMAIL_REFRESH_TOKEN_ME;
    const originalRefreshTokenOther = process.env.GMAIL_REFRESH_TOKEN_OTHER;
    
    try {
      const clientId = await getSecretValue('GMAIL_CLIENT_ID');
      const clientSecret = await getSecretValue('GMAIL_CLIENT_SECRET');
      const refreshTokenName = inbox === 'me' ? 'GMAIL_REFRESH_TOKEN_ME' : 'GMAIL_REFRESH_TOKEN_OTHER';
      const refreshToken = await getSecretValue(refreshTokenName);
      
      if (!clientId || !clientSecret || !refreshToken) {
        return {
          name: `Gmail modify capability for ${inbox}`,
          pass: false,
          message: `Missing credentials`,
          remediation: [
            `Verify secrets exist: gcloud secrets list --project=${PROJECT} | grep GMAIL`,
          ],
        };
      }
      
      // Set env vars for token provider
      process.env.GMAIL_CLIENT_ID = clientId;
      process.env.GMAIL_CLIENT_SECRET = clientSecret;
      if (inbox === 'me') {
        process.env.GMAIL_REFRESH_TOKEN_ME = refreshToken;
      } else {
        process.env.GMAIL_REFRESH_TOKEN_OTHER = refreshToken;
      }
      
      try {
        const oAuth2Client = new google.auth.OAuth2(
          clientId,
          clientSecret,
          'urn:ietf:wg:oauth:2.0:oob'
        );
        oAuth2Client.setCredentials({ refresh_token: refreshToken });
        
        const gmail = google.gmail({ version: 'v1', auth: oAuth2Client });
        
        // Test modify capability by listing labels (requires gmail.modify scope)
        const labels = await gmail.users.labels.list({ userId: 'me' });
        
        if (labels.data.labels && labels.data.labels.length > 0) {
          return {
            name: `Gmail modify capability for ${inbox}`,
            pass: true,
            message: `Labels accessible (${labels.data.labels.length} labels)`,
          };
        }
        
        return {
          name: `Gmail modify capability for ${inbox}`,
          pass: false,
          message: `Labels list returned empty`,
        };
      } catch (error: any) {
        const errorMsg = error.message || String(error);
        const is403 = errorMsg.includes('403') || errorMsg.includes('Forbidden') || errorMsg.includes('Insufficient Permission');
        
        if (is403) {
          return {
            name: `Gmail modify capability for ${inbox}`,
            pass: false,
            message: `403 Forbidden - refresh token lacks gmail.modify scope`,
            remediation: [
              `Remint token with modify scope: npm run gmail:mint:${inbox}`,
              `Then update secret: npm run gmail:secret:${inbox} -- --token="<new_token>"`,
            ],
          };
        }
        
        return {
          name: `Gmail modify capability for ${inbox}`,
          pass: false,
          message: `Auth failed: ${errorMsg.substring(0, 100)}`,
          remediation: [
            `Check refresh token: gcloud secrets versions access latest --secret=${refreshTokenName} --project=${PROJECT}`,
          ],
        };
      }
    } finally {
      // Restore original env vars
      if (originalClientId !== undefined) process.env.GMAIL_CLIENT_ID = originalClientId;
      if (originalClientSecret !== undefined) process.env.GMAIL_CLIENT_SECRET = originalClientSecret;
      if (originalRefreshTokenMe !== undefined) process.env.GMAIL_REFRESH_TOKEN_ME = originalRefreshTokenMe;
      if (originalRefreshTokenOther !== undefined) process.env.GMAIL_REFRESH_TOKEN_OTHER = originalRefreshTokenOther;
    }
  }
  
  // Skip in preview mode
  return {
    name: `Gmail modify capability for ${inbox}`,
    pass: false,
    message: 'Not checked (use --apply to enable)',
    remediation: [`npm run ingest:preflight -- --apply`],
  };
}

async function checkIAMRoles(): Promise<CheckResult[]> {
  const results: CheckResult[] = [];
  
  // Check project-level IAM
  const cmd = `gcloud projects get-iam-policy ${PROJECT} --format=json`;
  const policy = shellJSON<any>(cmd, true);
  
  if (!policy) {
    return [{
      name: 'IAM policy accessible',
      pass: false,
      message: 'Cannot read IAM policy',
      remediation: [`gcloud projects get-iam-policy ${PROJECT} --format=json`],
    }];
  }
  
  const bindings = policy.bindings || [];
  const saBindings = bindings.filter((b: any) => 
    b.members?.includes(`serviceAccount:${RUNTIME_SA}`)
  );
  
  const roles = new Set<string>();
  for (const binding of saBindings) {
    if (binding.role) {
      roles.add(binding.role);
    }
  }
  
  // Check BigQuery roles
  const hasJobUser = roles.has('roles/bigquery.jobUser') || roles.has('roles/bigquery.user');
  const hasDataEditor = roles.has('roles/bigquery.dataEditor') || roles.has('roles/bigquery.admin');
  const hasSecretAccessor = roles.has('roles/secretmanager.secretAccessor');
  
  results.push({
    name: 'BigQuery jobUser role',
    pass: hasJobUser,
    message: hasJobUser ? 'Role present' : 'Role missing',
    remediation: hasJobUser ? undefined : [
      `gcloud projects add-iam-policy-binding ${PROJECT} \\`,
      `  --member="serviceAccount:${RUNTIME_SA}" \\`,
      `  --role="roles/bigquery.jobUser"`,
    ],
  });
  
  results.push({
    name: 'BigQuery dataEditor role',
    pass: hasDataEditor,
    message: hasDataEditor ? 'Role present' : 'Role missing',
    remediation: hasDataEditor ? undefined : [
      `gcloud projects add-iam-policy-binding ${PROJECT} \\`,
      `  --member="serviceAccount:${RUNTIME_SA}" \\`,
      `  --role="roles/bigquery.dataEditor"`,
    ],
  });
  
  results.push({
    name: 'Secret Manager secretAccessor role',
    pass: hasSecretAccessor,
    message: hasSecretAccessor ? 'Role present' : 'Role missing',
    remediation: hasSecretAccessor ? undefined : [
      `gcloud projects add-iam-policy-binding ${PROJECT} \\`,
      `  --member="serviceAccount:${RUNTIME_SA}" \\`,
      `  --role="roles/secretmanager.secretAccessor"`,
    ],
  });
  
  return results;
}

async function checkJobConfig(jobName: string): Promise<CheckResult[]> {
  const results: CheckResult[] = [];
  
  const cmd = `gcloud run jobs describe ${jobName} --region=${REGION} --project=${PROJECT} --format=json`;
  const job = shellJSON<any>(cmd, true);
  
  if (!job) {
    return [{
      name: `Job ${jobName} exists`,
      pass: false,
      message: 'Job not found',
      remediation: [`npm run cloud:jobs:apply`],
    }];
  }
  
  results.push({
    name: `Job ${jobName} exists`,
    pass: true,
    message: 'Job found',
  });
  
  // Check args
  // Path: spec.template.spec.template.spec.containers[0] (nested template structure)
  const spec = job.spec;
  const outerTemplate = spec?.template;
  const outerTemplateSpec = outerTemplate?.spec;
  const innerTemplate = outerTemplateSpec?.template;
  const innerTemplateSpec = innerTemplate?.spec;
  const containers = innerTemplateSpec?.containers || [];
  const container = containers[0];
  const args = container?.args || [];
  
  const hasInbox = args.includes('--inbox');
  const inboxValue = args[args.indexOf('--inbox') + 1];
  const hasLimit = args.includes('--limit');
  const limitValue = args[args.indexOf('--limit') + 1];
  const hasNoDryRun = args.includes('--no-dry-run');
  
  const expectedInbox = jobName.includes('me') ? 'me' : 'other';
  
  results.push({
    name: `Job ${jobName} --inbox arg`,
    pass: hasInbox && inboxValue === expectedInbox,
    message: hasInbox ? `Value: ${inboxValue}` : 'Missing --inbox arg',
    remediation: hasInbox && inboxValue === expectedInbox ? undefined : [
      `gcloud run jobs update ${jobName} --region=${REGION} --project=${PROJECT} \\`,
      `  --args="${args.filter((a: string) => a !== '--inbox' && a !== inboxValue).join(',')},--inbox,${expectedInbox}"`,
    ],
  });
  
  results.push({
    name: `Job ${jobName} --limit arg`,
    pass: hasLimit && limitValue && parseInt(limitValue) > 0,
    message: hasLimit ? `Value: ${limitValue}` : 'Missing --limit arg',
    remediation: hasLimit ? undefined : [
      `gcloud run jobs update ${jobName} --region=${REGION} --project=${PROJECT} \\`,
      `  --args="${args.join(',')},--limit,500"`,
    ],
  });
  
  // Check env vars
  const envVars = container?.env || [];
  const envMap = new Map<string, string>();
  for (const env of envVars) {
    if (env.name && env.value) {
      envMap.set(env.name, env.value);
    }
  }
  
  const requiredEnvVars = [
    'GMAIL_QUERY',
    'GMAIL_PROCESSED_LABEL',
    'GMAIL_PAID_LABEL',
    'GMAIL_MARK_READ',
  ];
  
  for (const envVar of requiredEnvVars) {
    const hasVar = envMap.has(envVar);
    const value = envMap.get(envVar);
    results.push({
      name: `Job ${jobName} env ${envVar}`,
      pass: hasVar && value !== undefined,
      message: hasVar ? `Value: ${value}` : 'Missing',
      remediation: hasVar ? undefined : [
        `gcloud run jobs update ${jobName} --region=${REGION} --project=${PROJECT} \\`,
        `  --update-env-vars="${envVar}=<value>"`,
      ],
    });
  }
  
  return results;
}

async function checkDryRun(jobName: string): Promise<CheckResult> {
  // First check if job exists
  const checkJob = shell(`gcloud run jobs describe ${jobName} --region=${REGION} --project=${PROJECT} --format=json`, true);
  if (!checkJob.success) {
    return {
      name: `Dry-run execution for ${jobName}`,
      pass: false,
      message: 'Job does not exist',
      remediation: [`npm run cloud:jobs:apply`],
    };
  }
  
  // Get the script path from job config
  const jobDesc = shellJSON<any>(`gcloud run jobs describe ${jobName} --region=${REGION} --project=${PROJECT} --format=json`, true);
  if (!jobDesc) {
    return {
      name: `Dry-run execution for ${jobName}`,
      pass: false,
      message: 'Cannot read job configuration',
    };
  }
  
  const spec = jobDesc.spec;
  const outerTemplate = spec?.template;
  const outerTemplateSpec = outerTemplate?.spec;
  const innerTemplate = outerTemplateSpec?.template;
  const innerTemplateSpec = innerTemplate?.spec;
  const containers = innerTemplateSpec?.containers || [];
  const container = containers[0];
  const existingArgs = container?.args || [];
  const scriptPath = existingArgs[0] || 'dist/scripts/ingest-gmail.js';
  
  const inbox = jobName.includes('me') ? 'me' : 'other';
  // Execute job with temporary args override (must include script path)
  const cmd = `gcloud run jobs execute ${jobName} --region=${REGION} --project=${PROJECT} --args="${scriptPath},--dry-run,--limit,3,--inbox,${inbox}" --wait --format=json`;
  const result = shellJSON<any>(cmd, true);
  
  if (!result) {
    return {
      name: `Dry-run execution for ${jobName}`,
      pass: false,
      message: 'Execution failed or timed out',
      remediation: [
        `Check job status: gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --limit=1`,
        `View logs: gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=${jobName}" --limit=100 --project=${PROJECT}`,
      ],
    };
  }
  
  // Wait a bit for logs to appear
  await new Promise(resolve => setTimeout(resolve, 5000));
  
  // Get execution name from result or list
  let execName: string | null = null;
  if (result.metadata?.name) {
    execName = result.metadata.name.split('/').pop() || null;
  } else {
    const execCmd = `gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --format=json --limit=1`;
    const executions = shellJSON<Array<{ name?: string }>>(execCmd, true);
    if (executions && executions.length > 0 && executions[0].name) {
      execName = executions[0].name.split('/').pop() || null;
    }
  }
  
  if (!execName) {
    return {
      name: `Dry-run execution for ${jobName}`,
      pass: false,
      message: 'Cannot find execution name',
    };
  }
  
  // Check logs for expected output
  const logCmd = `gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=${jobName}" --limit=200 --format="value(textPayload)" --project=${PROJECT} --freshness=5m`;
  const logs = shell(logCmd, true);
  
  const logText = logs.output.toLowerCase();
  const hasFetched = logText.includes('fetched') || logText.includes('messages');
  const hasDryRun = logText.includes('dry-run') || logText.includes('dry run') || logText.includes('[dry run]');
  const hasNoWrites = !logText.includes('labeled') && !logText.includes('marked_read') || logText.includes('readonly');
  
  const allChecks = hasFetched && hasDryRun;
  
  return {
    name: `Dry-run execution for ${jobName}`,
    pass: allChecks,
    message: allChecks ? 'Dry-run completed successfully (no writes)' : `Missing expected output (fetched=${hasFetched}, dry-run=${hasDryRun})`,
    remediation: allChecks ? undefined : [
      `View recent logs: ${logCmd}`,
      `Check execution: gcloud run jobs executions describe ${execName} --region=${REGION} --project=${PROJECT}`,
    ],
  };
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually execute dry-run jobs (default: preview only)',
    })
    .parse();

  console.log('---');
  console.log('GMAIL INGEST PREFLIGHT');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Runtime SA: ${RUNTIME_SA}`);
  console.log('');

  const checks: CheckResult[] = [];

  // 1. Secrets check
  console.log('Checking secrets...');
  const secretNames = ['GMAIL_CLIENT_ID', 'GMAIL_CLIENT_SECRET', 'GMAIL_REFRESH_TOKEN_ME', 'GMAIL_REFRESH_TOKEN_OTHER'];
  for (const secretName of secretNames) {
    checks.push(await checkSecretExists(secretName));
    checks.push(await checkSecretAccess(secretName));
  }
  console.log('');

  // 2. Gmail auth check
  console.log('Checking Gmail authentication...');
  checks.push(await checkGmailAuth('me'));
  checks.push(await checkGmailAuth('other'));
  console.log('');

  // 2b. Gmail modify capability check (only when --apply is used)
  if (argv.apply) {
    console.log('Checking Gmail modify capability...');
    checks.push(await checkModifyCapability('me'));
    checks.push(await checkModifyCapability('other'));
    console.log('');
  } else {
    checks.push(await checkModifyCapability('me'));
    checks.push(await checkModifyCapability('other'));
  }

  // 3. IAM check
  console.log('Checking IAM roles...');
  checks.push(...await checkIAMRoles());
  console.log('');

  // 4. Job config check
  console.log('Checking job configurations...');
  checks.push(...await checkJobConfig('ncc-ingest-me'));
  checks.push(...await checkJobConfig('ncc-ingest-other'));
  console.log('');

  // 5. Dry-run execution
  if (argv.apply) {
    console.log('Executing dry-run jobs...');
    checks.push(await checkDryRun('ncc-ingest-me'));
    checks.push(await checkDryRun('ncc-ingest-other'));
  } else {
    console.log('Skipping dry-run execution (use --apply to enable)');
    checks.push({
      name: 'Dry-run execution',
      pass: false,
      message: 'Not executed (use --apply flag)',
      remediation: [`npm run ingest:preflight -- --apply`],
    });
  }
  console.log('');

  // Summary
  console.log('---');
  console.log('PREFLIGHT SUMMARY');
  console.log('');
  
  const passed = checks.filter(c => c.pass).length;
  const total = checks.length;
  
  for (const check of checks) {
    const icon = check.pass ? '✓' : '✗';
    const color = check.pass ? '\x1b[32m' : '\x1b[31m';
    const reset = '\x1b[0m';
    console.log(`${color}${icon}${reset} ${check.name}: ${check.message}`);
    if (!check.pass && check.remediation) {
      console.log('   Remediation:');
      for (const cmd of check.remediation) {
        console.log(`     ${cmd}`);
      }
    }
  }
  
  console.log('');
  console.log(`Results: ${passed}/${total} checks passed`);
  console.log('');
  
  if (passed === total) {
    console.log('✅ PREFLIGHT PASSED');
    console.log('');
    console.log('Next steps:');
    console.log('  1. npm run cloud:jobs:apply');
    console.log('  2. npm run cloud:schedule:apply');
    console.log('  3. npm run cloud:snapshot');
  } else {
    console.log('❌ PREFLIGHT FAILED');
    console.log('');
    console.log('Fix the issues above, then re-run:');
    console.log('  npm run ingest:preflight');
  }
  console.log('---');
  
  process.exit(passed === total ? 0 : 1);
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/legacy/full-chunk-and-embed.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { v4 as uuidv4 } from 'uuid';
import { getBestCleanedContent } from '../newsletter-search/src/lib/newsletter-cleaning';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const MESSAGES_TABLE = 'messages';
const CHUNKS_TABLE = 'chunks';
const LOCATION = 'us-central1';

const TARGET_CHUNK_SIZE = 800;
const OVERLAP_SIZE = 100;
const MIN_CHUNK_SIZE = 200;

interface NewsletterChunk {
  chunk_id: string;
  newsletter_id: string;
  chunk_index: number;
  chunk_text: string;
  chunk_embedding: number[] | null;
  sent_date: any;
  publisher_name: string;
  subject: string;
  version: number;
  created_at: string;
  updated_at: string;
}

function createSemanticChunks(text: string, targetSize: number = TARGET_CHUNK_SIZE): string[] {
  if (!text || text.length < MIN_CHUNK_SIZE) {
    return text ? [text] : [];
  }

  const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 0);
  
  if (paragraphs.length === 0) return [text];
  
  const chunks: string[] = [];
  let currentChunk = '';

  for (let i = 0; i < paragraphs.length; i++) {
    const paragraph = paragraphs[i].trim();
    
    if (currentChunk && (currentChunk.length + paragraph.length + 2 > targetSize)) {
      chunks.push(currentChunk.trim());
      currentChunk = paragraph;
    } else {
      if (currentChunk) {
        currentChunk += '\n\n' + paragraph;
      } else {
        currentChunk = paragraph;
      }
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }

  if (chunks.length === 1 && chunks[0].length > targetSize * 1.5) {
    return splitBySentences(chunks[0], targetSize);
  }

  return chunks.filter(chunk => chunk.length >= MIN_CHUNK_SIZE);
}

function splitBySentences(text: string, targetSize: number): string[] {
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
  const chunks: string[] = [];
  let currentChunk = '';

  for (const sentence of sentences) {
    if (currentChunk && (currentChunk.length + sentence.length > targetSize)) {
      chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += sentence;
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }

  return chunks.filter(chunk => chunk.length >= MIN_CHUNK_SIZE);
}

function addOverlap(chunks: string[], overlapSize: number = OVERLAP_SIZE): string[] {
  if (chunks.length <= 1) return chunks;

  const overlappedChunks = [chunks[0]];

  for (let i = 1; i < chunks.length; i++) {
    const prevChunk = chunks[i - 1];
    const currentChunk = chunks[i];

    const overlapText = prevChunk.slice(-overlapSize);
    const overlappedChunk = overlapText + '\n\n' + currentChunk;
    
    overlappedChunks.push(overlappedChunk);
  }

  return overlappedChunks;
}

async function generateEmbedding(text: string): Promise<number[]> {
  try {
    const { GoogleAuth } = require('google-auth-library');
    const auth = new GoogleAuth({
      scopes: ['https://www.googleapis.com/auth/cloud-platform']
    });
    const client = await auth.getClient();
    const accessToken = await client.getAccessToken();

    const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/text-embedding-004:predict`;
    
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${accessToken.token}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        instances: [
          {
            content: text,
            task_type: 'RETRIEVAL_DOCUMENT',
          }
        ]
      })
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`API returned ${response.status}: ${errorText}`);
    }

    const data = await response.json();
    
    if (data.predictions && data.predictions[0] && data.predictions[0].embeddings) {
      const embedding = data.predictions[0].embeddings.values || data.predictions[0].embeddings;
      if (Array.isArray(embedding)) {
        return embedding;
      }
    }
    
    throw new Error('No embedding returned from API');
  } catch (error) {
    console.error('❌ Embedding generation failed:', error);
    throw error;
  }
}

async function processNewsletterWithEmbeddings(newsletter: any): Promise<NewsletterChunk[]> {
  console.log(`\n📄 Processing: ${newsletter.subject}`);
  
  const cleanedContent = getBestCleanedContent(
    newsletter.body_text || '',
    newsletter.body_html || ''
  );

  if (!cleanedContent || cleanedContent.length < MIN_CHUNK_SIZE) {
    console.log(`   ⚠️  Insufficient content (${cleanedContent?.length || 0} chars)`);
    return [];
  }

  const chunks = createSemanticChunks(cleanedContent, TARGET_CHUNK_SIZE);
  const overlappedChunks = addOverlap(chunks, OVERLAP_SIZE);

  console.log(`   📏 Cleaned: ${cleanedContent.length} chars → ${overlappedChunks.length} chunks`);

  const chunkRecords: NewsletterChunk[] = [];

  for (let i = 0; i < overlappedChunks.length; i++) {
    const chunkText = overlappedChunks[i];
    
    console.log(`   🧠 Generating embedding for chunk ${i + 1}/${overlappedChunks.length}...`);
    const embedding = await generateEmbedding(chunkText);

    chunkRecords.push({
      chunk_id: uuidv4(),
      newsletter_id: newsletter.id,
      chunk_index: i,
      chunk_text: chunkText,
      chunk_embedding: embedding,
      sent_date: newsletter.sent_date,
      publisher_name: newsletter.publisher_name,
      subject: newsletter.subject,
      version: 1,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString()
    });

    // Small delay to avoid rate limits
    if (i < overlappedChunks.length - 1) {
      await new Promise(resolve => setTimeout(resolve, 200));
    }
  }

  return chunkRecords;
}

async function recreateTable(bigquery: BigQuery): Promise<void> {
  const dataset = bigquery.dataset(DATASET_ID);
  const table = dataset.table(CHUNKS_TABLE);

  console.log('🗑️  Deleting existing table...');
  try {
    await table.delete();
    console.log('✅ Table deleted');
  } catch (error) {
    console.log('⚠️  Table may not exist, continuing...');
  }

  console.log('📝 Creating new table with correct schema (ARRAY with REPEATED mode)...');
  
  await table.create({
    schema: [
      { name: 'chunk_id', type: 'STRING', mode: 'REQUIRED' },
      { name: 'newsletter_id', type: 'STRING', mode: 'REQUIRED' },
      { name: 'chunk_index', type: 'INTEGER', mode: 'REQUIRED' },
      { name: 'chunk_text', type: 'STRING', mode: 'REQUIRED' },
      { name: 'chunk_embedding', type: 'FLOAT64', mode: 'REPEATED' },
      { name: 'sent_date', type: 'TIMESTAMP', mode: 'NULLABLE' },
      { name: 'publisher_name', type: 'STRING', mode: 'NULLABLE' },
      { name: 'subject', type: 'STRING', mode: 'NULLABLE' },
      { name: 'version', type: 'INTEGER', mode: 'REQUIRED' },
      { name: 'created_at', type: 'TIMESTAMP', mode: 'REQUIRED' },
      { name: 'updated_at', type: 'TIMESTAMP', mode: 'REQUIRED' }
    ],
    timePartitioning: {
      type: 'DAY',
      field: 'created_at'
    }
  });

  console.log('✅ Table created');
}

async function main() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  try {
    // Recreate table
    await recreateTable(bigquery);

    // Fetch test newsletter
    const TEST_ID = '191eb243eb2e03f9';
    console.log('\n📥 Fetching test newsletter...');
    const query = `
      SELECT *
      FROM \`${PROJECT_ID}.${DATASET_ID}.${MESSAGES_TABLE}\`
      WHERE id = '${TEST_ID}'
      LIMIT 1
    `;

    const [rows] = await bigquery.query(query);
    const newsletter = rows[0];

    // Process newsletter with embeddings
    const chunks = await processNewsletterWithEmbeddings(newsletter);

    // Insert chunks into BigQuery
    if (chunks.length > 0) {
      console.log(`\n💾 Inserting ${chunks.length} chunks into BigQuery...`);
      const dataset = bigquery.dataset(DATASET_ID);
      const table = dataset.table(CHUNKS_TABLE);
      await table.insert(chunks);
      console.log(`✅ Inserted ${chunks.length} chunks with embeddings`);
    }

    console.log('\n🎉 Complete! Chunks and embeddings created successfully.');

  } catch (error) {
    console.error('❌ Failed:', error);
    throw error;
  }
}

main();
</file>

<file path="scripts/legacy/process-newsletters.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { v4 as uuidv4 } from 'uuid';
import { getBestCleanedContent } from '../newsletter-search/src/lib/newsletter-cleaning';
import * as fs from 'fs';
import * as path from 'path';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const MESSAGES_TABLE = 'messages';
const CHUNKS_TABLE = 'chunks';
const LOCATION = 'us-central1';

const TARGET_CHUNK_SIZE = 800;
const OVERLAP_SIZE = 100;
const MIN_CHUNK_SIZE = 200;

// Progress persistence file
const PROGRESS_FILE = path.join(__dirname, '..', 'processing-progress.json');

interface NewsletterChunk {
  chunk_id: string;
  newsletter_id: string;
  chunk_index: number;
  chunk_text: string;
  chunk_embedding: number[] | null;
  sent_date: any;
  publisher_name: string;
  subject: string;
  is_paid: boolean | null;
  version: number;
  created_at: string;
  updated_at: string;
}

interface ProcessingStats {
  total: number;
  processed: number;
  skipped: number;
  failed: number;
  chunksCreated: number;
  apiCalls: number;
  startTime: string;
  lastUpdateTime: string;
  processedNewsletterIds: string[];
  lastProcessedId?: string; // For cursor-based pagination
}

// Cost tracking
const EMBEDDING_COST_PER_1K_CHARS = 0.00001; // $0.00001 per 1k characters
const GEMINI_COST_PER_1K_TOKENS_INPUT = 0.25; // Approximate
const GEMINI_COST_PER_1K_TOKENS_OUTPUT = 1.0; // Approximate

let stats: ProcessingStats = {
  total: 0,
  processed: 0,
  skipped: 0,
  failed: 0,
  chunksCreated: 0,
  apiCalls: 0,
  startTime: new Date().toISOString(),
  lastUpdateTime: new Date().toISOString(),
  processedNewsletterIds: [],
  lastProcessedId: undefined
};

function createSemanticChunks(text: string, targetSize: number = TARGET_CHUNK_SIZE): string[] {
  if (!text || text.length < MIN_CHUNK_SIZE) {
    return text ? [text] : [];
  }

  const paragraphs = text.split(/\n\s*\n/).filter(p => p.trim().length > 0);
  
  if (paragraphs.length === 0) return [text];
  
  const chunks: string[] = [];
  let currentChunk = '';

  for (let i = 0; i < paragraphs.length; i++) {
    const paragraph = paragraphs[i].trim();
    
    if (currentChunk && (currentChunk.length + paragraph.length + 2 > targetSize)) {
      chunks.push(currentChunk.trim());
      currentChunk = paragraph;
    } else {
      if (currentChunk) {
        currentChunk += '\n\n' + paragraph;
      } else {
        currentChunk = paragraph;
      }
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }

  if (chunks.length === 1 && chunks[0].length > targetSize * 1.5) {
    return splitBySentences(chunks[0], targetSize);
  }

  return chunks.filter(chunk => chunk.length >= MIN_CHUNK_SIZE);
}

function splitBySentences(text: string, targetSize: number): string[] {
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];
  const chunks: string[] = [];
  let currentChunk = '';

  for (const sentence of sentences) {
    if (currentChunk && (currentChunk.length + sentence.length > targetSize)) {
      chunks.push(currentChunk.trim());
      currentChunk = sentence;
    } else {
      currentChunk += sentence;
    }
  }

  if (currentChunk) {
    chunks.push(currentChunk.trim());
  }

  return chunks.filter(chunk => chunk.length >= MIN_CHUNK_SIZE);
}

function addOverlap(chunks: string[], overlapSize: number = OVERLAP_SIZE): string[] {
  if (chunks.length <= 1) return chunks;

  const overlappedChunks = [chunks[0]];

  for (let i = 1; i < chunks.length; i++) {
    const prevChunk = chunks[i - 1];
    const currentChunk = chunks[i];

    const overlapText = prevChunk.slice(-overlapSize);
    const overlappedChunk = overlapText + '\n\n' + currentChunk;
    
    overlappedChunks.push(overlappedChunk);
  }

  return overlappedChunks;
}

async function generateEmbedding(text: string, retries: number = 3): Promise<number[]> {
  const { GoogleAuth } = require('google-auth-library');
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/text-embedding-004:predict`;
  
  for (let attempt = 0; attempt < retries; attempt++) {
    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${accessToken.token}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          instances: [
            {
              content: text,
              task_type: 'RETRIEVAL_DOCUMENT',
            }
          ]
        })
      });

      if (response.ok) {
        const data = await response.json();
        
        if (data.predictions && data.predictions[0] && data.predictions[0].embeddings) {
          const embedding = data.predictions[0].embeddings.values || data.predictions[0].embeddings;
          if (Array.isArray(embedding)) {
            stats.apiCalls++;
            return embedding;
          }
        }
        
        throw new Error('No embedding returned from API');
      }

      // Handle rate limiting (429) or temporary errors (500, 502, 503, 504)
      if (response.status === 429 || response.status === 500 || response.status === 502 || response.status === 503 || response.status === 504) {
        const waitTime = Math.pow(2, attempt) * 1000; // Exponential backoff: 1s, 2s, 4s...
        console.log(`   ⚠️  Rate limited or server error. Retrying in ${waitTime}ms... (attempt ${attempt + 1}/${retries})`);
        await new Promise(resolve => setTimeout(resolve, waitTime));
        continue;
      }

      const errorText = await response.text();
      throw new Error(`API returned ${response.status}: ${errorText}`);
    } catch (error) {
      if (attempt === retries - 1) {
        console.error('❌ Embedding generation failed after retries:', error);
        throw error;
      }
      const waitTime = Math.pow(2, attempt) * 1000;
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
  }

  throw new Error('Embedding generation failed after all retries');
}

async function processNewsletterWithEmbeddings(newsletter: any): Promise<NewsletterChunk[]> {
  const cleanedContent = getBestCleanedContent(
    newsletter.body_text || '',
    newsletter.body_html || ''
  );

  if (!cleanedContent || cleanedContent.length < MIN_CHUNK_SIZE) {
    return [];
  }

  const chunks = createSemanticChunks(cleanedContent, TARGET_CHUNK_SIZE);
  const overlappedChunks = addOverlap(chunks, OVERLAP_SIZE);

  const chunkRecords: NewsletterChunk[] = [];

  for (let i = 0; i < overlappedChunks.length; i++) {
    const chunkText = overlappedChunks[i];
    
    // Retry embedding generation with exponential backoff
    let embedding: number[];
    try {
      embedding = await generateEmbedding(chunkText, 3);
    } catch (error) {
      console.error(`   ❌ Failed to generate embedding for chunk ${i}: ${error}`);
      throw error; // Re-throw to skip this newsletter
    }

    chunkRecords.push({
      chunk_id: uuidv4(),
      newsletter_id: newsletter.id,
      chunk_index: i,
      chunk_text: chunkText,
      chunk_embedding: embedding,
      sent_date: newsletter.sent_date,
      publisher_name: newsletter.publisher_name,
      subject: newsletter.subject,
      is_paid: newsletter.is_paid || null,
      version: 1,
      created_at: new Date().toISOString(),
      updated_at: new Date().toISOString()
    });

    // No delay needed - exponential backoff handles rate limits
  }

  return chunkRecords;
}

function getExistingNewsletterIds(bigquery: BigQuery): Promise<Set<string>> {
  return new Promise(async (resolve, reject) => {
    try {
      const query = `
        SELECT DISTINCT newsletter_id
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
        WHERE newsletter_id IS NOT NULL
      `;
      const [rows] = await bigquery.query(query);
      resolve(new Set(rows.map((row: any) => row.newsletter_id)));
    } catch (error) {
      console.warn('⚠️  Could not fetch existing newsletters, starting fresh');
      resolve(new Set());
    }
  });
}

function calculateTimeRemaining(): string {
  if (stats.processed === 0) return 'N/A';
  
  const startTime = new Date(stats.startTime).getTime();
  const elapsed = (Date.now() - startTime) / 1000;
  const avgTimePerNewsletter = elapsed / stats.processed;
  const remaining = avgTimePerNewsletter * (stats.total - stats.processed);
  
  if (remaining < 60) return `${Math.round(remaining)}s`;
  if (remaining < 3600) return `${Math.round(remaining / 60)}m`;
  return `${Math.round(remaining / 3600)}h ${Math.round((remaining % 3600) / 60)}m`;
}

function estimateCost(): number {
  // Embeddings: $0.00001 per 1k characters
  // Assume average newsletter is 10k characters, becomes 12 chunks of 800 chars each
  const estimatedCharsPerNewsletter = 10000;
  const embeddingCost = stats.processed * (estimatedCharsPerNewsletter / 1000) * EMBEDDING_COST_PER_1K_CHARS;
  
  return embeddingCost;
}

function saveProgress() {
  try {
    fs.writeFileSync(PROGRESS_FILE, JSON.stringify(stats, null, 2));
  } catch (error) {
    console.warn('⚠️  Could not save progress file:', error);
  }
}

function loadProgress(): ProcessingStats | null {
  try {
    if (fs.existsSync(PROGRESS_FILE)) {
      const data = fs.readFileSync(PROGRESS_FILE, 'utf-8');
      return JSON.parse(data);
    }
  } catch (error) {
    console.warn('⚠️  Could not load progress file:', error);
  }
  return null;
}

function logProgress(newsletter: any) {
  stats.processed++;
  stats.processedNewsletterIds.push(newsletter.id);
  stats.lastUpdateTime = new Date().toISOString();
  
  const percent = ((stats.processed / stats.total) * 100).toFixed(1);
  const timeRemaining = calculateTimeRemaining();
  const cost = estimateCost();
  
  // Save progress after every newsletter
  saveProgress();
  
  console.log(`\n[${stats.processed}/${stats.total}] (${percent}%) Processing: ${newsletter.subject}`);
  console.log(`   Publisher: ${newsletter.publisher_name}`);
  console.log(`   Time remaining: ${timeRemaining} | Cost so far: $${cost.toFixed(4)} | API calls: ${stats.apiCalls}`);
  console.log(`   Completed: ${stats.processed}/${stats.total} | Failed: ${stats.failed} | Skipped: ${stats.skipped}`);
}

async function main() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  const limit = parseInt(process.env.PROCESS_LIMIT || '100');
  const startFrom = parseInt(process.env.START_FROM || '0');

  try {
    console.log('\n🚀 NEWSLETTER PROCESSING PIPELINE');
    console.log('=====================================\n');
    console.log(`Limit: ${limit} newsletters`);
    console.log(`Starting from: ${startFrom}`);

    // Try to load previous progress
    const savedProgress = loadProgress();
    if (savedProgress) {
      console.log(`📂 Found previous progress: ${savedProgress.processed} processed`);
      stats = savedProgress;
    }

    console.log('');

    // Get existing newsletter IDs for resume capability
    console.log('📋 Checking already processed newsletters...');
    const existingIds = await getExistingNewsletterIds(bigquery);
    
    // Merge saved progress with database IDs
    savedProgress?.processedNewsletterIds.forEach(id => existingIds.add(id));
    
    console.log(`   Found ${existingIds.size} already processed\n`);

    if (!savedProgress) {
      stats.startTime = new Date().toISOString();
    }

    // Process in small batches to avoid memory issues
    // Use cursor-based pagination (WHERE id > lastId) instead of OFFSET to avoid BigQuery memory errors
    const BATCH_SIZE = 1000;
    let totalToProcess = limit;
    let batchNumber = 0;
    let lastProcessedId = savedProgress?.lastProcessedId;
    
    if (!savedProgress) {
      stats.total = limit;
    }

    console.log(`📥 Processing ${limit} newsletters in batches of ${BATCH_SIZE}...`);
    if (lastProcessedId) {
      console.log(`📍 Resuming from newsletter ID: ${lastProcessedId}\n`);
    } else {
      console.log(`📍 Starting from the beginning\n`);
    }

    // Main processing loop: fetch batch, process, repeat
    while (totalToProcess > 0) {
      batchNumber++;
      const batchLimit = Math.min(BATCH_SIZE, totalToProcess);
      
      console.log(`\n═══════════════════════════════════════════════════════════════`);
      console.log(`📦 BATCH ${batchNumber}: Fetching ${batchLimit} newsletters...`);
      if (lastProcessedId) {
        console.log(`   Starting from ID > ${lastProcessedId}`);
      }
      console.log(`═══════════════════════════════════════════════════════════════\n`);
      
      // Cursor-based pagination: much more efficient than OFFSET at scale
      // PRIORITIZE: Process clean inbox messages first, then legacy
      // Use parameterized query to safely handle the lastProcessedId
      const queryOptions: any = {
        query: `
          SELECT *
          FROM \`${PROJECT_ID}.${DATASET_ID}.${MESSAGES_TABLE}\`
          WHERE (LENGTH(body_text) > 500 OR LENGTH(body_html) > 1000)
            AND id NOT IN (SELECT DISTINCT newsletter_id FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` WHERE newsletter_id IS NOT NULL)
            ${lastProcessedId ? 'AND id > @lastProcessedId' : ''}
          ORDER BY 
            CASE WHEN source_inbox = 'clean' THEN 0 ELSE 1 END,
            id ASC
          LIMIT @batchLimit
        `,
        params: {
          batchLimit: batchLimit
        }
      };
      
      if (lastProcessedId) {
        queryOptions.params.lastProcessedId = lastProcessedId;
      }

      // Retry logic for BigQuery resource errors
      let rows: any[] = [];
      let queryAttempts = 0;
      const maxQueryRetries = 3;
      
      while (queryAttempts < maxQueryRetries) {
        try {
          const [queryRows] = await bigquery.query(queryOptions);
          rows = queryRows;
          break; // Success, exit retry loop
        } catch (queryError: any) {
          queryAttempts++;
          const errorMessage = queryError?.message || String(queryError);
          
          // Check if it's a resource/memory error
          if (errorMessage.includes('Resources exceeded') || 
              errorMessage.includes('resourcesExceeded') ||
              errorMessage.includes('memory') ||
              errorMessage.includes('Resources exceeded during query execution')) {
            if (queryAttempts >= maxQueryRetries) {
              console.error(`\n❌ BigQuery resource error after ${maxQueryRetries} attempts:`);
              console.error(`   ${errorMessage}`);
              console.error(`\n💾 Progress saved. The job can be restarted and it will resume from the last processed ID.`);
              saveProgress();
              throw new Error(`BigQuery resource error: ${errorMessage}`);
            }
            
            // Exponential backoff with jitter
            const waitTime = Math.pow(2, queryAttempts) * 1000 + Math.random() * 1000;
            console.warn(`⚠️  BigQuery resource error (attempt ${queryAttempts}/${maxQueryRetries}). Retrying in ${Math.round(waitTime)}ms...`);
            await new Promise(resolve => setTimeout(resolve, waitTime));
          } else {
            // Non-resource error, re-throw immediately
            throw queryError;
          }
        }
      }

      console.log(`✅ Fetched ${rows.length} newsletters from BigQuery\n`);

      if (rows.length === 0) {
        console.log('✅ No more newsletters to fetch - we\'re done!\n');
        break;
      }

      // Process each newsletter in this batch
      for (const newsletter of rows) {
        // Skip if already processed
        if (existingIds.has(newsletter.id)) {
          stats.skipped++;
          console.log(`⏭️  Skipping already processed: ${newsletter.subject}`);
          lastProcessedId = newsletter.id; // Update cursor even for skipped items
          continue;
        }

        logProgress(newsletter);

        try {
          const chunks = await processNewsletterWithEmbeddings(newsletter);

          if (chunks.length > 0) {
            // Insert chunks
            const dataset = bigquery.dataset(DATASET_ID);
            const table = dataset.table(CHUNKS_TABLE);
            
            try {
              await table.insert(chunks);
              stats.chunksCreated += chunks.length;
              console.log(`   ✅ Created ${chunks.length} chunks`);
            } catch (insertError: any) {
              // Handle duplicate insert errors gracefully
              if (insertError?.message?.includes('duplicate') || insertError?.message?.includes('already exists')) {
                console.log(`   ⚠️  Chunks already exist (skipping duplicate insert)`);
                stats.chunksCreated += chunks.length; // Count them anyway for stats
              } else {
                throw insertError; // Re-throw if it's not a duplicate error
              }
            }
          } else {
            console.log(`   ⚠️  No chunks created (insufficient content)`);
          }
          
          // Update cursor after successful processing
          lastProcessedId = newsletter.id;
          stats.lastProcessedId = lastProcessedId;
        } catch (error) {
          stats.failed++;
          console.error(`   ❌ Failed: ${error instanceof Error ? error.message : error}`);
          // Continue processing - don't let one failure stop the pipeline
          // Still update cursor to avoid reprocessing failed items (they'll be skipped on retry)
          lastProcessedId = newsletter.id;
          stats.lastProcessedId = lastProcessedId;
        }
      }

      // Update progress and save after each batch
      totalToProcess -= rows.length;
      saveProgress(); // Save progress after each batch to enable recovery
      
      console.log(`\n✅ Batch ${batchNumber} complete. Processed ${rows.length} newsletters.`);
      console.log(`📊 Progress: ${stats.processed} processed, ${stats.skipped} skipped, ${stats.failed} failed`);
      console.log(`📦 Remaining: ${totalToProcess} newsletters`);
      console.log(`📍 Last processed ID: ${lastProcessedId}\n`);
    }

    // Final summary
    const endTime = new Date().getTime();
    const startTime = new Date(stats.startTime).getTime();
    const elapsedMinutes = (endTime - startTime) / 1000 / 60;

    console.log('\n\n=====================================');
    console.log('PROCESSING COMPLETE');
    console.log('=====================================');
    console.log(`Total newsletters: ${stats.total}`);
    console.log(`Processed: ${stats.processed}`);
    console.log(`Skipped: ${stats.skipped}`);
    console.log(`Failed: ${stats.failed}`);
    console.log(`Chunks created: ${stats.chunksCreated}`);
    console.log(`API calls: ${stats.apiCalls}`);
    console.log(`Total cost: $${estimateCost().toFixed(4)}`);
    console.log(`Elapsed time: ${elapsedMinutes.toFixed(1)} minutes (${(elapsedMinutes / 60).toFixed(2)} hours)`);
    console.log('=====================================\n');

    // Delete progress file on successful completion
    if (fs.existsSync(PROGRESS_FILE)) {
      fs.unlinkSync(PROGRESS_FILE);
      console.log('🗑️  Deleted progress file');
    }

  } catch (error) {
    console.error('\n❌ Pipeline failed:', error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error(`   Error details: ${errorMessage}`);
    
    // Save progress before exiting
    console.log(`\n💾 Progress saved. Resume by running the same command.`);
    if (stats.lastProcessedId) {
      console.log(`📍 Will resume from newsletter ID: ${stats.lastProcessedId}`);
    }
    saveProgress();
    
    // Exit with error code so Cloud Run knows the job failed
    process.exit(1);
  }
}

main();
</file>

<file path="scripts/legacy/README.md">
# Legacy Scripts

This folder contains scripts that are no longer actively used but are kept for reference.

## Moved on 2025-11-05

- `process-newsletters.ts` - Legacy newsletter processing script (replaced by `chunk-new.ts` and `embed-new-chunks.ts`)
- `full-chunk-and-embed.ts` - Legacy combined chunking and embedding script (replaced by separate chunk and embed jobs)
- `run-overnight-tranche1.sh` - Legacy shell script for overnight processing
- `setup-service-account.sh` - Legacy service account setup script (replaced by cloud bootstrap scripts)

These scripts are excluded from TypeScript compilation and are not part of the active deployment pipeline.
</file>

<file path="scripts/legacy/run-overnight-tranche1.sh">
#!/bin/bash

# Newsletter Control Center - Tranche 1 Processing Script
# Processes 15,000 newsletters overnight
# Est. time: ~8 hours
# Est. cost: ~$1.50

echo "🌙 Starting Tranche 1 Processing"
echo "================================="
echo "Newsletters: 0 to 14,999"
echo "Expected completion: ~8 hours"
echo "Expected cost: ~\$1.50"
echo ""
echo "Press Ctrl+C to pause (progress will be saved)"
echo ""
read -p "Press Enter to start..." -n 1

cd "$(dirname "$0")/.."

# Set environment variables
export START_FROM=0
export PROCESS_LIMIT=15000

# Run the processing script
npx tsx scripts/process-newsletters.ts

echo ""
echo "✅ Processing complete!"
</file>

<file path="scripts/legacy/setup-service-account.sh">
#!/bin/bash

# Service Account Setup Script for Newsletter Control Center
# This script creates a service account for BigQuery with proper permissions

set -e  # Exit on any error

PROJECT_ID="newsletter-control-center"
SERVICE_ACCOUNT_NAME="newsletter-bigquery-sa"
SERVICE_ACCOUNT_EMAIL="${SERVICE_ACCOUNT_NAME}@${PROJECT_ID}.iam.gserviceaccount.com"
KEY_FILE="$HOME/newsletter-bigquery-key.json"

echo "🚀 Setting up Service Account for BigQuery authentication..."
echo "Project: $PROJECT_ID"
echo "Service Account: $SERVICE_ACCOUNT_EMAIL"
echo ""

# Check if gcloud is installed and authenticated
if ! command -v gcloud &> /dev/null; then
    echo "❌ gcloud CLI not found. Please install it first:"
    echo "   https://cloud.google.com/sdk/docs/install"
    exit 1
fi

# Check if user is authenticated
if ! gcloud auth list --filter=status:ACTIVE --format="value(account)" | grep -q .; then
    echo "❌ Not authenticated with gcloud. Please run:"
    echo "   gcloud auth login"
    exit 1
fi

# Set the project
echo "📋 Setting project to $PROJECT_ID..."
gcloud config set project $PROJECT_ID

# Create service account (ignore error if it already exists)
echo "👤 Creating service account..."
gcloud iam service-accounts create $SERVICE_ACCOUNT_NAME \
    --display-name="Newsletter BigQuery Service Account" \
    --description="Service account for Newsletter Control Center BigQuery operations" \
    2>/dev/null || echo "   (Service account already exists)"

# Grant BigQuery permissions
echo "🔐 Granting BigQuery permissions..."
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT_EMAIL" \
    --role="roles/bigquery.dataEditor" \
    --quiet

gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT_EMAIL" \
    --role="roles/bigquery.jobUser" \
    --quiet

# Create and download service account key
echo "🔑 Creating service account key..."
gcloud iam service-accounts keys create $KEY_FILE \
    --iam-account=$SERVICE_ACCOUNT_EMAIL \
    --quiet

echo ""
echo "✅ Service Account setup complete!"
echo ""
echo "📁 Key file created at: $KEY_FILE"
echo "📧 Service Account: $SERVICE_ACCOUNT_EMAIL"
echo ""
echo "🔧 Next steps:"
echo "1. Add this line to your .env file:"
echo "   GOOGLE_APPLICATION_CREDENTIALS=$KEY_FILE"
echo ""
echo "2. Update your BigQuery code to use the service account"
echo "3. Test with: npx ts-node scripts/test-bigquery-auth.ts"
echo ""
echo "⚠️  Keep the key file secure and never commit it to version control!"
</file>

<file path="scripts/ops/check-recent-inserts.ts">
import 'dotenv/config';
import { getBigQuery } from '../../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  const bq = getBigQuery();

  console.log('---');
  console.log('RECENT BIGQUERY INSERTS (last 30 minutes)');
  console.log('---\n');

  // Count recent inserts
  const countQuery = `
    SELECT COUNT(*) as count
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE ingested_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 MINUTE)
  `;

  const [countRows] = await bq.query({ query: countQuery, location });
  const count = countRows[0]?.count || 0;
  console.log(`Count: ${count}\n`);

  if (count === 0) {
    console.log('No recent inserts found.\n');
    return;
  }

  // Get 5 most recent
  const recentQuery = `
    SELECT 
      gmail_message_id,
      subject,
      sent_date
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE ingested_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 MINUTE)
    ORDER BY ingested_at DESC
    LIMIT 5
  `;

  const [recentRows] = await bq.query({ query: recentQuery, location });
  console.log('Most recent 5:');
  recentRows.forEach((row: any, i: number) => {
    console.log(`  ${i + 1}. ID: ${row.gmail_message_id}`);
    console.log(`     Subject: ${row.subject || '(no subject)'}`);
    console.log(`     Sent: ${row.sent_date || 'N/A'}`);
  });
  console.log('');
}

if (require.main === module) {
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}
</file>

<file path="scripts/ops/create-uptime-and-alert.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { GoogleAuth } from 'google-auth-library';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';
const SERVICE_NAME = 'ncc-jobs-runner';
const EMAIL = 'john@internationalintrigue.io';
const CHANNEL_ID = process.env.UPTIME_CHANNEL_ID || '';

interface Args {
  apply?: boolean;
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

function shellJSON<T>(cmd: string, allowFail = false): T | null {
  try {
    const result = shell(cmd, allowFail);
    return result.success ? JSON.parse(result.output) : null;
  } catch {
    return null;
  }
}

async function getServiceUrl(): Promise<string> {
  const cmd = `gcloud run services describe ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format=json`;
  const service = shellJSON<any>(cmd, true);
  
  if (!service?.status?.url) {
    throw new Error(`Cannot find service URL for ${SERVICE_NAME}`);
  }
  
  return service.status.url;
}

async function findNotificationChannel(): Promise<string | null> {
  // If channel ID is provided via env var, use it directly
  if (CHANNEL_ID) {
    return CHANNEL_ID.startsWith('projects/') ? CHANNEL_ID : `projects/${PROJECT}/notificationChannels/${CHANNEL_ID}`;
  }
  
  // Try to find channel by listing (may fail due to permissions)
  try {
    const auth = new GoogleAuth({
      scopes: ['https://www.googleapis.com/auth/cloud-platform'],
    });
    const client = await auth.getClient();
    const tokenResponse = await client.getAccessToken();
    const token = tokenResponse.token;
    
    if (!token) {
      return null;
    }
    
    const url = `https://monitoring.googleapis.com/v3/projects/${PROJECT}/notificationChannels`;
    const response = await fetch(url, {
      headers: { Authorization: `Bearer ${token}` },
    });
    
    if (!response.ok) {
      return null;
    }
    
    const data = await response.json();
    const channels = data.notificationChannels || [];
    
    // Find first email channel whose display name contains "Email" or matches project email
    const channel = channels.find((ch: any) => {
      if (ch.type !== 'email') return false;
      const displayName = (ch.displayName || '').toLowerCase();
      const emailAddress = ch.labels?.email_address || '';
      return displayName.includes('email') || emailAddress === EMAIL;
    });
    
    return channel?.name || null;
  } catch {
    return null;
  }
}

async function getNotificationChannel(): Promise<string> {
  // Never create channels - only reuse existing ones
  const channel = await findNotificationChannel();
  
  if (!channel) {
    throw new Error(
      `Missing notification channel. Please set UPTIME_CHANNEL_ID=<channel-id> or ensure an email channel exists with "Email" in the display name. ` +
      `Channels must be created in the Cloud Console (Monitoring > Alerting > Notification Channels).`
    );
  }
  
  return channel;
}

async function checkUptimeCheckExists(checkName: string): Promise<string | null> {
  const cmd = `gcloud monitoring uptime list-configs --project=${PROJECT} --format=json`;
  const checks = shellJSON<Array<{ name: string; displayName: string }>>(cmd, true);
  
  if (!checks) {
    return null;
  }
  
  const found = checks.find(ch => ch.displayName === checkName || ch.name?.includes(checkName));
  return found?.name || null;
}

async function createUptimeCheck(serviceUrl: string): Promise<string> {
  const checkName = 'ncc-health-check';
  const checkDisplayName = 'NCC Health Check';
  
  // Check if exists
  const existingName = await checkUptimeCheckExists(checkName);
  if (existingName) {
    console.log(`✓ Uptime check ${checkDisplayName} already exists: ${existingName}`);
    return existingName;
  }
  
  // Use /health-check (Cloud Run appears to reserve /healthz)
  const healthUrl = `${serviceUrl}/health-check`;
  const url = new URL(healthUrl);
  
  const cmd = `gcloud monitoring uptime create https \
    --display-name="${checkDisplayName}" \
    --hostname="${url.hostname}" \
    --path="${url.pathname}" \
    --project=${PROJECT} \
    --format=json`;
  
  const result = shellJSON<{ name: string }>(cmd, false);
  if (!result?.name) {
    throw new Error('Failed to create uptime check');
  }
  
  console.log(`✓ Created uptime check: ${result.name}`);
  return result.name;
}

async function checkAlertPolicyExists(policyName: string): Promise<boolean> {
  const cmd = `gcloud alpha monitoring policies list --project=${PROJECT} --format=json`;
  const policies = shellJSON<Array<{ name: string; displayName: string }>>(cmd, true);
  
  if (!policies) {
    return false;
  }
  
  return policies.some(p => p.displayName === policyName || p.name?.includes(policyName));
}

async function createAlertPolicy(notificationChannelName: string, uptimeCheckName: string): Promise<void> {
  const policyName = 'NCC Health Alert';
  
  // Check if exists
  const exists = await checkAlertPolicyExists(policyName);
  if (exists) {
    console.log(`✓ Alert policy ${policyName} already exists`);
    return;
  }
  
  // Extract check ID from full resource name
  const checkId = uptimeCheckName.split('/').pop() || '';
  
  // Create alert policy JSON (Monitoring v3 schema)
  const policyJson = {
    displayName: policyName,
    combiner: 'OR',
    conditions: [
      {
        displayName: 'Uptime check failed',
        conditionThreshold: {
          filter: `resource.type="uptime_url" AND metric.type="monitoring.googleapis.com/uptime_check/check_passed" AND metric.labels.check_id="${checkId}"`,
          comparison: 'COMPARISON_LT',
          thresholdValue: 1,
          duration: '600s', // 10 minutes
          trigger: {
            count: 2,
          },
          evaluationMissingData: 'EVALUATION_MISSING_DATA_NO_OP',
        },
      },
    ],
    notificationChannels: [notificationChannelName],
    alertStrategy: {
      autoClose: '1800s', // 30 minutes
    },
  };
  
  const fs = require('fs');
  const os = require('os');
  const path = require('path');
  const policyFile = path.join(os.tmpdir(), `ncc-alert-policy-${Date.now()}.json`);
  fs.writeFileSync(policyFile, JSON.stringify(policyJson, null, 2));
  
  try {
    // Install alpha component if needed (non-interactive)
    const installCmd = `gcloud components install alpha --quiet 2>&1 || true`;
    shell(installCmd, true);
    
    const cmd = `gcloud alpha monitoring policies create \
      --policy-from-file=${policyFile} \
      --project=${PROJECT} \
      --quiet`;
    
    shell(cmd, false);
    console.log(`✓ Created alert policy: ${policyName}`);
  } finally {
    // Clean up temp file
    try {
      fs.unlinkSync(policyFile);
    } catch {
      // Ignore cleanup errors
    }
  }
}

async function main() {
  const argv = await yargs(hideBin(process.argv))
    .option('apply', {
      type: 'boolean',
      default: false,
      description: 'Actually create resources (default: preview only)',
    })
    .parseAsync() as Args;
  
  console.log('---');
  console.log('UPTIME CHECK & ALERT SETUP');
  console.log(`Project: ${PROJECT}`);
  console.log(`Region: ${REGION}`);
  console.log(`Service: ${SERVICE_NAME}`);
  console.log('');
  
  if (!argv.apply) {
    console.log('🔍 PREVIEW MODE (use --apply to create resources)');
    console.log('');
  }
  
  // Get service URL
  console.log('Getting service URL...');
  const serviceUrl = await getServiceUrl();
  const healthUrl = `${serviceUrl}/health-check`;
  console.log(`✓ Service URL: ${serviceUrl}`);
  console.log(`✓ Health endpoint: ${healthUrl}`);
  console.log('');
  
  // Find notification channel (never create)
  console.log('Finding notification channel...');
  let notificationChannelName: string | null = null;
  
  try {
    notificationChannelName = await findNotificationChannel();
    
    if (notificationChannelName) {
      console.log(`✓ Found notification channel: ${notificationChannelName}`);
    } else {
      if (argv.apply) {
        console.error('');
        console.error('❌ ERROR: Notification channel not found.');
        if (CHANNEL_ID) {
          console.error(`   Using provided ID: ${CHANNEL_ID}`);
        } else {
          console.error(`   Set UPTIME_CHANNEL_ID=<channel-id> to use a specific channel`);
        }
        console.error('');
        console.error('   Channels must be created in Cloud Console:');
        console.error('   Monitoring > Alerting > Notification Channels');
        console.error('');
        process.exit(1);
      } else {
        console.log(`📋 Would use notification channel (first email channel with "Email" in name, or set UPTIME_CHANNEL_ID)`);
      }
    }
  } catch (error: any) {
    if (argv.apply) {
      console.error('');
      console.error(`❌ ERROR: ${error.message}`);
      process.exit(1);
      } else {
        console.log(`📋 Would use notification channel (requires UPTIME_CHANNEL_ID if listing fails)`);
      }
  }
  console.log('');
  
  // Check/create uptime check
  console.log('Checking uptime check...');
  let uptimeCheckName = await checkUptimeCheckExists('ncc-health-check');
  
      if (uptimeCheckName) {
        console.log(`✓ Uptime check already exists: ${uptimeCheckName}`);
      } else {
        if (argv.apply) {
          uptimeCheckName = await createUptimeCheck(serviceUrl);
        } else {
          console.log(`📋 Would create uptime check for: ${healthUrl}`);
        }
      }
  console.log('');
  
  // Check/create alert policy
  console.log('Checking alert policy...');
  const policyName = 'NCC Health Alert';
  const alertExists = await checkAlertPolicyExists(policyName);
  
  if (alertExists) {
    console.log(`✓ Alert policy already exists`);
  } else {
    if (argv.apply && notificationChannelName && uptimeCheckName) {
      await createAlertPolicy(notificationChannelName, uptimeCheckName);
    } else {
      console.log(`📋 Would create alert policy:`);
      console.log(`   - Name: ${policyName}`);
      console.log(`   - Combiner: OR`);
      console.log(`   - Condition: Uptime check fails (2 of 3 evaluations in 10 minutes)`);
      console.log(`   - Notification: ${notificationChannelName || 'email channel'}`);
    }
  }
  console.log('');
  
  if (!argv.apply) {
    console.log('---');
    console.log('To create these resources, run:');
    console.log('  npm run ops:alert:apply');
    console.log('---');
  } else {
    console.log('---');
    console.log('✅ Setup complete');
    console.log('---');
  }
}

main().catch((error) => {
  console.error('Fatal error:', error);
  process.exit(1);
});
</file>

<file path="scripts/ops/pipeline-status.ts">
import 'dotenv/config';
import { execSync } from 'child_process';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';

interface JobSummary {
  job: string;
  status: 'PASS' | 'FAIL' | 'UNKNOWN';
  fetched?: number;
  inserted?: number;
  labeled?: number;
  markedRead?: number;
  errors?: string[];
  lastRun?: string;
}

async function getJobExecutionHistory(jobName: string): Promise<any> {
  try {
    const cmd = `gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --limit=1 --format=json`;
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    const data = JSON.parse(output);
    return data[0] || null;
  } catch (error: any) {
    return null;
  }
}

async function getJobLogs(jobName: string, executionName?: string): Promise<string[]> {
  try {
    let filter = `resource.type=cloud_run_job AND resource.labels.job_name=${jobName}`;
    if (executionName) {
      const execId = executionName.split('/').pop();
      filter += ` AND resource.labels.execution_name=${execId}`;
    }
    
    const cmd = `gcloud logging read "${filter}" --limit=100 --format="value(textPayload)" --project=${PROJECT} --freshness=24h`;
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return output.split('\n').filter(line => line.trim().length > 0);
  } catch (error: any) {
    return [];
  }
}

function extractMetrics(logs: string[]): Partial<JobSummary> {
  const metrics: Partial<JobSummary> = {};
  
  for (const line of logs) {
    // Extract fetched count
    const fetchedMatch = line.match(/Gmail:\s*fetched\s+(\d+)\s+messages/i);
    if (fetchedMatch) {
      metrics.fetched = parseInt(fetchedMatch[1], 10);
    }
    
    // Extract inserted count
    const insertedMatch = line.match(/inserted_raw=(\d+)/i) || line.match(/inserted\s+(\d+)\s+messages/i);
    if (insertedMatch) {
      metrics.inserted = parseInt(insertedMatch[1], 10);
    }
    
    // Extract labeled count
    const labeledMatch = line.match(/labeled=(\d+)/i) || line.match(/Gmail:\s*labeled=(\d+)/i);
    if (labeledMatch) {
      metrics.labeled = parseInt(labeledMatch[1], 10);
    }
    
    // Extract marked_read count
    const markedReadMatch = line.match(/marked_read=(\d+)/i);
    if (markedReadMatch) {
      metrics.markedRead = parseInt(markedReadMatch[1], 10);
    }
  }
  
  return metrics;
}

async function checkJob(jobName: string): Promise<JobSummary> {
  const summary: JobSummary = {
    job: jobName,
    status: 'UNKNOWN',
  };
  
  const execution = await getJobExecutionHistory(jobName);
  if (execution) {
    summary.lastRun = execution.metadata?.creationTimestamp || 'unknown';
    const status = execution.status?.conditions?.[0]?.status || 'Unknown';
    if (status === 'True' && execution.status?.conditions?.[0]?.type === 'Complete') {
      summary.status = 'PASS';
    } else if (status === 'True' && execution.status?.conditions?.[0]?.type === 'Failed') {
      summary.status = 'FAIL';
    }
  }
  
  const logs = await getJobLogs(jobName, execution?.metadata?.name);
  const metrics = extractMetrics(logs);
  Object.assign(summary, metrics);
  
  // Check for errors
  const errorLines = logs.filter(line => 
    line.toLowerCase().includes('error') || 
    line.toLowerCase().includes('failed') ||
    line.toLowerCase().includes('exception')
  );
  if (errorLines.length > 0) {
    summary.errors = errorLines.slice(0, 5); // Keep first 5 errors
  }
  
  return summary;
}

async function main(): Promise<void> {
  console.log('═══════════════════════════════════════════════════════════════');
  console.log('PIPELINE STATUS REPORT');
  console.log('═══════════════════════════════════════════════════════════════\n');
  
  const jobs = ['ncc-ingest-me', 'ncc-ingest-other'];
  const summaries: JobSummary[] = [];
  
  for (const jobName of jobs) {
    console.log(`Checking ${jobName}...`);
    const summary = await checkJob(jobName);
    summaries.push(summary);
  }
  
  console.log('\n═══════════════════════════════════════════════════════════════');
  console.log('SUMMARY');
  console.log('═══════════════════════════════════════════════════════════════\n');
  
  for (const summary of summaries) {
    console.log(`Job: ${summary.job}`);
    console.log(`  Status: ${summary.status}`);
    if (summary.lastRun) {
      console.log(`  Last Run: ${summary.lastRun}`);
    }
    if (summary.fetched !== undefined) {
      console.log(`  Fetched: ${summary.fetched}`);
    }
    if (summary.inserted !== undefined) {
      console.log(`  Inserted: ${summary.inserted}`);
    }
    if (summary.labeled !== undefined) {
      console.log(`  Labeled: ${summary.labeled}`);
    }
    if (summary.markedRead !== undefined) {
      console.log(`  Marked Read: ${summary.markedRead}`);
    }
    if (summary.errors && summary.errors.length > 0) {
      console.log(`  Errors: ${summary.errors.length} found`);
      summary.errors.forEach(err => console.log(`    - ${err.substring(0, 100)}`));
    }
    console.log('');
  }
}

if (require.main === module) {
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}
</file>

<file path="scripts/ops/verify-health.ts">
import 'dotenv/config';
import { execSync, spawnSync } from 'child_process';

function shell(cmd: string): string {
  try {
    return execSync(cmd, { encoding: 'utf8', stdio: ['ignore', 'pipe', 'pipe'] }).toString().trim();
  } catch (error: any) {
    const stderr = error.stderr?.toString() || '';
    throw new Error(`Command failed: ${cmd}\n${stderr}`);
  }
}

function getRunnerUrl(): string {
  // Try env var first
  const url = process.env.NCC_RUNNER_URL;
  if (url) {
    return url;
  }
  
  // Fall back to gcloud describe
  const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const REGION = process.env.NCC_REGION || 'us-central1';
  const SERVICE_NAME = 'ncc-jobs-runner';
  
  // Check if we need to switch to human user for auth
  let currentAccount = '';
  try {
    currentAccount = shell('gcloud config get-value account');
  } catch {
    currentAccount = '';
  }
  
  const needsHumanAuth = currentAccount.endsWith('.iam.gserviceaccount.com');
  
  if (needsHumanAuth) {
    console.log('Switching to human user for service URL lookup...');
    spawnSync('gcloud', ['auth', 'login', '--update-adc', '--quiet'], { stdio: 'inherit' });
    shell(`gcloud config set project ${PROJECT}`);
  }
  
  try {
    const url = shell(`gcloud run services describe ${SERVICE_NAME} --region=${REGION} --project=${PROJECT} --format="value(status.url)"`);
    return url;
  } finally {
    // Switch back if needed
    if (needsHumanAuth) {
      const KEY = process.env.GOOGLE_APPLICATION_CREDENTIALS || 'secrets/gcp/ncc-local-dev.json';
      const SA = 'newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com';
      spawnSync('gcloud', ['auth', 'activate-service-account', SA, '--key-file', KEY, '--project', PROJECT], { stdio: 'inherit' });
    }
  }
}

async function main(): Promise<void> {
  const runnerUrl = getRunnerUrl();
  const healthUrl = `${runnerUrl}/health-check`;
  
  try {
    const response = await fetch(healthUrl);
    const status = response.status;
    
    if (status === 401 || status === 403) {
      console.error('HEALTH FAIL: Health endpoint must allow unauthenticated access for Uptime Checks.');
      process.exit(1);
    }
    
    // Parse response body
    const text = await response.text();
    let data: any;
    try {
      data = JSON.parse(text);
    } catch {
      console.error(`HEALTH FAIL: HTTP ${status} - Invalid JSON response: ${text.substring(0, 100)}`);
      process.exit(1);
    }
    
    // Check if endpoint is accessible (200 OK) even if health check fails
    if (status === 200) {
      console.log(`HEALTH ENDPOINT ACCESSIBLE: HTTP ${status}`);
      console.log(`Response: ${JSON.stringify(data, null, 2)}`);
    }
    
    const jobsOk = data.jobs_ok === true;
    const coverageOk = data.coverage_ok === true;
    
    if (jobsOk && coverageOk) {
      console.log('HEALTH OK');
      process.exit(0);
    } else {
      const reasons: string[] = [];
      if (!jobsOk) reasons.push('jobs_ok=false');
      if (!coverageOk) reasons.push('coverage_ok=false');
      console.log(`HEALTH ENDPOINT ACCESSIBLE (health check failed: ${reasons.join(', ')})`);
      // Exit 0 because endpoint is accessible (actual health status is separate)
      process.exit(0);
    }
  } catch (error: any) {
    console.error(`HEALTH FAIL: ${error.message || 'unknown error'}`);
    process.exit(1);
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Fatal error:', err.message || err);
    process.exit(1);
  });
}
</file>

<file path="scripts/ops/verify-ingest-live.ts">
import 'dotenv/config';
import { execSync } from 'child_process';
import { getGmail } from '../../src/gmail/client';
import type { gmail_v1 } from 'googleapis';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';
const PROCESSED_LABEL = process.env.GMAIL_PROCESSED_LABEL || 'Ingested';
// Use the exact query that jobs use (from deploy-jobs.ts)
const JOB_QUERY = 'is:unread -label:Ingested';

interface JobMetrics {
  fetched?: number;
  inserted?: number;
  labeled?: number;
  markedRead?: number;
  errors: string[];
}

interface MessageState {
  id: string;
  hasProcessedLabel: boolean;
  isRead: boolean;
}

function shell(cmd: string, allowFail = false): { success: boolean; output: string } {
  try {
    const output = execSync(cmd, { encoding: 'utf8', stdio: 'pipe' });
    return { success: true, output: output.trim() };
  } catch (error: any) {
    if (allowFail) {
      return { success: false, output: error.stderr?.toString() || error.message || String(error) };
    }
    throw new Error(`Command failed: ${cmd}\n${error.stderr?.toString() || error.message}`);
  }
}

function extractMetrics(logText: string): JobMetrics {
  const metrics: JobMetrics = { errors: [] };
  
  // Extract fetched count
  const fetchedMatch = logText.match(/Gmail:\s*fetched\s+(\d+)\s+messages/i);
  if (fetchedMatch) {
    metrics.fetched = parseInt(fetchedMatch[1], 10);
  }
  
  // Extract inserted count
  const insertedMatch = logText.match(/inserted_raw=(\d+)/i) || logText.match(/inserted\s+(\d+)\s+messages/i);
  if (insertedMatch) {
    metrics.inserted = parseInt(insertedMatch[1], 10);
  }
  
  // Extract labeled count
  const labeledMatch = logText.match(/labeled=(\d+)/i);
  if (labeledMatch) {
    metrics.labeled = parseInt(labeledMatch[1], 10);
  }
  
  // Extract marked_read count
  const markedReadMatch = logText.match(/marked_read=(\d+)/i);
  if (markedReadMatch) {
    metrics.markedRead = parseInt(markedReadMatch[1], 10);
  }
  
  // Extract errors
  const errorLines = logText.split('\n').filter(line => 
    line.toLowerCase().includes('error') && 
    !line.toLowerCase().includes('no error')
  );
  metrics.errors = errorLines.slice(0, 5);
  
  return metrics;
}

async function getMessageIdsBefore(inbox: 'me' | 'other'): Promise<string[]> {
  const gmail = await getGmail(inbox);
  const query = `${JOB_QUERY} newer_than:1d`;
  
  try {
    const listRes = await gmail.users.messages.list({
      userId: 'me',
      q: query,
      maxResults: 10,
    });
    
    const messageIds = (listRes.data.messages || [])
      .map(m => m.id!)
      .filter(Boolean);
    
    return messageIds;
  } catch (error: any) {
    console.error(`Failed to fetch messages for ${inbox}: ${error.message}`);
    return [];
  }
}

async function getMessageStates(inbox: 'me' | 'other', messageIds: string[]): Promise<MessageState[]> {
  if (messageIds.length === 0) return [];
  
  const gmail = await getGmail(inbox);
  
  // Get labels map
  let labelsMap = new Map<string, string>();
  try {
    const labelsRes = await gmail.users.labels.list({ userId: 'me' });
    if (labelsRes.data.labels) {
      for (const label of labelsRes.data.labels) {
        if (label.id && label.name) {
          labelsMap.set(label.id, label.name);
        }
      }
    }
  } catch (error: any) {
    console.error(`Failed to list labels for ${inbox}: ${error.message}`);
  }
  
  const states: MessageState[] = [];
  
  for (const msgId of messageIds) {
    try {
      const msgRes = await gmail.users.messages.get({
        userId: 'me',
        id: msgId,
        format: 'metadata',
      });
      
      const labelIds = msgRes.data.labelIds || [];
      const labelNames = labelIds.map(id => labelsMap.get(id) || '').filter(Boolean);
      
      const hasProcessedLabel = labelNames.some(name => 
        name.toLowerCase() === PROCESSED_LABEL.toLowerCase()
      );
      const isRead = !labelIds.includes('UNREAD');
      
      states.push({
        id: msgId,
        hasProcessedLabel,
        isRead,
      });
    } catch (error: any) {
      // Message might have been deleted or inaccessible
      console.error(`Failed to get message ${msgId}: ${error.message}`);
    }
  }
  
  return states;
}

async function executeJob(jobName: string, inbox: 'me' | 'other'): Promise<{ metrics: JobMetrics; execName: string | null }> {
  console.log(`Executing ${jobName}...`);
  
  // Get script path from job config
  const jobDescCmd = `gcloud run jobs describe ${jobName} --region=${REGION} --project=${PROJECT} --format=json`;
  const jobDesc = shell(jobDescCmd, true);
  
  let scriptPath = 'dist/scripts/ingest-gmail.js';
  if (jobDesc.success) {
    try {
      const jobData = JSON.parse(jobDesc.output);
      const containers = jobData.spec?.template?.spec?.template?.spec?.containers || [];
      const container = containers[0];
      const existingArgs = container?.args || [];
      if (existingArgs.length > 0) {
        scriptPath = existingArgs[0];
      }
    } catch {
      // Use default
    }
  }
  
  // Execute job
  const execCmd = `gcloud run jobs execute ${jobName} --region=${REGION} --project=${PROJECT} --args="${scriptPath},--no-dry-run,--limit=5,--inbox,${inbox}" --wait --format=json`;
  const execResult = shell(execCmd, true);
  
  if (!execResult.success) {
    const errorMsg = execResult.output;
    console.error(`  ❌ Execution failed: ${errorMsg.substring(0, 200)}`);
    return {
      metrics: { errors: [errorMsg] },
      execName: null,
    };
  }
  
  // Parse execution name
  let execName: string | null = null;
  try {
    const execData = JSON.parse(execResult.output);
    if (execData.metadata?.name) {
      execName = execData.metadata.name.split('/').pop() || null;
    }
  } catch {
    // Try to get from list
    const listCmd = `gcloud run jobs executions list --job=${jobName} --region=${REGION} --project=${PROJECT} --format=json --limit=1`;
    const listResult = shell(listCmd, true);
    if (listResult.success) {
      try {
        const executions = JSON.parse(listResult.output);
        if (executions && executions.length > 0 && executions[0].name) {
          execName = executions[0].name.split('/').pop() || null;
        }
      } catch {}
    }
  }
  
  // Wait for logs to appear
  await new Promise(resolve => setTimeout(resolve, 5000));
  
  // Get logs
  const logFilter = execName
    ? `resource.type=cloud_run_job AND resource.labels.job_name=${jobName} AND resource.labels.execution_name=${execName}`
    : `resource.type=cloud_run_job AND resource.labels.job_name=${jobName}`;
  
  const logCmd = `gcloud logging read "${logFilter}" --limit=200 --format="value(textPayload)" --project=${PROJECT} --freshness=10m`;
  const logResult = shell(logCmd, true);
  
  const metrics = extractMetrics(logResult.output);
  
  return { metrics, execName };
}

async function verifyInbox(inbox: 'me' | 'other'): Promise<{
  metrics: JobMetrics;
  beforeStates: MessageState[];
  afterStates: MessageState[];
  changed: number;
}> {
  const jobName = `ncc-ingest-${inbox}`;
  
  // Step 1: Get message IDs before execution
  console.log(`\n[${inbox.toUpperCase()}] Fetching candidate messages...`);
  const messageIds = await getMessageIdsBefore(inbox);
  console.log(`  Found ${messageIds.length} candidate message(s)`);
  
  if (messageIds.length === 0) {
    console.log(`  ⚠️  No unread messages found matching query: ${JOB_QUERY} newer_than:1d`);
    return {
      metrics: { errors: [] },
      beforeStates: [],
      afterStates: [],
      changed: 0,
    };
  }
  
  // Step 2: Get initial state
  const beforeStates = await getMessageStates(inbox, messageIds);
  const unprocessedCount = beforeStates.filter(s => !s.hasProcessedLabel && !s.isRead).length;
  console.log(`  ${unprocessedCount} message(s) are unread and unprocessed`);
  
  // Step 3: Execute job
  const { metrics, execName } = await executeJob(jobName, inbox);
  console.log(`  Execution: ${execName || 'unknown'}`);
  if (metrics.fetched !== undefined) console.log(`  Fetched: ${metrics.fetched}`);
  if (metrics.inserted !== undefined) console.log(`  Inserted: ${metrics.inserted}`);
  if (metrics.labeled !== undefined) console.log(`  Labeled: ${metrics.labeled}`);
  if (metrics.markedRead !== undefined) console.log(`  Marked read: ${metrics.markedRead}`);
  if (metrics.errors.length > 0) {
    console.log(`  Errors: ${metrics.errors.length}`);
    metrics.errors.slice(0, 2).forEach(err => {
      const shortErr = err.length > 150 ? err.substring(0, 150) + '...' : err;
      console.log(`    - ${shortErr}`);
    });
  }
  
  // Step 4: Wait a bit for Gmail API to reflect changes
  await new Promise(resolve => setTimeout(resolve, 3000));
  
  // Step 5: Get state after execution
  const afterStates = await getMessageStates(inbox, messageIds);
  
  // Step 6: Count changes
  const changed = afterStates.filter((after, idx) => {
    const before = beforeStates[idx];
    if (!before) return false;
    // Changed if: now has processed label (didn't before) OR now is read (wasn't before)
    return (!before.hasProcessedLabel && after.hasProcessedLabel) ||
           (!before.isRead && after.isRead);
  }).length;
  
  console.log(`  Changed: ${changed} message(s)`);
  
  return {
    metrics,
    beforeStates,
    afterStates,
    changed,
  };
}

async function main(): Promise<void> {
  console.log('═══════════════════════════════════════════════════════════════');
  console.log('VERIFY INGEST LIVE');
  console.log('═══════════════════════════════════════════════════════════════');
  console.log(`Query: ${JOB_QUERY}`);
  console.log(`Processed Label: ${PROCESSED_LABEL}\n`);
  
  // Verify both inboxes
  const meResult = await verifyInbox('me');
  const otherResult = await verifyInbox('other');
  
  // Determine PASS/FAIL
  const meChanged = meResult.changed;
  const otherChanged = otherResult.changed;
  const meLabeled = meResult.metrics.labeled || 0;
  const otherLabeled = otherResult.metrics.labeled || 0;
  const meMarkedRead = meResult.metrics.markedRead || 0;
  const otherMarkedRead = otherResult.metrics.markedRead || 0;
  
  const passed = meChanged >= 1 && otherChanged >= 1;
  
  console.log('\n═══════════════════════════════════════════════════════════════');
  if (passed) {
    console.log(`VERIFY: PASS (me: ${meLabeled} labeled/${meMarkedRead} marked, other: ${otherLabeled} labeled/${otherMarkedRead} marked)`);
  } else {
    let reason = '';
    if (meChanged === 0 && otherChanged === 0) {
      reason = 'No messages changed in either inbox';
    } else if (meChanged === 0) {
      reason = 'No messages changed in me inbox';
    } else {
      reason = 'No messages changed in other inbox';
    }
    
    // Add hints
    const hints: string[] = [];
    if (meResult.beforeStates.length === 0 && otherResult.beforeStates.length === 0) {
      hints.push('No unread messages found - check query or wait for new emails');
    }
    if (meLabeled === 0 && otherLabeled === 0) {
      hints.push('No labels applied - check token scope (gmail.modify), label name, or GMAIL_READONLY=false');
    }
    if (meMarkedRead === 0 && otherMarkedRead === 0) {
      hints.push('No messages marked read - check GMAIL_MARK_READ=true');
    }
    if (meResult.metrics.errors.length > 0 || otherResult.metrics.errors.length > 0) {
      hints.push('Job errors detected - check logs');
    }
    
    console.log(`VERIFY: FAIL (reason: ${reason}${hints.length > 0 ? '; ' + hints.join('; ') : ''})`);
  }
  console.log('═══════════════════════════════════════════════════════════════\n');
  
  process.exit(passed ? 0 : 1);
}

if (require.main === module) {
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}
</file>

<file path="scripts/backfill-sent-date.ts">
import 'dotenv/config';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { getBigQuery, getTable } from '../src/bq/client';
import { getGmail } from '../src/gmail/client';
import { getHeader } from '../src/lib/parseMessage';
import type { gmail_v1 } from 'googleapis';

// Helper to parse Date header string (copied from ingest-gmail.ts)
function parseHeaderDate(raw?: string): Date | null {
  if (!raw) return null;
  // remove " (UTC)" or similar comment blocks to help the parser
  const cleaned = raw.replace(/\s+\([^)]*\)/g, ' ').trim();
  const d = new Date(cleaned);
  return Number.isNaN(d.getTime()) ? null : d;
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('limit', {
      type: 'number',
      default: 200,
      description: 'Number of rows to process',
    })
    .option('dry-run', {
      type: 'boolean',
      default: true,
      description: 'Run in dry-run mode (no actual updates)',
    })
    .parse();

  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();
  const limit = argv.limit;
  const dryRun = argv['dry-run'];

  console.log('Backfill Config:');
  console.log(`  project: ${projectId}`);
  console.log(`  dataset: ${datasetId}`);
  console.log(`  limit: ${limit}`);
  console.log(`  dry_run: ${dryRun}\n`);

  // Check if internal_date_ms or internal_date columns exist
  const columnsQuery = `
    SELECT column_name
    FROM \`${projectId}.${datasetId}.INFORMATION_SCHEMA.COLUMNS\`
    WHERE table_name = 'raw_emails'
      AND column_name IN ('internal_date_ms', 'internal_date')
  `;

  const [columnRows] = await bq.query({
    query: columnsQuery,
    location,
  });

  const availableColumns = (columnRows as Array<{ column_name: string }>).map((r) => r.column_name);
  const hasInternalDateMs = availableColumns.includes('internal_date_ms');
  const hasInternalDate = availableColumns.includes('internal_date');

  // Build column selection based on what's available
  const selectColumns = ['gmail_message_id', 'inbox', 'ingested_at'];
  if (hasInternalDateMs) selectColumns.push('internal_date_ms');
  if (hasInternalDate) selectColumns.push('internal_date');

  // Select rows with NULL sent_date
  const selectQuery = `
    SELECT ${selectColumns.join(', ')}
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE sent_date IS NULL
    ORDER BY ingested_at DESC
    LIMIT @limit
  `;

  const [rows] = await bq.query({
    query: selectQuery,
    params: { limit },
    location,
  });

  const nullRows = rows as Array<{
    gmail_message_id: string;
    inbox: string;
    ingested_at: string;
    internal_date_ms?: number;
    internal_date?: number;
  }>;

  console.log(`Found ${nullRows.length} rows with NULL sent_date\n`);

  let checked = 0;
  let updated = 0;
  let skipped = 0;

  // Group by inbox to minimize Gmail API calls
  const inboxGroups = new Map<string, gmail_v1.Gmail>();
  const getGmailClient = async (inbox: string): Promise<gmail_v1.Gmail> => {
    const inboxType = inbox === 'other' ? 'other' : 'me';
    if (!inboxGroups.has(inboxType)) {
      inboxGroups.set(inboxType, await getGmail(inboxType));
    }
    return inboxGroups.get(inboxType)!;
  };

  for (const row of nullRows) {
    checked++;
    let sentDate: Date | null = null;

    // Try internal_date_ms or internal_date column first
    if (hasInternalDateMs && row.internal_date_ms !== undefined && row.internal_date_ms !== null) {
      const ms = Number(row.internal_date_ms);
      if (Number.isFinite(ms) && ms > 0) {
        sentDate = new Date(ms);
      }
    } else if (hasInternalDate && row.internal_date !== undefined && row.internal_date !== null) {
      const ms = Number(row.internal_date);
      if (Number.isFinite(ms) && ms > 0) {
        sentDate = new Date(ms);
      }
    }

    // If still no date, fetch from Gmail API
    if (!sentDate) {
      try {
        const gmail = await getGmailClient(row.inbox);
        const msgRes = await gmail.users.messages.get({
          userId: 'me',
          id: row.gmail_message_id,
          format: 'metadata',
        });

        const msg = msgRes.data;
        if (msg.internalDate) {
          const ms = Number(msg.internalDate);
          if (Number.isFinite(ms) && ms > 0) {
            sentDate = new Date(ms);
          }
        }

        // Fallback to Date header
        if (!sentDate) {
          const dateHeaderString = getHeader(msg, 'Date');
          sentDate = parseHeaderDate(dateHeaderString);
        }
      } catch (error: any) {
        console.error(`Error fetching message ${row.gmail_message_id}:`, error.message);
        skipped++;
        continue;
      }
    }

    if (!sentDate) {
      skipped++;
      continue;
    }

    const sentDateIso = sentDate.toISOString();

    if (dryRun) {
      console.log(`[DRY RUN] Would update ${row.gmail_message_id}: ${sentDateIso}`);
      updated++;
    } else {
      // Update using parameterized query
      const updateQuery = `
        UPDATE \`${projectId}.${datasetId}.raw_emails\`
        SET sent_date = @sentDate
        WHERE gmail_message_id = @gmailMessageId
      `;

      try {
        await bq.query({
          query: updateQuery,
          params: {
            sentDate: sentDateIso,
            gmailMessageId: row.gmail_message_id,
          },
          location,
        });
        updated++;
      } catch (error: any) {
        console.error(`Error updating ${row.gmail_message_id}:`, error.message);
        skipped++;
      }
    }
  }

  console.log(`\nResults: checked=${checked}, updated=${updated}, skipped=${skipped}`);
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/chunk-new.ts">
import 'dotenv/config';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { getBigQuery, getTable } from '../src/bq/client';
import { htmlToText } from '../src/lib/parseMessage';
import { v4 as uuidv4 } from 'uuid';

interface ChunkRow {
  chunk_id: string;
  gmail_message_id: string;
  publisher_id: string | null;
  source_part: string | null;
  char_start: number | null;
  char_end: number | null;
  chunk_index: number;
  chunk_text: string;
  created_at: string;
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('limit', {
      type: 'number',
      default: 10,
      description: 'Number of emails to process',
    })
    .option('dry-run', {
      type: 'boolean',
      default: true,
      description: 'Run in dry-run mode (no actual writes)',
    })
    .parse();

  const projectId = process.env.BQ_PROJECT_ID;
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();
  const limit = argv.limit;
  const dryRun = argv['dry-run'];

  console.log('Chunk Config:');
  console.log(`  project: ${projectId}`);
  console.log(`  dataset: ${datasetId}`);
  console.log(`  location: ${location}`);
  console.log(`  limit: ${limit}`);
  console.log(`  dry_run: ${dryRun}\n`);

  // Select emails that don't have chunks yet
  const selectQuery = `
    SELECT 
      gmail_message_id,
      body_html,
      body_text,
      sent_date
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE gmail_message_id NOT IN (
      SELECT DISTINCT gmail_message_id 
      FROM \`${projectId}.${datasetId}.chunks\`
      WHERE gmail_message_id IS NOT NULL
    )
    ORDER BY sent_date DESC NULLS LAST
    LIMIT @limit
  `;

  const [rows] = await bq.query({
    query: selectQuery,
    params: { limit },
    location,
  });

  const selectedEmails = rows as Array<{
    gmail_message_id: string;
    body_html: string | null;
    body_text: string | null;
    sent_date: string | null;
  }>;

  console.log(`Selected ${selectedEmails.length} emails to chunk\n`);

  let tooShort = 0;
  let chunksBuilt = 0;
  const allChunks: ChunkRow[] = [];

  for (const email of selectedEmails) {
    // Pick content: prefer HTML, fall back to text
    const content = email.body_html
      ? htmlToText(email.body_html)
      : email.body_text || '';

    if (content.length < 10) {
      tooShort++;
      continue;
    }

    // Split into ~800-char chunks with 100-char overlap
    const chunks = splitIntoChunks(content, 800, 100);

    for (let i = 0; i < chunks.length; i++) {
      const chunkText = chunks[i];
      const charStart = content.indexOf(chunkText);
      const charEnd = charStart + chunkText.length;

      allChunks.push({
        chunk_id: uuidv4(),
        gmail_message_id: email.gmail_message_id,
        publisher_id: null,
        source_part: null,
        char_start: charStart >= 0 ? charStart : null,
        char_end: charEnd >= 0 ? charEnd : null,
        chunk_index: i,
        chunk_text: chunkText,
        created_at: new Date().toISOString(),
      });

      chunksBuilt++;
    }
  }

  console.log(`Results:`);
  console.log(`  selected_emails: ${selectedEmails.length}`);
  console.log(`  too_short: ${tooShort}`);
  console.log(`  chunks_built: ${chunksBuilt}`);

  if (dryRun) {
    console.log('\n[DRY RUN] Would insert chunks if --no-dry-run');
    return;
  }

  if (allChunks.length === 0) {
    console.log('No chunks to insert.');
    return;
  }

  // Check for existing chunks (idempotency)
  const existingChunkKeys = new Set<string>();
  if (allChunks.length > 0) {
    const gmailIds = Array.from(new Set(allChunks.map((c) => c.gmail_message_id)));
    const existingQuery = `
      SELECT gmail_message_id, chunk_index
      FROM \`${projectId}.${datasetId}.chunks\`
      WHERE gmail_message_id IN UNNEST(@gmailIds)
    `;
    const [existingRows] = await bq.query({
      query: existingQuery,
      params: { gmailIds },
      location,
    });
    for (const row of existingRows as Array<{ gmail_message_id: string; chunk_index: number }>) {
      existingChunkKeys.add(`${row.gmail_message_id}:${row.chunk_index}`);
    }
  }

  // Filter out existing chunks
  const newChunks = allChunks.filter(
    (c) => !existingChunkKeys.has(`${c.gmail_message_id}:${c.chunk_index}`)
  );

  if (newChunks.length === 0) {
    console.log('All chunks already exist. Nothing to insert.');
    return;
  }

  // Insert new chunks in batches (BigQuery has limits on insert size)
  const chunksTable = await getTable('chunks');
  const INSERT_BATCH_SIZE = 500;
  
  let totalInserted = 0;
  for (let i = 0; i < newChunks.length; i += INSERT_BATCH_SIZE) {
    const batch = newChunks.slice(i, i + INSERT_BATCH_SIZE);
    await chunksTable.insert(batch);
    totalInserted += batch.length;
  }

  console.log(`Inserted ${totalInserted} chunks`);
}

// Simple chunking: split text into ~targetSize chunks with overlap
function splitIntoChunks(text: string, targetSize: number, overlap: number): string[] {
  if (text.length <= targetSize) {
    return [text];
  }

  const chunks: string[] = [];
  let start = 0;

  while (start < text.length) {
    const end = Math.min(start + targetSize, text.length);
    chunks.push(text.slice(start, end));
    
    // Advance start position, ensuring we make progress
    const nextStart = end - overlap;
    if (nextStart <= start) {
      // Prevent infinite loop: ensure we always advance
      start = end;
    } else {
      start = nextStart;
    }
    
    // Safety check: if we've reached the end, break
    if (end >= text.length) {
      break;
    }
  }

  return chunks;
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/create-unified-views.ts">
import 'dotenv/config';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { getBigQuery } from '../src/bq/client';
import * as fs from 'fs/promises';
import * as path from 'path';

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('dry-run', {
      type: 'boolean',
      default: true,
      description: 'Run in dry-run mode (print statements, do not execute)',
    })
    .parse();

  const projectId = process.env.BQ_PROJECT_ID;
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const dryRun = argv['dry-run'];
  const sqlFilePath = path.resolve(__dirname, '../docs/UNIFIED_VIEWS.sql');

  console.log('Config:');
  console.log(`  project: ${projectId}`);
  console.log(`  dataset: ${datasetId}`);
  console.log(`  location: ${location}`);
  console.log(`  sql_file: ${sqlFilePath}`);
  console.log(`  dry_run: ${dryRun}\n`);

  // Read SQL file
  let sqlContent: string;
  try {
    sqlContent = await fs.readFile(sqlFilePath, 'utf8');
  } catch (error: any) {
    throw new Error(`Failed to read SQL file: ${error.message}`);
  }

  // Split on semicolons and filter out empty/whitespace-only statements
  const statements = sqlContent
    .split(';')
    .map(s => s.trim())
    .filter(s => s.length > 0 && s.toUpperCase().includes('CREATE OR REPLACE VIEW'));

  if (statements.length === 0) {
    throw new Error('No CREATE OR REPLACE VIEW statements found in SQL file');
  }

  console.log(`Found ${statements.length} CREATE OR REPLACE VIEW statement(s)\n`);

  const bq = getBigQuery();

  for (let i = 0; i < statements.length; i++) {
    const statement = statements[i];
    const statementNum = i + 1;

    // Extract view name from statement for logging
    // Handle both backtick-quoted and unquoted formats: `project.dataset.view` or project.dataset.view
    const viewMatch = statement.match(/CREATE OR REPLACE VIEW\s+`?([^`\s]+)`?/i);
    const viewName = viewMatch ? viewMatch[1].replace(/`/g, '') : `statement_${statementNum}`;

    if (dryRun) {
      console.log(`--- Statement ${statementNum}: ${viewName} ---`);
      console.log(statement);
      console.log('---\n');
    } else {
      try {
        await bq.query({
          query: statement,
          location,
        });
        console.log(`Created/updated view: ${viewName}`);
      } catch (error: any) {
        const errorMsg = error.message || String(error);
        throw new Error(`Statement ${statementNum} (${viewName}) failed: ${errorMsg}`);
      }
    }
  }

  if (dryRun) {
    console.log('[DRY RUN] Would execute statements above if --no-dry-run');
  } else {
    console.log(`\nSuccessfully created/updated ${statements.length} view(s)`);
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/embed-new-chunks.ts">
import 'dotenv/config';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { getBigQuery, getTable } from '../src/bq/client';
import { embedBatch } from '../src/embeddings/vertex';
import type { Table } from '@google-cloud/bigquery';

interface ChunkRow {
  chunk_id: string;
  gmail_message_id: string;
  chunk_index: number;
  chunk_text: string;
}

interface EmbeddingRow {
  chunk_id: string;
  model: string;
  dim: number;
  embedding: number[];
  created_at: string;
}

async function insertRowsSafe(
  table: Table,
  rows: any[],
  minBatch = 25,
  attempt = 1
): Promise<number> {
  // Inserts rows; if payload too big or RangeError, split batch recursively.
  // Returns number of rows successfully inserted.
  if (rows.length === 0) return 0;

  try {
    await table.insert(rows);
    return rows.length;
  } catch (err: any) {
    const msg = String(err?.message || err);
    const tooBig =
      msg.includes('Request payload size exceeds') ||
      msg.includes('request too large') ||
      msg.includes('413') ||
      msg.includes('Invalid string length') ||
      msg.includes('RangeError');

    if (tooBig && rows.length > minBatch) {
      const mid = Math.floor(rows.length / 2);
      const left = rows.slice(0, mid);
      const right = rows.slice(mid);
      const a = await insertRowsSafe(table, left, minBatch, attempt + 1);
      const b = await insertRowsSafe(table, right, minBatch, attempt + 1);
      return a + b;
    }

    // transient retry (Backoff on 5xx/EOF)
    const transient =
      msg.includes('internal') ||
      msg.includes('EAI_AGAIN') ||
      msg.includes('500') ||
      msg.includes('503') ||
      msg.includes('retry');

    if (transient && attempt <= 3) {
      const delay = 500 * attempt;
      await new Promise((r) => setTimeout(r, delay));
      return insertRowsSafe(table, rows, minBatch, attempt + 1);
    }

    throw err;
  }
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('limit', {
      type: 'number',
      default: 100,
      description: 'Number of chunks to process',
    })
    .option('dry-run', {
      type: 'boolean',
      default: true,
      description: 'Run in dry-run mode (no actual writes)',
    })
    .option('insert-batch', {
      type: 'number',
      description: 'Number of embeddings to insert per batch',
    })
    .parse();

  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const bqLocation = process.env.BQ_LOCATION || 'US';
  // Vertex AI uses region codes, not BigQuery locations
  const locationMap: Record<string, string> = {
    'US': 'us-central1',
    'EU': 'europe-west1',
    'asia-northeast1': 'asia-northeast1',
  };
  const vertexLocation = process.env.EMB_LOCATION || locationMap[bqLocation] || 'us-central1';
  const batchSize = parseInt(process.env.EMB_BATCH_SIZE || '32', 10);
  const insertBatchEnv = process.env.EMB_INSERT_BATCH ? Number(process.env.EMB_INSERT_BATCH) : null;
  const insertBatch = argv['insert-batch'] ?? insertBatchEnv ?? 500;

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();
  const limit = argv.limit;
  const dryRun = argv['dry-run'];

  console.log('Embed Config:');
  console.log(`  project: ${projectId}`);
  console.log(`  dataset: ${datasetId}`);
  console.log(`  location: ${vertexLocation}`);
  console.log(`  limit: ${limit}`);
  console.log(`  batch_size: ${batchSize}`);
  console.log(`  insert_batch_size: ${insertBatch}`);
  console.log(`  dry_run: ${dryRun}\n`);

  // Select chunks that don't have embeddings yet
  const selectQuery = `
    SELECT 
      ch.chunk_id,
      ch.gmail_message_id,
      ch.chunk_index,
      ch.chunk_text
    FROM \`${projectId}.${datasetId}.chunks\` ch
    WHERE NOT EXISTS (
      SELECT 1 
      FROM \`${projectId}.${datasetId}.chunk_embeddings\` ce
      WHERE ce.chunk_id = ch.chunk_id
    )
    ORDER BY ch.created_at DESC
    LIMIT @limit
  `;

      const [rows] = await bq.query({
        query: selectQuery,
        params: { limit },
        location: bqLocation,
      });

  const chunks = rows as ChunkRow[];
  const selectedChunks = chunks.length;
  console.log(`selected_chunks=${selectedChunks}\n`);

  if (chunks.length === 0) {
    console.log('No chunks to process.');
    return;
  }

  const model = process.env.EMB_MODEL || 'text-embedding-004';
  let embedBatches = 0;
  let totalInserted = 0;
  const pendingRows: EmbeddingRow[] = [];

  // Get table reference for inserts
  const embeddingsTable = await getTable('chunk_embeddings');

  if (dryRun) {
    console.log(`[DRY RUN] Would process ${selectedChunks} chunks`);
    console.log(`  embed_batches=${Math.ceil(selectedChunks / batchSize)} (size=${batchSize})`);
    console.log(`  insert_batch_size=${insertBatch}`);
    console.log(`  Would generate and insert embeddings for ${selectedChunks} chunks`);
    return;
  }

  // Process in batches (embedding API batches)
  for (let i = 0; i < chunks.length; i += batchSize) {
    const batch = chunks.slice(i, i + batchSize);
    embedBatches++;

    const texts = batch.map((chunk) => chunk.chunk_text);

    try {
      const embeddings = await embedBatch(texts, { model, location: vertexLocation });

      for (let j = 0; j < batch.length; j++) {
        const chunk = batch[j];
        const embedding = embeddings[j];

        if (!embedding || !Array.isArray(embedding)) {
          console.error(`Invalid embedding for chunk ${chunk.chunk_id}`);
          continue;
        }

        pendingRows.push({
          chunk_id: chunk.chunk_id,
          model: model,
          dim: embedding.length,
          embedding: embedding,
          created_at: new Date().toISOString(),
        });
      }
    } catch (error: any) {
      console.error(`Error generating embeddings for batch ${embedBatches}:`, error.message);
      // Continue with next batch
    }

    // Flush pending rows when we reach insert batch size
    while (pendingRows.length >= insertBatch) {
      const batchToInsert = pendingRows.splice(0, insertBatch);
      try {
        const inserted = await insertRowsSafe(embeddingsTable, batchToInsert);
        totalInserted += inserted;
        console.log(`flush_insert rows=${inserted} total_inserted=${totalInserted}`);
      } catch (error: any) {
        const failedIds = batchToInsert.slice(0, 5).map((r) => r.chunk_id).join(', ');
        console.error(`Failed to insert batch (first chunk_ids: ${failedIds}...):`, error.message);
        throw error;
      }
    }
  }

  // Flush any remaining rows
  if (pendingRows.length > 0) {
    try {
      const inserted = await insertRowsSafe(embeddingsTable, pendingRows);
      totalInserted += inserted;
      console.log(`flush_insert rows=${inserted} total_inserted=${totalInserted}`);
    } catch (error: any) {
      const failedIds = pendingRows.slice(0, 5).map((r) => r.chunk_id).join(', ');
      console.error(`Failed to insert final batch (first chunk_ids: ${failedIds}...):`, error.message);
      throw error;
    }
  }

  console.log(`\nResults: selected_chunks=${selectedChunks}, embed_batches=${embedBatches} (size=${batchSize}), inserted_embeddings=${totalInserted}`);
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/historical-report.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID;
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const legacyDataset = process.env.LEGACY_DATASET || 'ncc_newsletters';
  const legacyRawTable = process.env.LEGACY_RAW_TABLE || 'messages';
  const legacyChunksTable = process.env.LEGACY_CHUNKS_TABLE || 'chunks';
  const legacyEmbTable = process.env.LEGACY_EMB_TABLE || 'chunk_embeddings';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  interface TableStats {
    rows: number;
    first: string;
    last: string;
  }

  const formatDate = (date: any): string => {
    if (!date) return 'N/A';
    if (typeof date === 'string') {
      try {
        return new Date(date).toISOString();
      } catch {
        return 'N/A';
      }
    }
    if (date.value) {
      try {
        return new Date(date.value).toISOString();
      } catch {
        return 'N/A';
      }
    }
    return 'N/A';
  };

  const queryTableStats = async (
    tablePath: string,
    countCol: string = '*',
    dateCol: string | null = null
  ): Promise<TableStats | 'not found'> => {
    try {
      // Build query
      let query = `SELECT COUNT(${countCol}) AS cnt`;
      if (dateCol) {
        query += `, MIN(${dateCol}) AS first_date, MAX(${dateCol}) AS last_date`;
      }
      query += ` FROM \`${tablePath}\``;

      const [rows] = await bq.query({
        query,
        location,
      });

      const result = rows[0] as any;
      return {
        rows: Number(result.cnt) || 0,
        first: dateCol ? formatDate(result.first_date) : 'N/A',
        last: dateCol ? formatDate(result.last_date) : 'N/A',
      };
    } catch (error: any) {
      // If table doesn't exist, return 'not found'
      const errorMsg = error.message || String(error);
      if (
        errorMsg.includes('Not found') ||
        errorMsg.includes('does not exist') ||
        errorMsg.includes('Table not found') ||
        errorMsg.includes('was not found')
      ) {
        return 'not found';
      }
      throw error;
    }
  };

  const queryLegacyRawTable = async (): Promise<TableStats | 'not found'> => {
    const tablePath = `${projectId}.${legacyDataset}.${legacyRawTable}`;
    
    // Try sent_date first, then internal_date, then created_at
    const dateColumns = ['sent_date', 'internal_date', 'created_at'];
    
    for (const dateCol of dateColumns) {
      try {
        const query = `
          SELECT 
            COUNT(*) AS cnt,
            MIN(${dateCol}) AS first_date,
            MAX(${dateCol}) AS last_date
          FROM \`${tablePath}\`
        `;
        
        const [rows] = await bq.query({
          query,
          location,
        });

        const result = rows[0] as any;
        if (result.cnt !== null && result.cnt !== undefined) {
          return {
            rows: Number(result.cnt) || 0,
            first: formatDate(result.first_date),
            last: formatDate(result.last_date),
          };
        }
      } catch (error: any) {
        // If column doesn't exist, try next one
        if (error.message?.includes('Unrecognized name') || error.message?.includes('Invalid field name')) {
          continue;
        }
        // If table doesn't exist, return 'not found'
        if (error.message?.includes('Not found') || error.message?.includes('does not exist')) {
          return 'not found';
        }
        throw error;
      }
    }

    // If we get here, try without date columns
    try {
      const query = `SELECT COUNT(*) AS cnt FROM \`${tablePath}\``;
      const [rows] = await bq.query({
        query,
        location,
      });
      const result = rows[0] as any;
      return {
        rows: Number(result.cnt) || 0,
        first: 'N/A',
        last: 'N/A',
      };
    } catch (error: any) {
      if (error.message?.includes('Not found') || error.message?.includes('does not exist')) {
        return 'not found';
      }
      throw error;
    }
  };

  // Production tables
  const prodRawStats = await queryTableStats(
    `${projectId}.${datasetId}.raw_emails`,
    '*',
    'COALESCE(sent_date, ingested_at)'
  );

  const prodChunksStats = await queryTableStats(
    `${projectId}.${datasetId}.chunks`,
    '*',
    'created_at'
  );

  const prodEmbStats = await queryTableStats(
    `${projectId}.${datasetId}.chunk_embeddings`,
    '*',
    'created_at'
  );

  // Legacy tables
  const legacyRawStats = await queryLegacyRawTable();

  const legacyChunksStats = await queryTableStats(
    `${projectId}.${legacyDataset}.${legacyChunksTable}`,
    '*',
    'created_at'
  );

  const legacyEmbStats = await queryTableStats(
    `${projectId}.${legacyDataset}.${legacyEmbTable}`,
    '*',
    'created_at'
  );

  // Format stats for output
  const formatStats = (stats: TableStats | 'not found'): string => {
    if (stats === 'not found') {
      return 'rows=not found | first=N/A | last=N/A';
    }
    return `rows=${stats.rows} | first=${stats.first} | last=${stats.last}`;
  };

  // Print report
  console.log('---');
  console.log('HISTORICAL REPORT');
  console.log('Production:');
  console.log(`  raw_emails: ${formatStats(prodRawStats)}`);
  console.log(`  chunks: ${formatStats(prodChunksStats)}`);
  console.log(`  chunk_embeddings: ${formatStats(prodEmbStats)}`);
  console.log(`Legacy (dataset=${legacyDataset}):`);
  console.log(`  messages: ${formatStats(legacyRawStats)}`);
  console.log(`  chunks: ${formatStats(legacyChunksStats)}`);
  console.log(`  chunk_embeddings: ${formatStats(legacyEmbStats)}`);
  console.log('Unification hint: Use views if legacy tables exist and schemas differ.');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/ingest-gmail.ts">
import 'dotenv/config';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';
import { getGmail } from '../src/gmail/client';
import { getTable } from '../src/bq/client';
import { extractPlaintext, getHeader } from '../src/lib/parseMessage';
import { extractEmailAddress } from '../src/lib/gmail';
import { createHash } from 'crypto';
import type { gmail_v1 } from 'googleapis';

interface IngestConfig {
  projectId: string;
  dataset: string;
  location: string;
  query: string;
  processedLabel: string;
  paidLabel: string;
  markRead: boolean;
  inbox: 'me' | 'other';
  dryRun: boolean;
  limit: number;
}

function validateEnv(): void {
  const required = [
    'BQ_PROJECT_ID',
    'BQ_DATASET',
    'BQ_LOCATION',
    'GMAIL_QUERY',
    'GMAIL_PROCESSED_LABEL',
    'GMAIL_PAID_LABEL',
    'GMAIL_MARK_READ',
  ];

  const missing = required.filter(key => !process.env[key]);
  if (missing.length > 0) {
    throw new Error(`Missing required env vars: ${missing.join(', ')}`);
  }
}

async function main(): Promise<void> {
  const argv = await yargs(hideBin(process.argv))
    .option('dry-run', {
      type: 'boolean',
      default: true,
      description: 'Run in dry-run mode (no actual API calls)',
    })
    .option('limit', {
      type: 'number',
      default: 10,
      description: 'Maximum number of emails to process',
    })
    .option('inbox', {
      type: 'string',
      choices: ['me', 'other'] as const,
      default: 'me',
      description: 'Inbox to process',
    })
    .option('reauth', {
      type: 'boolean',
      default: false,
      description: 'Force re-authorization by deleting existing token',
    })
    .parse();

  validateEnv();

  const config: IngestConfig = {
    projectId: process.env.BQ_PROJECT_ID!,
    dataset: process.env.BQ_DATASET!,
    location: process.env.BQ_LOCATION!,
    query: process.env.GMAIL_QUERY!,
    processedLabel: process.env.GMAIL_PROCESSED_LABEL!,
    paidLabel: process.env.GMAIL_PAID_LABEL!,
    markRead: process.env.GMAIL_MARK_READ === 'true',
    inbox: argv.inbox as 'me' | 'other',
    dryRun: argv['dry-run'],
    limit: argv.limit,
  };

  console.log('Ingest Config:');
  console.log(`  project: ${config.projectId}`);
  console.log(`  dataset: ${config.dataset}`);
  console.log(`  location: ${config.location}`);
  console.log(`  query: ${config.query}`);
  console.log(`  processed_label: ${config.processedLabel}`);
  console.log(`  paid_label: ${config.paidLabel}`);
  console.log(`  mark_read: ${config.markRead}`);
  console.log(`  inbox: ${config.inbox}`);
  console.log(`  dry_run: ${config.dryRun}`);
  console.log(`  limit: ${config.limit}\n`);

  const isReadonly = process.env.GMAIL_READONLY !== 'false'; // default true
  if (isReadonly) {
    console.log('Gmail: READONLY mode active — skipping modifications');
  }

  let gmail: gmail_v1.Gmail;
  try {
    gmail = await getGmail(config.inbox, { reauth: (argv as any).reauth ?? false });
  } catch (error: any) {
    const errorMsg = error.message || JSON.stringify(error);
    if (errorMsg.includes('invalid_rapt') || errorMsg.includes('invalid_grant')) {
      console.error('Auth requires re-consent. Re-run with --reauth (and ensure Desktop credentials).');
    } else {
      console.error('Auth failed. Try --reauth');
    }
    process.exit(1);
  }

  // Get labels map
  let labelsRes;
  try {
    labelsRes = await gmail.users.labels.list({ userId: 'me' });
  } catch (error: any) {
    throw new Error(`Gmail labels.list failed: ${error.message || 'unknown error'}`);
  }
  const labelsMap = new Map<string, string>();
  const labelIdMap = new Map<string, string>(); // name -> id for applying labels
  if (labelsRes.data.labels) {
    for (const label of labelsRes.data.labels) {
      if (label.id && label.name) {
        labelsMap.set(label.id, label.name);
        labelIdMap.set(label.name, label.id);
      }
    }
  }

  // List messages
  let listRes;
  try {
    listRes = await gmail.users.messages.list({
      userId: 'me',
      q: config.query,
      maxResults: config.limit,
    });
  } catch (error: any) {
    throw new Error(`Gmail messages.list failed: ${error.message || 'unknown error'}`);
  }

  const messageIds = (listRes.data.messages || []).map(m => m.id!).filter(Boolean);
  console.log(`Gmail: fetched ${messageIds.length} messages`);

  if (messageIds.length === 0) {
    console.log('No messages to process.');
    return;
  }

  // Check existing messages (idempotency)
  let existingIds: Set<string>;
  let rawEmailsTable;
  try {
    rawEmailsTable = await getTable('raw_emails');
    const existingQuery = `
      SELECT gmail_message_id
      FROM \`${config.projectId}.${config.dataset}.raw_emails\`
      WHERE gmail_message_id IN UNNEST(@messageIds)
    `;
    const [existingRows] = await rawEmailsTable.bigQuery.query({
      query: existingQuery,
      params: { messageIds },
      location: config.location,
    });
    existingIds = new Set(existingRows.map((row: any) => row.gmail_message_id));
  } catch (error: any) {
    throw new Error(`BQ idempotency query failed: ${error.message || 'unknown error'}`);
  }
  const newIds = messageIds.filter(id => !existingIds.has(id));

  if (config.dryRun) {
    // Dry run: fetch metadata only for preview
    const samples: Array<{
      date: string;
      from: string;
      subject: string;
      labelNames: string[];
    }> = [];

    for (const msgId of messageIds.slice(0, 10)) {
      let msgRes;
      try {
        msgRes = await gmail.users.messages.get({
          userId: 'me',
          id: msgId,
          format: 'metadata',
          metadataHeaders: ['Subject', 'From', 'To', 'Date'],
        });
      } catch (error: any) {
        const errorMsg = error.message || JSON.stringify(error);
        if (errorMsg.includes('invalid_rapt') || errorMsg.includes('invalid_grant')) {
          throw new Error('Auth requires re-consent. Re-run with --reauth (and ensure Desktop credentials).');
        }
        throw new Error(`Gmail messages.get failed: ${error.message || 'unknown error'}`);
      }

      const headers = msgRes.data.payload?.headers || [];
      const getHeaderValue = (name: string) =>
        headers.find(h => h.name?.toLowerCase() === name.toLowerCase())?.value || '';

      const labelIds = msgRes.data.labelIds || [];
      const labelNames = labelIds
        .map(id => labelsMap.get(id))
        .filter((name): name is string => !!name);

      samples.push({
        date: getHeaderValue('Date'),
        from: getHeaderValue('From'),
        subject: getHeaderValue('Subject'),
        labelNames,
      });
    }

    console.log('Sample (first 10):');
    for (const sample of samples) {
      console.log(`  - ${sample.date} | ${sample.from} | ${sample.subject} | labels: [${sample.labelNames.join(', ')}]`);
    }

    const previewLabelCount = samples.filter(s => s.labelNames.includes(config.paidLabel)).length;
    console.log(`paid_label matches (preview): ${previewLabelCount}`);
    console.log('[DRY RUN] Would insert to BigQuery and apply Gmail labels if --no-dry-run');
    return;
  }

  // Non-dry-run: fetch full messages and insert to BigQuery
  if (newIds.length === 0) {
    console.log('All messages already ingested. Nothing to do.');
    return;
  }

  const rawEmailsRows: any[] = [];
  const emailLabelsRows: any[] = [];
  const newMessageIds: string[] = [];

  // Helper to extract HTML content
  function extractHtmlContent(msg: gmail_v1.Schema$Message): string | null {
    if (!msg || !msg.payload) return null;
    const parts: gmail_v1.Schema$MessagePart[] = [];
    function walk(part?: gmail_v1.Schema$MessagePart) {
      if (!part) return;
      parts.push(part);
      if (part.parts) part.parts.forEach(walk);
    }
    walk(msg.payload);
    for (const part of parts) {
      if ((part.mimeType || '').toLowerCase().startsWith('text/html')) {
        const data = part.body?.data;
        if (data) {
          const normalized = data.replace(/-/g, '+').replace(/_/g, '/');
          const buff = Buffer.from(normalized, 'base64');
          return buff.toString('utf-8');
        }
      }
    }
    return null;
  }

  // Helper to extract name from From header
  function extractFromName(fromHeader: string): string {
    const match = fromHeader.match(/^(.+?)\s*<[^>]+>$/);
    if (match && match[1]) {
      return match[1].replace(/^["']|["']$/g, '').trim();
    }
    return '';
  }

  // Helper to parse Date header string
  function parseHeaderDate(raw?: string): Date | null {
    if (!raw) return null;
    // remove " (UTC)" or similar comment blocks to help the parser
    const cleaned = raw.replace(/\s+\([^)]*\)/g, ' ').trim();
    const d = new Date(cleaned);
    return Number.isNaN(d.getTime()) ? null : d;
  }

  for (const msgId of newIds) {
    try {
      const fullMsg = await gmail.users.messages.get({
        userId: 'me',
        id: msgId,
        format: 'full',
      });

      const msg = fullMsg.data;
      if (!msg || !msg.id) continue;

      const fromHeader = getHeader(msg, 'From');
      const fromEmail = extractEmailAddress(fromHeader);
      const fromName = extractFromName(fromHeader);
      const subject = getHeader(msg, 'Subject') || '';
      const replyTo = getHeader(msg, 'Reply-To') || '';
      const listId = getHeader(msg, 'List-Id') || '';
      const messageIdHeader = getHeader(msg, 'Message-ID') || '';
      const historyId = msg.historyId?.toString() || '';

      // Parse sent_date: prefer Date header, fallback to internalDate
      const dateHeaderString = getHeader(msg, 'Date');
      const headerDate = parseHeaderDate(dateHeaderString);
      const internalMs = Number(msg.internalDate);
      const sentDateObj = headerDate ?? (Number.isFinite(internalMs) ? new Date(internalMs) : null);
      const sentDate = sentDateObj ? sentDateObj.toISOString() : null;

      const bodyText = extractPlaintext(msg);
      const bodyHtml = extractHtmlContent(msg);

      // Compute content_hash
      const contentHash = createHash('sha256')
        .update(bodyText || bodyHtml || '')
        .digest('hex');

      // Check if paid (by label name match)
      const labelIds = msg.labelIds || [];
      const labelNames = labelIds
        .map(id => labelsMap.get(id))
        .filter((name): name is string => !!name);
      const isPaid = labelNames.includes(config.paidLabel);

      rawEmailsRows.push({
        gmail_message_id: msg.id,
        inbox: config.inbox,
        history_id: historyId || null,
        message_id_header: messageIdHeader || null,
        subject: subject || null,
        from_email: fromEmail || null,
        from_name: fromName || null,
        reply_to: replyTo || null,
        list_id: listId || null,
        sent_date: sentDate,
        body_html: bodyHtml,
        body_text: bodyText || null,
        content_hash: contentHash,
        is_paid: isPaid,
        ingested_at: new Date().toISOString(),
      });

      // Build label rows
      for (const labelId of labelIds) {
        const labelName = labelsMap.get(labelId);
        if (labelName) {
          emailLabelsRows.push({
            gmail_message_id: msg.id,
            label_id: labelId,
            label_name: labelName,
          });
        }
      }

      newMessageIds.push(msg.id);
    } catch (error: any) {
      console.error(`Error processing message ${msgId}:`, error.message);
    }
  }

  // Insert raw_emails
  if (rawEmailsRows.length > 0) {
    if (!rawEmailsTable) {
      rawEmailsTable = await getTable('raw_emails');
    }
    await rawEmailsTable.insert(rawEmailsRows);
  }

  // Insert email_labels (idempotent: check existing pairs)
  const emailLabelsTable = await getTable('email_labels');
  let existingLabelPairs: Set<string> = new Set();
  if (emailLabelsRows.length > 0) {
    const uniqueGmailIds = Array.from(new Set(emailLabelsRows.map((r) => r.gmail_message_id)));
    const existingLabelsQuery = `
      SELECT gmail_message_id, label_name
      FROM \`${config.projectId}.${config.dataset}.email_labels\`
      WHERE gmail_message_id IN UNNEST(@gmailIds)
    `;
    try {
      const [existingLabelRows] = await emailLabelsTable.bigQuery.query({
        query: existingLabelsQuery,
        params: { gmailIds: uniqueGmailIds },
        location: config.location,
      });
      for (const row of existingLabelRows as Array<{ gmail_message_id: string; label_name: string }>) {
        existingLabelPairs.add(`${row.gmail_message_id}:${row.label_name}`);
      }
    } catch (error: any) {
      // If query fails, continue with empty set (will insert all, but better than failing)
    }
  }

  // Deduplicate and filter out existing pairs
  const labelMap = new Map<string, { gmail_message_id: string; label_id: string; label_name: string }>();
  for (const row of emailLabelsRows) {
    const key = `${row.gmail_message_id}:${row.label_id}`;
    const pairKey = `${row.gmail_message_id}:${row.label_name}`;
    if (!labelMap.has(key) && !existingLabelPairs.has(pairKey)) {
      labelMap.set(key, row);
    }
  }
  const uniqueLabelRows = Array.from(labelMap.values());
  if (uniqueLabelRows.length > 0) {
    await emailLabelsTable.insert(uniqueLabelRows);
  }

  const nullSentDateCount = rawEmailsRows.filter((r) => !r.sent_date).length;
  console.log(`BQ: existing/skipped=${existingIds.size}, inserted_raw=${rawEmailsRows.length}, inserted_labels=${uniqueLabelRows.length}, null_sent_date=${nullSentDateCount}`);

  // Apply Gmail labels and mark as read (skip if readonly or dry-run)
  let labeledCount = 0;
  let alreadyLabeledCount = 0;
  let markedReadCount = 0;

  if (isReadonly) {
    console.log('Gmail: READONLY mode active — skipping modifications');
  } else if (config.dryRun) {
    // Dry run: no Gmail modifications
  } else if (newMessageIds.length > 0) {
    // Fetch message metadata to check current labels
    const messageMetadata = new Map<string, { labelIds: string[]; labelNames: string[] }>();
    for (const msgId of newMessageIds) {
      try {
        const msgRes = await gmail.users.messages.get({
          userId: 'me',
          id: msgId,
          format: 'metadata',
        });
        const labelIds = msgRes.data.labelIds || [];
        const labelNames = labelIds
          .map((id) => labelsMap.get(id))
          .filter((name): name is string => !!name);
        messageMetadata.set(msgId, { labelIds, labelNames });
      } catch (error: any) {
        console.error(`Error fetching metadata for ${msgId}:`, error.message);
      }
    }

    const processedLabelId = labelIdMap.get(config.processedLabel);
    if (!processedLabelId) {
      // Create label if it doesn't exist
      try {
        const createRes = await gmail.users.labels.create({
          userId: 'me',
          requestBody: { name: config.processedLabel },
        });
        if (createRes.data.id) {
          labelIdMap.set(config.processedLabel, createRes.data.id);
        }
      } catch (error: any) {
        // Label might already exist, try to find it
        const labelsRes = await gmail.users.labels.list({ userId: 'me' });
        if (labelsRes.data.labels) {
          for (const label of labelsRes.data.labels) {
            if (label.id && label.name === config.processedLabel) {
              labelIdMap.set(config.processedLabel, label.id);
              break;
            }
          }
        }
      }
    }

    for (const msgId of newMessageIds) {
      try {
        const metadata = messageMetadata.get(msgId);
        if (!metadata) continue;

        const addLabelIds: string[] = [];
        const removeLabelIds: string[] = [];

        // Check if processed label already exists
        const hasProcessedLabel = metadata.labelNames.includes(config.processedLabel);
        if (!hasProcessedLabel) {
          const labelId = processedLabelId || labelIdMap.get(config.processedLabel);
          if (labelId) {
            addLabelIds.push(labelId);
          }
        } else {
          alreadyLabeledCount++;
        }

        // Check if mark-as-read is needed
        if (config.markRead && metadata.labelIds.includes('UNREAD')) {
          removeLabelIds.push('UNREAD');
        }

        if (addLabelIds.length > 0 || removeLabelIds.length > 0) {
          await gmail.users.messages.modify({
            userId: 'me',
            id: msgId,
            requestBody: {
              addLabelIds: addLabelIds.length > 0 ? addLabelIds : undefined,
              removeLabelIds: removeLabelIds.length > 0 ? removeLabelIds : undefined,
            },
          });
          if (addLabelIds.length > 0) labeledCount++;
          if (removeLabelIds.length > 0) markedReadCount++;
        }
      } catch (error: any) {
        console.error(`Error modifying message ${msgId}:`, error.message);
      }
    }

    console.log(`Gmail: labeled=${labeledCount}, already_labeled=${alreadyLabeledCount}, marked_read=${markedReadCount}`);
  }

  // Post-run reconcile summary
  console.log('');
  console.log('---');
  console.log('RECONCILE SUMMARY:');
  console.log(`  New emails ingested: ${rawEmailsRows.length}`);
  console.log(`  New labels applied: ${uniqueLabelRows.length}`);
  console.log(`  Existing emails skipped: ${existingIds.size}`);
  if (!config.dryRun && !isReadonly && newMessageIds.length > 0) {
    console.log(`  Gmail labels applied: ${labeledCount} (${alreadyLabeledCount} already had label)`);
    console.log(`  Messages marked read: ${markedReadCount}`);
  }
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/migrate-legacy-to-prod.ts">
import 'dotenv/config';

import { getBigQuery } from '../src/bq/client';

type StepResult = { step: string; inserted: number; };

const projectId = process.env.BQ_PROJECT_ID;
const prodDataset = process.env.BQ_DATASET || 'ncc_production';
const legacyDataset = process.env.LEGACY_DATASET || 'ncc_newsletters';
const location = process.env.BQ_LOCATION || 'US';

// Args: --apply (boolean), --limit N (number)
const args = process.argv.slice(2);
const APPLY = args.includes('--apply');
const limitFlagIdx = args.findIndex(a => a === '--limit');
const BATCH_LIMIT = limitFlagIdx >= 0 ? Math.max(1, Number(args[limitFlagIdx + 1])) : 100000;

if (!projectId) throw new Error('BQ_PROJECT_ID env var is required');

const bq = getBigQuery();

const fq = (ds: string, table: string) => `\`${projectId}.${ds}.${table}\``;

async function q<T=any>(sql: string): Promise<T[]> {
  const [rows] = await bq.query({ query: sql, location });
  return rows as T[];
}

function withLimit(sql: string, lim?: number) {
  return (lim && lim > 0) ? `${sql}\nLIMIT ${lim}` : sql;
}

async function migrateMessages(): Promise<StepResult> {
  const countSQL = `
    SELECT COUNT(*) AS to_insert
    FROM ${fq(legacyDataset, 'messages')} m
    WHERE NOT EXISTS (
      SELECT 1 FROM ${fq(prodDataset, 'raw_emails')} re
      WHERE re.gmail_message_id = m.id
    )`;
  const [{ to_insert }] = await q<{to_insert: string | number}>(countSQL);
  let remaining = Number(to_insert) || 0;
  let inserted = 0;

  const insertBase = `
    INSERT INTO ${fq(prodDataset, 'raw_emails')}
    (gmail_message_id,inbox,history_id,message_id_header,subject,from_email,from_name,reply_to,list_id,
     sent_date,body_html,body_text,content_hash,is_paid,ingested_at)
    SELECT
      m.id,
      CAST(NULL AS STRING), CAST(NULL AS STRING), CAST(NULL AS STRING),
      m.subject, m.sender, CAST(NULL AS STRING), CAST(NULL AS STRING), m.list_id,
      CAST(m.sent_date AS TIMESTAMP),
      m.body_html, m.body_text, CAST(NULL AS STRING),
      COALESCE(m.is_paid, FALSE),
      CAST(COALESCE(m.received_date, m.sent_date, CURRENT_TIMESTAMP()) AS TIMESTAMP)
    FROM ${fq(legacyDataset, 'messages')} m
    WHERE NOT EXISTS (
      SELECT 1 FROM ${fq(prodDataset, 'raw_emails')} re
      WHERE re.gmail_message_id = m.id
    )`;

  if (!APPLY) {
    console.log(`[DRY] messages → raw_emails would insert: ${remaining}`);
    return { step: 'messages', inserted: 0 };
  }

  while (remaining > 0) {
    const batch = Math.min(remaining, BATCH_LIMIT);
    await q(withLimit(insertBase, batch));
    inserted += batch;
    const [{ to_insert: after }] = await q<{to_insert: string | number}>(countSQL);
    remaining = Number(after) || 0;
    console.log(`[APPLY] messages batch inserted=${batch}, remaining=${remaining}`);
    if (batch === 0) break;
  }
  return { step: 'messages', inserted };
}

async function migrateChunks(): Promise<StepResult> {
  const countSQL = `
    SELECT COUNT(*) AS to_insert
    FROM ${fq(legacyDataset, 'chunks')} c
    WHERE NOT EXISTS (
      SELECT 1 FROM ${fq(prodDataset, 'chunks')} pc
      WHERE pc.chunk_id = c.chunk_id
    )`;
  const [{ to_insert }] = await q<{to_insert: string | number}>(countSQL);
  let remaining = Number(to_insert) || 0;
  let inserted = 0;

  const insertBase = `
    INSERT INTO ${fq(prodDataset, 'chunks')}
    (chunk_id,gmail_message_id,publisher_id,source_part,char_start,char_end,chunk_index,chunk_text,created_at)
    SELECT
      c.chunk_id, c.newsletter_id,
      CAST(NULL AS STRING), CAST(NULL AS STRING),
      CAST(NULL AS INT64), CAST(NULL AS INT64),
      c.chunk_index, c.chunk_text,
      COALESCE(c.created_at, CURRENT_TIMESTAMP())
    FROM ${fq(legacyDataset, 'chunks')} c
    WHERE NOT EXISTS (
      SELECT 1 FROM ${fq(prodDataset, 'chunks')} pc
      WHERE pc.chunk_id = c.chunk_id
    )`;

  if (!APPLY) {
    console.log(`[DRY] chunks → chunks would insert: ${remaining}`);
    return { step: 'chunks', inserted: 0 };
  }

  while (remaining > 0) {
    const batch = Math.min(remaining, BATCH_LIMIT);
    await q(withLimit(insertBase, batch));
    inserted += batch;
    const [{ to_insert: after }] = await q<{to_insert: string | number}>(countSQL);
    remaining = Number(after) || 0;
    console.log(`[APPLY] chunks batch inserted=${batch}, remaining=${remaining}`);
    if (batch === 0) break;
  }
  return { step: 'chunks', inserted };
}

async function migrateEmbeddings(): Promise<StepResult> {
  // Only legacy rows that actually have embeddings
  const countSQL = `
    SELECT COUNT(*) AS to_insert
    FROM ${fq(legacyDataset, 'chunks')} c
    WHERE c.chunk_embedding IS NOT NULL
      AND NOT EXISTS (
        SELECT 1 FROM ${fq(prodDataset, 'chunk_embeddings')} e
        WHERE e.chunk_id = c.chunk_id
      )`;
  const [{ to_insert }] = await q<{to_insert: string | number}>(countSQL);
  let remaining = Number(to_insert) || 0;
  let inserted = 0;

  const insertBase = `
    INSERT INTO ${fq(prodDataset, 'chunk_embeddings')}
    (chunk_id,model,dim,embedding,created_at)
    SELECT
      c.chunk_id,
      "legacy",
      ARRAY_LENGTH(c.chunk_embedding),
      c.chunk_embedding,
      COALESCE(c.updated_at, c.created_at, CURRENT_TIMESTAMP())
    FROM ${fq(legacyDataset, 'chunks')} c
    WHERE c.chunk_embedding IS NOT NULL
      AND NOT EXISTS (
        SELECT 1 FROM ${fq(prodDataset, 'chunk_embeddings')} e
        WHERE e.chunk_id = c.chunk_id
      )`;

  if (!APPLY) {
    console.log(`[DRY] legacy embeddings → prod.chunk_embeddings would insert: ${remaining}`);
    return { step: 'embeddings', inserted: 0 };
  }

  while (remaining > 0) {
    const batch = Math.min(remaining, BATCH_LIMIT);
    await q(withLimit(insertBase, batch));
    inserted += batch;
    const [{ to_insert: after }] = await q<{to_insert: string | number}>(countSQL);
    remaining = Number(after) || 0;
    console.log(`[APPLY] embeddings batch inserted=${batch}, remaining=${remaining}`);
    if (batch === 0) break;
  }
  return { step: 'embeddings', inserted };
}

async function main() {
  console.log(`---\nLEGACY → PROD MIGRATION (${APPLY ? 'APPLY' : 'DRY'})`);
  console.log(`Project=${projectId} Location=${location} Legacy=${legacyDataset} Prod=${prodDataset} Limit=${BATCH_LIMIT}\n`);

  const r1 = await migrateMessages();
  const r2 = await migrateChunks();
  const r3 = await migrateEmbeddings();

  console.log('\nSUMMARY:');
  if (!APPLY) {
    console.log('DRY RUN only (no rows inserted).');
  } else {
    console.log(`Inserted: messages=${r1.inserted}, chunks=${r2.inserted}, embeddings=${r3.inserted}`);
  }
  console.log('---');
}

main().catch(err => {
  console.error('Migration failed.\n', err?.message || err);
  process.exit(1);
});
</file>

<file path="scripts/report-legacy-schema.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = 'ncc_newsletters';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  console.log('---');
  console.log('LEGACY SCHEMA');
  console.log('');

  // Query columns for messages table
  const messagesColumnsQuery = `
    SELECT column_name, data_type
    FROM \`${projectId}.${datasetId}.INFORMATION_SCHEMA.COLUMNS\`
    WHERE table_name = 'messages'
    ORDER BY ordinal_position
  `;

  let messagesColumns: Array<{ column_name: string; data_type: string }> = [];
  try {
    const [rows] = await bq.query({
      query: messagesColumnsQuery,
      location,
    });
    messagesColumns = rows as Array<{ column_name: string; data_type: string }>;
  } catch (error: any) {
    console.error(`Error querying messages columns: ${error.message}`);
    return;
  }

  console.log('messages columns:');
  for (const col of messagesColumns) {
    console.log(`  - ${col.column_name} ${col.data_type}`);
  }
  console.log('');

  // Query sample row from messages table
  const messagesIdFields = ['gmail_message_id', 'message_id', 'id', 'gmail_id', 'subject', 'sent_date'];
  const messagesExistingFields = messagesColumns
    .map(c => c.column_name)
    .filter(name => messagesIdFields.includes(name));

  if (messagesExistingFields.length > 0) {
    const messagesSampleQuery = `
      SELECT ${messagesExistingFields.map(f => `\`${f}\``).join(', ')}
      FROM \`${projectId}.${datasetId}.messages\`
      LIMIT 1
    `;

    try {
      const [rows] = await bq.query({
        query: messagesSampleQuery,
        location,
      });
      if (rows.length > 0) {
        const sample = rows[0] as Record<string, any>;
        const samplePairs = messagesExistingFields
          .map(field => `${field}=${sample[field] ?? 'NULL'}`)
          .join(', ');
        console.log('messages sample keys:');
        console.log(`  ${samplePairs}`);
      } else {
        console.log('messages sample keys:');
        console.log('  (no rows found)');
      }
    } catch (error: any) {
      console.error(`Error querying messages sample: ${error.message}`);
    }
  } else {
    console.log('messages sample keys:');
    console.log('  (no ID fields found)');
  }
  console.log('');

  // Query columns for chunks table
  const chunksColumnsQuery = `
    SELECT column_name, data_type
    FROM \`${projectId}.${datasetId}.INFORMATION_SCHEMA.COLUMNS\`
    WHERE table_name = 'chunks'
    ORDER BY ordinal_position
  `;

  let chunksColumns: Array<{ column_name: string; data_type: string }> = [];
  try {
    const [rows] = await bq.query({
      query: chunksColumnsQuery,
      location,
    });
    chunksColumns = rows as Array<{ column_name: string; data_type: string }>;
  } catch (error: any) {
    console.error(`Error querying chunks columns: ${error.message}`);
    return;
  }

  console.log('chunks columns:');
  for (const col of chunksColumns) {
    console.log(`  - ${col.column_name} ${col.data_type}`);
  }
  console.log('');

  // Query sample row from chunks table
  const chunksIdFields = ['chunk_id', 'newsletter_id', 'gmail_message_id', 'chunk_index'];
  const chunksExistingFields = chunksColumns
    .map(c => c.column_name)
    .filter(name => chunksIdFields.includes(name));

  if (chunksExistingFields.length > 0) {
    const chunksSampleQuery = `
      SELECT ${chunksExistingFields.map(f => `\`${f}\``).join(', ')}
      FROM \`${projectId}.${datasetId}.chunks\`
      LIMIT 1
    `;

    try {
      const [rows] = await bq.query({
        query: chunksSampleQuery,
        location,
      });
      if (rows.length > 0) {
        const sample = rows[0] as Record<string, any>;
        const samplePairs = chunksExistingFields
          .map(field => `${field}=${sample[field] ?? 'NULL'}`)
          .join(', ');
        console.log('chunks sample keys:');
        console.log(`  ${samplePairs}`);
      } else {
        console.log('chunks sample keys:');
        console.log('  (no rows found)');
      }
    } catch (error: any) {
      console.error(`Error querying chunks sample: ${error.message}`);
    }
  } else {
    console.log('chunks sample keys:');
    console.log('  (no ID fields found)');
  }
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/report-reconcile.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  console.log('---');
  console.log('RECONCILIATION REPORT (PROD)');
  console.log('');

  // Define t0 = 24 hours ago
  const t0 = 'TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)';

  // Last 24h queries
  const raw24hQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE ingested_at >= ${t0}
  `;

  const emailsWithChunks24hQuery = `
    SELECT COUNT(DISTINCT gmail_message_id) AS count
    FROM \`${projectId}.${datasetId}.chunks\`
    WHERE gmail_message_id IN (
      SELECT gmail_message_id
      FROM \`${projectId}.${datasetId}.raw_emails\`
      WHERE ingested_at >= ${t0}
    )
  `;

  const chunks24hQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.chunks\`
    WHERE gmail_message_id IN (
      SELECT gmail_message_id
      FROM \`${projectId}.${datasetId}.raw_emails\`
      WHERE ingested_at >= ${t0}
    )
  `;

  const embeddedChunks24hQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.chunk_embeddings\`
    WHERE chunk_id IN (
      SELECT chunk_id
      FROM \`${projectId}.${datasetId}.chunks\`
      WHERE gmail_message_id IN (
        SELECT gmail_message_id
        FROM \`${projectId}.${datasetId}.raw_emails\`
        WHERE ingested_at >= ${t0}
      )
    )
  `;

  // All-time queries
  const rawAllQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.raw_emails\`
  `;

  const emailsWithChunksAllQuery = `
    SELECT COUNT(DISTINCT gmail_message_id) AS count
    FROM \`${projectId}.${datasetId}.chunks\`
  `;

  const chunksAllQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.chunks\`
  `;

  const embeddedChunksAllQuery = `
    SELECT COUNT(*) AS count
    FROM \`${projectId}.${datasetId}.chunk_embeddings\`
  `;

  // Execute queries
  let raw24h = 0;
  let emailsWithChunks24h = 0;
  let chunks24h = 0;
  let embeddedChunks24h = 0;
  let rawAll = 0;
  let emailsWithChunksAll = 0;
  let chunksAll = 0;
  let embeddedChunksAll = 0;

  try {
    const [raw24hRows] = await bq.query({ query: raw24hQuery, location });
    raw24h = (raw24hRows[0] as { count: number }).count;

    const [emailsWithChunks24hRows] = await bq.query({ query: emailsWithChunks24hQuery, location });
    emailsWithChunks24h = (emailsWithChunks24hRows[0] as { count: number }).count;

    const [chunks24hRows] = await bq.query({ query: chunks24hQuery, location });
    chunks24h = (chunks24hRows[0] as { count: number }).count;

    const [embeddedChunks24hRows] = await bq.query({ query: embeddedChunks24hQuery, location });
    embeddedChunks24h = (embeddedChunks24hRows[0] as { count: number }).count;

    const [rawAllRows] = await bq.query({ query: rawAllQuery, location });
    rawAll = (rawAllRows[0] as { count: number }).count;

    const [emailsWithChunksAllRows] = await bq.query({ query: emailsWithChunksAllQuery, location });
    emailsWithChunksAll = (emailsWithChunksAllRows[0] as { count: number }).count;

    const [chunksAllRows] = await bq.query({ query: chunksAllQuery, location });
    chunksAll = (chunksAllRows[0] as { count: number }).count;

    const [embeddedChunksAllRows] = await bq.query({ query: embeddedChunksAllQuery, location });
    embeddedChunksAll = (embeddedChunksAllRows[0] as { count: number }).count;
  } catch (error: any) {
    console.error(`Error executing queries: ${error.message}`);
    process.exit(1);
  }

  // Calculate percentages (with divide-by-zero guards)
  const pctEmailsWithChunks24h = raw24h > 0 ? Math.round((emailsWithChunks24h / raw24h) * 1000) / 10 : 0;
  const pctChunksEmbedded24h = chunks24h > 0 ? Math.round((embeddedChunks24h / chunks24h) * 1000) / 10 : 0;
  const pctEmailsWithChunksAll = rawAll > 0 ? Math.round((emailsWithChunksAll / rawAll) * 1000) / 10 : 0;
  const pctChunksEmbeddedAll = chunksAll > 0 ? Math.round((embeddedChunksAll / chunksAll) * 1000) / 10 : 0;

  // Print report
  console.log('Window: last_24h');
  console.log(`raw_emails: ${raw24h}`);
  console.log(`emails_chunked: ${emailsWithChunks24h} (${pctEmailsWithChunks24h}%)`);
  console.log(`chunks: ${chunks24h}`);
  console.log(`chunks_embedded: ${embeddedChunks24h} (${pctChunksEmbedded24h}%)`);
  console.log('');
  console.log('Window: all_time');
  console.log(`raw_emails: ${rawAll}`);
  console.log(`emails_chunked: ${emailsWithChunksAll} (${pctEmailsWithChunksAll}%)`);
  console.log(`chunks: ${chunksAll}`);
  console.log(`chunks_embedded: ${embeddedChunksAll} (${pctChunksEmbeddedAll}%)`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/report-unified.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  console.log('---');
  console.log('UNIFIED VIEWS REPORT');
  console.log('');

  // Query row counts for each view
  const views = [
    'newsletter-control-center.ncc_production.v_all_raw_emails',
    'newsletter-control-center.ncc_production.v_all_chunks',
    'newsletter-control-center.ncc_production.v_all_chunk_embeddings',
  ];

  for (const viewName of views) {
    const viewNameShort = viewName.split('.').pop() || viewName;
    const countQuery = `SELECT COUNT(*) AS row_count FROM \`${viewName}\``;

    try {
      const [rows] = await bq.query({
        query: countQuery,
        location,
      });
      const rowCount = (rows[0] as { row_count: number }).row_count;
      console.log(`${viewNameShort}: rows=${rowCount}`);
    } catch (error: any) {
      console.error(`Error querying ${viewNameShort}: ${error.message}`);
      console.log(`${viewNameShort}: rows=ERROR`);
    }
  }

  console.log('');
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/run-pipeline.ts">
#!/usr/bin/env ts-node

import { Command } from 'commander';
import * as ingestion from '../src/core/ingestion';
import * as processor from '../src/core/processor';
import * as publisher from '../src/core/publisher';

const program = new Command();

program
  .name('run-pipeline')
  .description('Newsletter Control Center Pipeline CLI')
  .version('1.0.0');

program
  .command('ingest')
  .description('Ingest new newsletters from Gmail')
  .option('--inbox <inbox>', 'Inbox to ingest from: all or email address', 'all')
  .action(async (options) => {
    try {
      const inbox = options.inbox || 'all';
      await ingestion.ingestNewNewsletters(inbox === 'all' ? 'all' : inbox);
      console.log('✅ Ingestion complete');
    } catch (error: any) {
      if (error.message.includes('not implemented yet')) {
        console.log('⚠️  Not implemented yet');
      } else {
        console.error('❌ Ingestion failed:', error.message);
        process.exit(1);
      }
    }
  });

program
  .command('process')
  .description('Process unchunked messages (chunk and embed)')
  .action(async () => {
    try {
      await processor.processUnchunkedMessages();
      console.log('✅ Processing complete');
    } catch (error: any) {
      if (error.message.includes('not implemented yet')) {
        console.log('⚠️  Not implemented yet');
      } else {
        console.error('❌ Processing failed:', error.message);
        process.exit(1);
      }
    }
  });

program
  .command('full')
  .description('Run full pipeline: ingest then process')
  .action(async () => {
    try {
      console.log('📥 Starting ingestion...');
      await ingestion.ingestNewNewsletters('all');
      console.log('✅ Ingestion complete');
      
      console.log('⚙️  Starting processing...');
      await processor.processUnchunkedMessages();
      console.log('✅ Processing complete');
      
      console.log('✅ Full pipeline complete');
    } catch (error: any) {
      if (error.message.includes('not implemented yet')) {
        console.log('⚠️  Not implemented yet');
      } else {
        console.error('❌ Pipeline failed:', error.message);
        process.exit(1);
      }
    }
  });

program
  .command('fix-publishers')
  .description('Fix duplicate publishers via alias merge')
  .action(async () => {
    try {
      // Placeholder for publisher fix logic
      console.log('⚠️  Publisher fix not implemented yet');
    } catch (error: any) {
      console.error('❌ Publisher fix failed:', error.message);
      process.exit(1);
    }
  });

program.parse();
</file>

<file path="scripts/setup-bigquery.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

async function main() {
  const projectId = process.env.BQ_PROJECT_ID!;
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) throw new Error('BQ_PROJECT_ID is required');

  const bq = new BigQuery({ projectId });
  await bq.dataset(datasetId, { location }).get({ autoCreate: true });

  const ddls: string[] = [
    // control tables
    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.ingest_state\` (
      inbox STRING,
      last_history_id STRING,
      last_success_at TIMESTAMP
    );`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.processing_status\` (
      gmail_message_id STRING,
      stage STRING,
      error STRING,
      updated_at TIMESTAMP
    );`,

    // core tables
    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.raw_emails\` (
      gmail_message_id STRING,
      inbox STRING,
      history_id STRING,
      message_id_header STRING,
      subject STRING,
      from_email STRING,
      from_name STRING,
      reply_to STRING,
      list_id STRING,
      sent_date TIMESTAMP,
      body_html STRING,
      body_text STRING,
      content_hash STRING,
      is_paid BOOL,
      ingested_at TIMESTAMP
    )
    PARTITION BY DATE(ingested_at)
    CLUSTER BY inbox, gmail_message_id;`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.email_labels\` (
      gmail_message_id STRING,
      label_id STRING,
      label_name STRING
    );`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.publishers\` (
      publisher_id STRING,
      service STRING,
      site_id STRING,
      domain_root STRING,
      display_name STRING,
      first_seen_at TIMESTAMP,
      last_seen_at TIMESTAMP
    );`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.publisher_aliases\` (
      alias_service STRING,
      alias_site_id STRING,
      publisher_id STRING
    );`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.chunks\` (
      chunk_id STRING,
      gmail_message_id STRING,
      publisher_id STRING,
      source_part STRING,
      char_start INT64,
      char_end INT64,
      chunk_index INT64,
      chunk_text STRING,
      created_at TIMESTAMP
    )
    PARTITION BY DATE(created_at)
    CLUSTER BY publisher_id, gmail_message_id;`,

    `CREATE TABLE IF NOT EXISTS \`${projectId}.${datasetId}.chunk_embeddings\` (
      chunk_id STRING,
      model STRING,
      dim INT64,
      embedding ARRAY<FLOAT64>,
      created_at TIMESTAMP
    )
    CLUSTER BY chunk_id;`,
  ];

  for (const sql of ddls) {
    console.log('Ensuring:', sql.split('\n')[0]);
    await bq.query({ query: sql, location });
  }

  console.log('Setup complete.', { dataset: `${projectId}.${datasetId}`, tablesEnsured: ddls.length });
}

main().catch(err => {
  console.error('setup-bigquery failed:', err);
  process.exit(1);
});
</file>

<file path="scripts/smoke-check.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID;
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  console.log('Smoke Check: Newsletter Control Center\n');

  // Check raw_emails
  const rawEmailsQuery = `
    SELECT 
      COUNT(*) as total,
      MAX(ingested_at) as latest_ingest,
      COUNT(DISTINCT gmail_message_id) as unique_messages
    FROM \`${projectId}.${datasetId}.raw_emails\`
  `;

  const [rawEmailsRows] = await bq.query({
    query: rawEmailsQuery,
    location,
  });

  const rawEmails = rawEmailsRows[0] as {
    total: number;
    latest_ingest: string | null;
    unique_messages: number;
  };

  console.log('raw_emails:');
  console.log(`  Total rows: ${rawEmails.total.toLocaleString()}`);
  console.log(`  Unique messages: ${rawEmails.unique_messages.toLocaleString()}`);
  console.log(`  Latest ingest: ${rawEmails.latest_ingest || 'N/A'}\n`);

  // Check chunks
  const chunksQuery = `
    SELECT 
      COUNT(*) as total,
      COUNT(DISTINCT gmail_message_id) as unique_messages,
      MAX(created_at) as latest_chunk
    FROM \`${projectId}.${datasetId}.chunks\`
  `;

  const [chunksRows] = await bq.query({
    query: chunksQuery,
    location,
  });

  const chunks = chunksRows[0] as {
    total: number;
    unique_messages: number;
    latest_chunk: string | null;
  };

  console.log('chunks:');
  console.log(`  Total chunks: ${chunks.total.toLocaleString()}`);
  console.log(`  Unique messages: ${chunks.unique_messages.toLocaleString()}`);
  console.log(`  Latest chunk: ${chunks.latest_chunk || 'N/A'}\n`);

  // Check recent ingestion (last 24h)
  const recentQuery = `
    SELECT COUNT(*) as recent_count
    FROM \`${projectId}.${datasetId}.raw_emails\`
    WHERE ingested_at > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
  `;

  const [recentRows] = await bq.query({
    query: recentQuery,
    location,
  });

  const recent = recentRows[0] as { recent_count: number };
  console.log(`Recent (24h): ${recent.recent_count.toLocaleString()} emails ingested`);

  if (recent.recent_count === 0 && rawEmails.total > 0) {
    console.log('\n⚠️  Warning: No recent ingestion in last 24 hours');
  }
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/smoke.ts">
import 'dotenv/config';
import { getBigQuery } from '../src/bq/client';

async function main(): Promise<void> {
  const projectId = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const bq = getBigQuery();

  console.log('---');
  console.log('NCC SMOKE TEST');
  console.log(`Project: ${projectId}`);
  console.log(`Dataset: ${datasetId}`);
  console.log(`Location: ${location}`);
  console.log('');

  // Query a: last 24h and all-time counts
  const countQuery = `
    SELECT 
      COUNTIF(ingested_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)) AS last_24h,
      COUNT(*) AS all_time
    FROM \`${projectId}.${datasetId}.raw_emails\`
  `;

  console.log('Query 1 (counts):');
  console.log(countQuery);
  console.log('');

  let counts: { last_24h: number; all_time: number };
  try {
    const [countRows] = await bq.query({
      query: countQuery,
      location,
    });
    counts = countRows[0] as { last_24h: number; all_time: number };
  } catch (error: any) {
    console.error('❌ Query 1 failed:');
    console.error(`   Location: ${location}`);
    console.error(`   Project: ${projectId}`);
    console.error(`   Dataset: ${datasetId}`);
    console.error(`   Error: ${error.message || String(error)}`);
    throw error;
  }

  // Query b: latest 5 emails
  const latestQuery = `
    SELECT 
      gmail_message_id,
      subject,
      is_paid,
      sent_date
    FROM \`${projectId}.${datasetId}.raw_emails\`
    ORDER BY sent_date DESC NULLS LAST
    LIMIT 5
  `;

  console.log('Query 2 (latest):');
  console.log(latestQuery);
  console.log('');

  let latest: Array<{
    gmail_message_id: string;
    subject: string | null;
    is_paid: boolean | null;
    sent_date: string | null;
  }>;
  try {
    const [latestRows] = await bq.query({
      query: latestQuery,
      location,
    });
    latest = latestRows as Array<{
      gmail_message_id: string;
      subject: string | null;
      is_paid: boolean | null;
      sent_date: string | null;
    }>;
  } catch (error: any) {
    console.error('❌ Query 2 failed:');
    console.error(`   Location: ${location}`);
    console.error(`   Project: ${projectId}`);
    console.error(`   Dataset: ${datasetId}`);
    console.error(`   Error: ${error.message || String(error)}`);
    throw error;
  }

  // Query c: chunk coverage
  const coverageQuery = `
    SELECT 
      COUNT(DISTINCT re.gmail_message_id) AS raw_ids,
      COUNT(DISTINCT ch.gmail_message_id) AS chunked_ids
    FROM \`${projectId}.${datasetId}.raw_emails\` re
    LEFT JOIN \`${projectId}.${datasetId}.chunks\` ch
           ON re.gmail_message_id = ch.gmail_message_id
  `;

  console.log('Query 3 (coverage):');
  console.log(coverageQuery);
  console.log('');

  let coverage: { raw_ids: number; chunked_ids: number };
  try {
    const [coverageRows] = await bq.query({
      query: coverageQuery,
      location,
    });
    coverage = coverageRows[0] as { raw_ids: number; chunked_ids: number };
  } catch (error: any) {
    console.error('❌ Query 3 failed:');
    console.error(`   Location: ${location}`);
    console.error(`   Project: ${projectId}`);
    console.error(`   Dataset: ${datasetId}`);
    console.error(`   Error: ${error.message || String(error)}`);
    throw error;
  }

  // Query d: chunk and embedding counts
  const embeddingQuery = `
    SELECT
      COUNT(*) AS total_chunks,
      COUNT(ce.chunk_id) AS embedded_chunks
    FROM \`${projectId}.${datasetId}.chunks\` ch
    LEFT JOIN \`${projectId}.${datasetId}.chunk_embeddings\` ce
           ON ce.chunk_id = ch.chunk_id
  `;

  console.log('Query 4 (embeddings):');
  console.log(embeddingQuery);
  console.log('');

  let embeddingCoverage: { total_chunks: number; embedded_chunks: number };
  try {
    const [embeddingRows] = await bq.query({
      query: embeddingQuery,
      location,
    });
    embeddingCoverage = embeddingRows[0] as { total_chunks: number; embedded_chunks: number };
  } catch (error: any) {
    console.error('❌ Query 4 failed:');
    console.error(`   Location: ${location}`);
    console.error(`   Project: ${projectId}`);
    console.error(`   Dataset: ${datasetId}`);
    console.error(`   Error: ${error.message || String(error)}`);
    throw error;
  }

  // Print output
  console.log('Results:');
  console.log(`Raw emails: last_24h=${counts.last_24h} | all_time=${counts.all_time}`);
  console.log('Latest 5:');
  for (const row of latest) {
    let sentDate = 'N/A';
    if (row.sent_date) {
      try {
        const date = new Date(row.sent_date);
        if (!isNaN(date.getTime())) {
          sentDate = date.toISOString();
        }
      } catch {
        // Keep as N/A
      }
    }
    const isPaid = row.is_paid ? 'paid' : 'free';
    const subject = row.subject || '(no subject)';
    console.log(`  - ${sentDate} | ${isPaid} | ${subject}`);
  }
  const chunkPct = coverage.raw_ids > 0 
    ? Math.round((coverage.chunked_ids / coverage.raw_ids) * 100)
    : 0;
  console.log(`Chunk coverage: raw_ids=${coverage.raw_ids} | chunked_ids=${coverage.chunked_ids} | ${chunkPct}%`);
  const embeddingPct = embeddingCoverage.total_chunks > 0
    ? Math.round((embeddingCoverage.embedded_chunks / embeddingCoverage.total_chunks) * 100)
    : 0;
  console.log(`Embedding coverage: total_chunks=${embeddingCoverage.total_chunks} | embedded_chunks=${embeddingCoverage.embedded_chunks} | ${embeddingPct}%`);
  console.log('');
  
  // PASS summary
  const rawCount = counts.all_time;
  const chunkedEmails = coverage.chunked_ids;
  const chunks = embeddingCoverage.total_chunks;
  const embedded = embeddingCoverage.embedded_chunks;
  
  console.log(`SMOKE PASS: raw=${rawCount} | chunked_emails=${chunkedEmails}/${rawCount} | chunks=${chunks} | embedded=${embedded}`);
  console.log('---');
}

if (require.main === module) {
  main().catch((err) => {
    console.error('Error:', err.message || err);
    process.exit(1);
  });
}

export default main;
</file>

<file path="scripts/verify-gcp-auth.ts">
import 'dotenv/config';

import fs from 'fs';

import { BigQuery } from '@google-cloud/bigquery';

async function main() {
  const projectId = process.env.BQ_PROJECT_ID || '';
  const location = process.env.BQ_LOCATION || 'US';
  const keyPath = process.env.GOOGLE_APPLICATION_CREDENTIALS || '';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID is required in .env');
  }
  if (keyPath && !fs.existsSync(keyPath)) {
    throw new Error(`GOOGLE_APPLICATION_CREDENTIALS points to a missing file: ${keyPath}`);
  }

  const bq = new BigQuery({ projectId });
  const [rows] = await bq.query({ query: 'SELECT 1 AS ok', location });
  const ok = rows && rows[0] && rows[0].ok === 1;

  console.log('GCP auth OK', { projectId, location, ok });
}

main().catch((err) => {
  console.error('GCP auth failed:', err?.message || err);
  process.exit(1);
});
</file>

<file path="src/api/admin.ts">
/**
 * Admin API - pipeline control endpoints
 * POST /api/admin/ingest
 * POST /api/admin/process
 * GET /api/admin/status
 */

export default async function handler(req: any, res: any) {
  const { method, url } = req;
  
  // For now, just return 200 with {ok:true}; we'll wire later
  res.status(200).json({ ok: true, message: 'Admin API not implemented yet' });
}
</file>

<file path="src/api/intelligence.ts">
/**
 * Intelligence API - RAG-powered query endpoint
 * Placeholder for single RAG endpoint
 */

export default async function handler(req: any, res: any) {
  // Placeholder implementation
  res.status(200).json({ message: 'Intelligence API not implemented yet' });
}
</file>

<file path="src/api/jobs-runner.ts">
import express from 'express';
import { GoogleAuth } from 'google-auth-library';
import { checkHealth } from '../ops/health';

const app = express();
app.use(express.json());

const PROJECT_ID = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';

// Initialize auth client
const auth = new GoogleAuth({
  scopes: ['https://www.googleapis.com/auth/cloud-platform'],
});

// Route guard: Allow /health-check publicly, require auth for all other routes
app.use((req, res, next) => {
  // Allow unauthenticated GET requests to /health-check
  if (req.path === '/health-check' && req.method === 'GET') {
    return next();
  }
  
  // All other routes require Bearer token
  const authHeader = req.headers.authorization;
  if (!authHeader || !authHeader.startsWith('Bearer ')) {
    return res.status(401).json({
      error: 'Unauthorized',
      message: 'This endpoint requires authentication. Provide a Bearer token in the Authorization header.',
    });
  }
  
  // TODO: Verify JWT token with Google's audience = service URL
  // For now, just check that the header exists (Cloud Run validates OIDC tokens at platform level)
  next();
});

interface RunJobRequest {
  job: string;
}

app.post('/run', async (req, res) => {
  try {
    const { job }: RunJobRequest = req.body;

    if (!job) {
      return res.status(400).json({ error: 'Missing job field in request body' });
    }

    // Validate job name
    const validJobs = ['ncc-chunks', 'ncc-embeddings', 'ncc-smoke'];
    if (!validJobs.includes(job)) {
      return res.status(400).json({ error: `Invalid job: ${job}. Must be one of: ${validJobs.join(', ')}` });
    }

    // Get access token
    const client = await auth.getClient();
    const tokenResponse = await client.getAccessToken();
    const accessToken = tokenResponse.token;

    if (!accessToken) {
      throw new Error('Failed to get access token');
    }

    // Call Cloud Run Jobs API
    const jobName = `projects/${PROJECT_ID}/locations/${REGION}/jobs/${job}`;
    const apiUrl = `https://${REGION}-run.googleapis.com/v2/${jobName}:run`;

    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${accessToken}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({}),
    });

    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Cloud Run Jobs API error: ${response.status} ${errorText}`);
    }

    const result = await response.json();

    res.json({
      success: true,
      job,
      execution: result.name || 'Unknown',
    });
  } catch (error: any) {
    console.error('Error running job:', error);
    res.status(500).json({
      success: false,
      error: error.message || 'Internal server error',
    });
  }
});

// Health check
app.get('/health', (req, res) => {
  res.json({ status: 'ok' });
});

// Production health check handler
async function handleHealthCheck(req: express.Request, res: express.Response): Promise<void> {
  try {
    const health = await checkHealth();
    
    if (health.ok) {
      res.status(200).json({
        ok: true,
        jobs_ok: true,
        coverage_ok: health.details.reconcile.chunkCoverage === 100 && health.details.reconcile.embeddingCoverage === 100,
        timestamp: new Date().toISOString(),
        details: {
          jobs: health.details.jobs.map(j => ({
            job: j.job,
            lastSuccessTime: j.lastSuccessTime?.toISOString() || null,
            status: j.status,
          })),
          reconcile: health.details.reconcile,
        },
      });
    } else {
      res.status(500).json({
        ok: false,
        jobs_ok: false,
        coverage_ok: false,
        timestamp: new Date().toISOString(),
        reason: health.reason,
        details: {
          jobs: health.details.jobs.map(j => ({
            job: j.job,
            lastSuccessTime: j.lastSuccessTime?.toISOString() || null,
            status: j.status,
          })),
          reconcile: health.details.reconcile,
        },
      });
    }
  } catch (error: any) {
    console.error('Health check error:', error);
    res.status(500).json({
      ok: false,
      jobs_ok: false,
      coverage_ok: false,
      timestamp: new Date().toISOString(),
      reason: `Health check failed: ${error.message || 'unknown error'}`,
      details: {},
    });
  }
}

// Production health check (using /healthz per requirements, with /health-check as fallback)
app.get('/healthz', handleHealthCheck);
app.get('/health-check', handleHealthCheck);

const PORT = process.env.PORT || 8080;
app.listen(PORT, () => {
  console.log(`Jobs runner listening on port ${PORT}`);
});
</file>

<file path="src/api/search.ts">
/**
 * Search API - keyword search endpoint
 * Placeholder for search endpoint
 */

export default async function handler(req: any, res: any) {
  // Placeholder implementation
  res.status(200).json({ message: 'Search API not implemented yet' });
}
</file>

<file path="src/bq/client.ts">
import { BigQuery } from '@google-cloud/bigquery';

let bqInstance: BigQuery | null = null;

export function getBigQuery(): BigQuery {
  if (!bqInstance) {
    const projectId = process.env.BQ_PROJECT_ID;
    if (!projectId) {
      throw new Error('BQ_PROJECT_ID environment variable is required');
    }
    
    // Use service account key if GOOGLE_APPLICATION_CREDENTIALS is set
    const keyFilename = process.env.GOOGLE_APPLICATION_CREDENTIALS;
    if (keyFilename) {
      bqInstance = new BigQuery({ 
        projectId,
        keyFilename 
      });
    } else {
      // Fall back to Application Default Credentials
      bqInstance = new BigQuery({ projectId });
    }
  }
  return bqInstance;
}

export async function getDataset() {
  const bq = getBigQuery();
  const datasetId = process.env.BQ_DATASET || 'ncc_production';
  const location = process.env.BQ_LOCATION || 'US';
  const dataset = bq.dataset(datasetId, { location });
  await dataset.get({ autoCreate: true });
  return dataset;
}

export async function getTable(name: string) {
  const dataset = await getDataset();
  return dataset.table(name);
}
</file>

<file path="src/core/checkpoint.ts">
/**
 * Checkpoint module - tracks processing stages and errors
 */

export function getStage(_: string): string | null {
  return null;
}

export function setStage(_: string, __: string): void {
  // No-op placeholder
}

export function setError(_: string, __: string): void {
  // No-op placeholder
}
</file>

<file path="src/core/ingestion.ts">
/**
 * Ingestion module - fetches new newsletters from Gmail
 */

export async function ingestNewNewsletters(inboxOrAll: 'all' | string): Promise<void> {
  throw new Error('ingestNewNewsletters not implemented yet');
}
</file>

<file path="src/core/processor.ts">
/**
 * Processing module - chunks and embeds newsletter content
 */

export async function processUnchunkedMessages(): Promise<void> {
  throw new Error('processUnchunkedMessages not implemented yet');
}
</file>

<file path="src/core/publisher.ts">
/**
 * Publisher canonicalization module
 */

export function publisherCanonical(_: any): any {
  throw new Error('publisherCanonical not implemented yet');
}
</file>

<file path="src/embeddings/vertex.ts">
import { GoogleAuth } from 'google-auth-library';

export interface EmbedOptions {
  model?: string;
  location?: string;
}

export async function embedBatch(
  texts: string[],
  options: EmbedOptions = {}
): Promise<number[][]> {
  const projectId = process.env.BQ_PROJECT_ID;
  const model = options.model || process.env.EMB_MODEL || 'text-embedding-004';
  // Vertex AI uses region codes (us-central1), not BigQuery locations (US)
  // Map common BigQuery locations to Vertex AI regions
  const bqLocation = process.env.BQ_LOCATION || 'US';
  const locationMap: Record<string, string> = {
    'US': 'us-central1',
    'EU': 'europe-west1',
    'asia-northeast1': 'asia-northeast1',
  };
  const location = options.location || process.env.EMB_LOCATION || locationMap[bqLocation] || 'us-central1';

  if (!projectId) {
    throw new Error('BQ_PROJECT_ID environment variable is required');
  }

  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform'],
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  if (!accessToken.token) {
    throw new Error('Failed to get access token');
  }

  const endpoint = `https://${location}-aiplatform.googleapis.com/v1/projects/${projectId}/locations/${location}/publishers/google/models/${model}:predict`;

  const instances = texts.map((text) => ({
    content: text,
    task_type: 'RETRIEVAL_DOCUMENT',
  }));

  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      Authorization: `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ instances }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Vertex AI API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();

  if (!data.predictions || !Array.isArray(data.predictions)) {
    throw new Error('Invalid response format from Vertex AI API');
  }

  const embeddings: number[][] = data.predictions.map((pred: any) => {
    if (pred.embeddings) {
      return pred.embeddings.values || pred.embeddings;
    }
    throw new Error('Missing embeddings in prediction');
  });

  return embeddings;
}
</file>

<file path="src/gmail/client.ts">
import { google, gmail_v1 } from 'googleapis';

import { authenticate } from '@google-cloud/local-auth';

import * as fs from 'fs/promises';

import * as path from 'path';

import { getOAuthCredentials } from './token-provider';



const TOKEN_DIR = path.resolve('.tokens');

const TOKEN_PATH = (inbox: 'me' | 'other') => path.join(TOKEN_DIR, `token.${inbox}.json`);

const CREDENTIALS_PATH = path.resolve('credentials.json');



async function ensureTokenDir() {

  try { await fs.mkdir(TOKEN_DIR, { recursive: true }); } catch {}

}



async function loadSavedCredentials(inbox: 'me' | 'other') {

  try {

    const content = await fs.readFile(TOKEN_PATH(inbox), 'utf8');

    const creds = JSON.parse(content);

    return google.auth.fromJSON(creds) as any; // authorized_user payload

  } catch {

    return null;

  }

}



async function saveCredentials(auth: any, inbox: 'me' | 'other') {

  const raw = await fs.readFile(CREDENTIALS_PATH, 'utf8');

  const content = JSON.parse(raw);

  const keys = content.installed || content.web;

  const payload = {

    type: 'authorized_user',

    client_id: keys.client_id,

    client_secret: keys.client_secret,

    refresh_token: auth.credentials.refresh_token,

  };

  await ensureTokenDir();

  await fs.writeFile(TOKEN_PATH(inbox), JSON.stringify(payload, null, 2));

}



export async function deleteToken(inbox: 'me' | 'other'): Promise<void> {
  try { await fs.unlink(TOKEN_PATH(inbox)); } catch {}
}



export async function getGmail(inbox: 'me' | 'other', opts?: { reauth?: boolean }): Promise<gmail_v1.Gmail> {

  if (opts?.reauth) await deleteToken(inbox);



  const isReadonly = process.env.GMAIL_READONLY !== 'false'; // default true

  const scope = isReadonly

    ? ['https://www.googleapis.com/auth/gmail.readonly']

    : ['https://www.googleapis.com/auth/gmail.modify'];



  // Try token provider first (cloud path: env vars, or local path: files)
  const credentials = await getOAuthCredentials(inbox);

  if (credentials) {

    // Construct OAuth2Client directly from credentials (headless path)
    const oAuth2Client = new google.auth.OAuth2(

      credentials.client_id,

      credentials.client_secret,

      'urn:ietf:wg:oauth:2.0:oob' // unused with refresh token but required

    );

    oAuth2Client.setCredentials({ refresh_token: credentials.refresh_token });



    try {

      const gmail = google.gmail({ version: 'v1', auth: oAuth2Client });

      await gmail.users.labels.list({ userId: 'me' });

      console.log(`Gmail: authorized ${inbox} (headless)`);

      return gmail;

    } catch (error: any) {

      const errorMsg = error.message || JSON.stringify(error);

      if (errorMsg.includes('invalid_rapt') || errorMsg.includes('invalid_grant')) {

        throw new Error('Auth requires re-consent. Refresh token may be revoked. Re-run with --reauth (and ensure Desktop credentials).');

      }

      throw new Error(`Gmail auth failed: ${error.message || 'unknown error'}`);

    }

  }



  // Fall back to local file-based token loading (existing behavior)
  let saved;

  try {

    saved = await loadSavedCredentials(inbox);

  } catch {

    saved = null;

  }



  if (saved) {

    try {

      const gmail = google.gmail({ version: 'v1', auth: saved });

      await gmail.users.labels.list({ userId: 'me' });

      console.log(`Gmail: authorized ${inbox}`);

      return gmail;

    } catch (error: any) {

      const errorMsg = error.message || JSON.stringify(error);

      if (errorMsg.includes('invalid_rapt') || errorMsg.includes('invalid_grant')) {

        throw new Error('Auth requires re-consent. Re-run with --reauth (and ensure Desktop credentials).');

      }

      throw new Error(`Gmail auth failed: ${error.message || 'unknown error'}`);

    }

  }



  // Last resort: interactive OAuth flow (local development only)
  let client;

  try {

    client = await authenticate({

      scopes: scope,

      keyfilePath: CREDENTIALS_PATH,

    });

  } catch (error: any) {

    const errorMsg = error.message || JSON.stringify(error);

    if (errorMsg.includes('invalid_rapt') || errorMsg.includes('invalid_grant')) {

      throw new Error('Auth requires re-consent. Re-run with --reauth (and ensure Desktop credentials).');

    }

    throw new Error(`Gmail authenticate failed: ${error.message || 'unknown error'}`);

  }



  await saveCredentials(client, inbox);

  const gmail = google.gmail({ version: 'v1', auth: client });

  console.log(`Gmail: authorized ${inbox}`);

  return gmail;

}
</file>

<file path="src/gmail/token-provider.ts">
import * as fs from 'fs/promises';
import * as path from 'path';

export interface OAuthCredentials {
  client_id: string;
  client_secret: string;
  refresh_token: string;
}

const TOKEN_DIR = path.resolve('.tokens');
const TOKEN_PATH = (inbox: 'me' | 'other') => path.join(TOKEN_DIR, `token.${inbox}.json`);
const CREDENTIALS_PATH = path.resolve('credentials.json');

/**
 * Get OAuth credentials for the specified inbox.
 * 
 * Cloud path (preferred): Reads from environment variables:
 *   - GMAIL_CLIENT_ID
 *   - GMAIL_CLIENT_SECRET
 *   - GMAIL_REFRESH_TOKEN_ME (for 'me' inbox)
 *   - GMAIL_REFRESH_TOKEN_OTHER (for 'other' inbox)
 * 
 * Local path (fallback): Reads from local files:
 *   - credentials.json (for client_id/client_secret)
 *   - .tokens/token.{me|other}.json (for refresh_token)
 * 
 * @param inbox - 'me' or 'other'
 * @returns OAuth credentials or null if not found
 */
export async function getOAuthCredentials(inbox: 'me' | 'other'): Promise<OAuthCredentials | null> {
  // Cloud path: prefer environment variables
  const clientId = process.env.GMAIL_CLIENT_ID;
  const clientSecret = process.env.GMAIL_CLIENT_SECRET;
  const refreshTokenEnv = inbox === 'me' 
    ? process.env.GMAIL_REFRESH_TOKEN_ME 
    : process.env.GMAIL_REFRESH_TOKEN_OTHER;

  if (clientId && clientSecret && refreshTokenEnv) {
    return {
      client_id: clientId,
      client_secret: clientSecret,
      refresh_token: refreshTokenEnv,
    };
  }

  // Local path: fall back to file-based tokens
  try {
    // Read credentials.json for client_id/client_secret
    const credentialsContent = await fs.readFile(CREDENTIALS_PATH, 'utf8');
    const credentials = JSON.parse(credentialsContent);
    const keys = credentials.installed || credentials.web;
    
    if (!keys?.client_id || !keys?.client_secret) {
      return null;
    }

    // Read token file for refresh_token
    const tokenContent = await fs.readFile(TOKEN_PATH(inbox), 'utf8');
    const tokenData = JSON.parse(tokenContent);
    
    // Token file format: { type: 'authorized_user', client_id, client_secret, refresh_token }
    if (tokenData.refresh_token) {
      return {
        client_id: keys.client_id,
        client_secret: keys.client_secret,
        refresh_token: tokenData.refresh_token,
      };
    }
  } catch (error) {
    // File not found or parse error - return null
    return null;
  }

  return null;
}
</file>

<file path="src/lib/bigquery.ts">
/**
 * BigQuery client module
 * Placeholder client
 */

export const bq = {}; // placeholder client
</file>

<file path="src/lib/config.ts">
/**
 * Configuration module - loads environment variables
 */

export const cfg = {
  projectId: process.env.BQ_PROJECT_ID || '',
  dataset: process.env.BQ_DATASET || 'ncc_production',
  location: process.env.BQ_LOCATION || 'US',
  adminToken: process.env.ADMIN_TOKEN || '',
  ingestLabel: process.env.GMAIL_INGEST_LABEL || 'Ingested',
  paidLabel: process.env.GMAIL_PAID_LABEL || 'Paid $',
};
</file>

<file path="src/lib/vertex.ts">
/**
 * Vertex AI client module
 * Placeholder client
 */

export const vertex = {}; // placeholder
</file>

<file path="src/ops/health.ts">
import { getBigQuery } from '../bq/client';
import { GoogleAuth } from 'google-auth-library';

const PROJECT = process.env.BQ_PROJECT_ID || 'newsletter-control-center';
const REGION = process.env.NCC_REGION || 'us-central1';
const DATASET = process.env.BQ_DATASET || 'ncc_production';
const LOCATION = process.env.BQ_LOCATION || 'US';

const auth = new GoogleAuth({
  scopes: ['https://www.googleapis.com/auth/cloud-platform'],
});

interface JobExecutionTime {
  job: string;
  lastSuccessTime: Date | null;
  status: 'success' | 'stale' | 'missing';
}

interface ReconcileStats {
  rawEmails: number;
  emailsChunked: number;
  chunks: number;
  chunksEmbedded: number;
  chunkCoverage: number; // 0-100
  embeddingCoverage: number; // 0-100
}

interface HealthCheckResult {
  ok: boolean;
  reason?: string;
  details: {
    jobs: JobExecutionTime[];
    reconcile: ReconcileStats;
  };
}

/**
 * Get last successful execution time for a Cloud Run job using API
 */
async function getLastJobExecutionTime(jobName: string): Promise<JobExecutionTime> {
  try {
    const client = await auth.getClient();
    const tokenResponse = await client.getAccessToken();
    const accessToken = tokenResponse.token;
    
    if (!accessToken) {
      throw new Error('Failed to get access token');
    }
    
    // List executions for the job, filtering for successful ones
    const apiUrl = `https://${REGION}-run.googleapis.com/v2/projects/${PROJECT}/locations/${REGION}/jobs/${jobName}/executions?pageSize=1`;
    
    const response = await fetch(apiUrl, {
      headers: {
        Authorization: `Bearer ${accessToken}`,
      },
    });
    
    if (!response.ok) {
      throw new Error(`API error: ${response.status}`);
    }
    
    const data = await response.json();
    const executions = data.executions || [];
    
    // Find the most recent successful execution
    const successfulExec = executions.find((exec: any) => 
      exec.status?.conditions?.some((c: any) => c.type === 'Completed' && c.status === 'True')
    );
    
    if (successfulExec) {
      const completionTime = successfulExec.status?.completionTime;
      
      if (completionTime) {
        const lastSuccess = new Date(completionTime);
        const now = new Date();
        const minutesAgo = (now.getTime() - lastSuccess.getTime()) / (1000 * 60);
        
        return {
          job: jobName,
          lastSuccessTime: lastSuccess,
          status: minutesAgo <= 120 ? 'success' : 'stale',
        };
      }
    }
    
    return {
      job: jobName,
      lastSuccessTime: null,
      status: 'missing',
    };
  } catch (error) {
    // Job might not exist or no executions
    return {
      job: jobName,
      lastSuccessTime: null,
      status: 'missing',
    };
  }
}

/**
 * Get reconcile stats for last 24 hours
 */
async function getReconcileStats(): Promise<ReconcileStats> {
  const bq = getBigQuery();
  const t0 = 'TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)';
  
  const queries = {
    raw24h: `
      SELECT COUNT(*) AS count
      FROM \`${PROJECT}.${DATASET}.raw_emails\`
      WHERE ingested_at >= ${t0}
    `,
    emailsWithChunks24h: `
      SELECT COUNT(DISTINCT gmail_message_id) AS count
      FROM \`${PROJECT}.${DATASET}.chunks\`
      WHERE gmail_message_id IN (
        SELECT gmail_message_id
        FROM \`${PROJECT}.${DATASET}.raw_emails\`
        WHERE ingested_at >= ${t0}
      )
    `,
    chunks24h: `
      SELECT COUNT(*) AS count
      FROM \`${PROJECT}.${DATASET}.chunks\`
      WHERE gmail_message_id IN (
        SELECT gmail_message_id
        FROM \`${PROJECT}.${DATASET}.raw_emails\`
        WHERE ingested_at >= ${t0}
      )
    `,
    embeddedChunks24h: `
      SELECT COUNT(*) AS count
      FROM \`${PROJECT}.${DATASET}.chunk_embeddings\`
      WHERE chunk_id IN (
        SELECT chunk_id
        FROM \`${PROJECT}.${DATASET}.chunks\`
        WHERE gmail_message_id IN (
          SELECT gmail_message_id
          FROM \`${PROJECT}.${DATASET}.raw_emails\`
          WHERE ingested_at >= ${t0}
        )
      )
    `,
  };
  
  try {
    const [raw24hRows] = await bq.query({ query: queries.raw24h, location: LOCATION });
    const rawEmails = (raw24hRows[0] as { count: number }).count;
    
    const [emailsWithChunks24hRows] = await bq.query({ query: queries.emailsWithChunks24h, location: LOCATION });
    const emailsChunked = (emailsWithChunks24hRows[0] as { count: number }).count;
    
    const [chunks24hRows] = await bq.query({ query: queries.chunks24h, location: LOCATION });
    const chunks = (chunks24hRows[0] as { count: number }).count;
    
    const [embeddedChunks24hRows] = await bq.query({ query: queries.embeddedChunks24h, location: LOCATION });
    const chunksEmbedded = (embeddedChunks24hRows[0] as { count: number }).count;
    
    const chunkCoverage = rawEmails > 0 ? Math.round((emailsChunked / rawEmails) * 10000) / 100 : 100;
    const embeddingCoverage = chunks > 0 ? Math.round((chunksEmbedded / chunks) * 10000) / 100 : 100;
    
    return {
      rawEmails,
      emailsChunked,
      chunks,
      chunksEmbedded,
      chunkCoverage,
      embeddingCoverage,
    };
  } catch (error: any) {
    console.error('Error getting reconcile stats:', error);
    // Return safe defaults
    return {
      rawEmails: 0,
      emailsChunked: 0,
      chunks: 0,
      chunksEmbedded: 0,
      chunkCoverage: 0,
      embeddingCoverage: 0,
    };
  }
}

/**
 * Run health check
 */
export async function checkHealth(): Promise<HealthCheckResult> {
  const monitoredJobs = ['ncc-ingest-me', 'ncc-ingest-other', 'ncc-chunks', 'ncc-embeddings'];
  
  // Get job execution times (parallel)
  const jobPromises = monitoredJobs.map(job => getLastJobExecutionTime(job));
  const jobs = await Promise.all(jobPromises);
  
  // Get reconcile stats
  const reconcile = await getReconcileStats();
  
  // Check conditions
  const allJobsRecent = jobs.every(j => j.status === 'success');
  const chunkCoverageOk = reconcile.chunkCoverage === 100;
  const embeddingCoverageOk = reconcile.embeddingCoverage === 100;
  
  const ok = allJobsRecent && chunkCoverageOk && embeddingCoverageOk;
  
  const reasons: string[] = [];
  if (!allJobsRecent) {
    const staleJobs = jobs.filter(j => j.status !== 'success').map(j => j.job);
    reasons.push(`Jobs not recent: ${staleJobs.join(', ')}`);
  }
  if (!chunkCoverageOk) {
    reasons.push(`Chunk coverage: ${reconcile.chunkCoverage}% (expected 100%)`);
  }
  if (!embeddingCoverageOk) {
    reasons.push(`Embedding coverage: ${reconcile.embeddingCoverage}% (expected 100%)`);
  }
  
  return {
    ok,
    reason: reasons.length > 0 ? reasons.join('; ') : undefined,
    details: {
      jobs,
      reconcile,
    },
  };
}
</file>

<file path="src/types/index.ts">
/**
 * Type definitions
 */

export type Inbox = 'all' | string;
</file>

<file path=".gcloudignore">
# .gcloudignore - ensure package.json is included
# Override default patterns to include necessary files

# Include package files
!package.json
!package-lock.json

# Ignore common development files
node_modules/
.git/
.gitignore
.env
.env.local
*.log
.DS_Store

# Keep source code
!src/
!scripts/
!tsconfig.json
</file>

<file path="debug/substack-politics.html">
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="norton-safeweb-site-verification" content="24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd" />
        
        <link rel="preconnect" href="https://substackcdn.com" />
        

        
            <title data-rh="true">Explore | Substack</title>
            
            <meta data-rh="true" name="theme-color" content="#ffffff"/><meta data-rh="true" name="theme-color" content="#16171d"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="og:title" content="Explore | Substack"/><meta data-rh="true" name="twitter:title" content="Explore | Substack"/><meta data-rh="true" name="description" content="See the top posts on Substack today"/><meta data-rh="true" property="og:description" content="See the top posts on Substack today"/><meta data-rh="true" name="twitter:description" content="See the top posts on Substack today"/><meta data-rh="true" property="og:image" content="https://substackcdn.com/image/fetch/$s_!RY_a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Freader%2Fbrowse-page-thumb.jpg"/><meta data-rh="true" name="twitter:image" content="https://substackcdn.com/image/fetch/$s_!RY_a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Freader%2Fbrowse-page-thumb.jpg"/><meta data-rh="true" name="twitter:card" content="summary"/>
            
            
        

        

        <style>
          @layer legacy, tailwind, pencraftReset, pencraft;
        </style>

        
        <link rel="preload" as="style" href="https://substackcdn.com/bundle/theme/reader2.69c1077a2331d7ca4074.css" />
        
        
        

        
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8700.57a7ba5d.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8447.9939e29c.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/1172.eae5b575.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/9163.16c4aa11.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6068.0d496b62.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4108.32661963.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4347.dbcb881f.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/55.177652a1.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/reader2.57927340.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4347.dbcb881f.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4108.32661963.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6068.0d496b62.css" />
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/9163.16c4aa11.css" />
            
        

        
        
        
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover" />
        <meta name="author" content="Substack" />
        <meta property="og:url" content="https://substack.com/browse/politics" />
        
        
        <link rel="canonical" href="https://substack.com/browse/politics" />
        

        

        

        
            
                <link rel="shortcut icon" href="https://substackcdn.com/icons/substack/favicon.ico" sizes="32x32">
            
        
            
                <link rel="icon" href="https://substackcdn.com/icons/substack/icon.svg" type="image/svg+xml">
            
        
            
                <link rel="apple-touch-icon" href="https://substackcdn.com/icons/substack/apple-touch-icon.png">
            
        
            
        
            
        
            
        

        

        

        
            <style>
    /* Cahuenga */
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 300;
        src: local('Cahuenga Light'), local('Cahuenga-Light'), url(https://substackcdn.com/fonts/Cahuenga-Light.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 300;
        src: local('Cahuenga Light Italic'), local('Cahuenga-Light-Italic'), url(https://substackcdn.com/fonts/Cahuenga-LightItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 400;
        src: local('Cahuenga Regular'), local('Cahuenga-Regular'), url(https://substackcdn.com/fonts/Cahuenga-Regular.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 400;
        src: local('Cahuenga Italic'), local('Cahuenga-Italic'), url(https://substackcdn.com/fonts/Cahuenga-Italic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 500;
        src: local('Cahuenga SemiBold'), local('Cahuenga-SemiBold'), url(https://substackcdn.com/fonts/Cahuenga-SemiBold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 500;
        src: local('Cahuenga SemiBold Italic'), local('Cahuenga-SemiBold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-SemiBoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 600;
        src: local('Cahuenga Bold'), local('Cahuenga-Bold'), url(https://substackcdn.com/fonts/Cahuenga-Bold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 600;
        src: local('Cahuenga Bold Italic'), local('Cahuenga-Bold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-BoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 700;
        src: local('Cahuenga ExtraBold'), local('Cahuenga-ExtraBold'), url(https://substackcdn.com/fonts/Cahuenga-ExtraBold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 700;
        src: local('Cahuenga ExtraBold Italic'), local('Cahuenga-ExtraBold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-ExtraBoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
</style>

        
        
          <style>
            @font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8on7mTNmnUHowCw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onXmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onTmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onrmTNmnUHo.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M9knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M2knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M3knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M5knjsS_ul.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3FafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3OafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3PafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3BafaPWnII.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}
            
          </style>
        
        

        <style>:root{--background_pop:#ff6719;--cover_bg_color:#FFFFFF;--background_pop_darken:#ff5600;--print_on_pop:#ffffff;--border_subtle:#f6e7df;--background_subtle:rgba(255, 232, 221, 0.4);--print_pop:#ff6719;--cover_print_primary:#363737;--cover_print_secondary:#757575;--cover_print_tertiary:#b6b6b6;--cover_border_color:#ff6719;--background_contrast_1:#f0f0f0;--background_contrast_2:#dddddd;--background_contrast_3:#b7b7b7;--background_contrast_4:#929292;--background_contrast_5:#515151;--color_theme_detail:#e6e6e6;--background_contrast_pop:rgba(255, 103, 25, 0.4);--input_background:#ffffff;--cover_input_background:#ffffff;--tooltip_background:#191919;--background_pop_rgb:255, 103, 25;--background_pop_rgb_pc:255 103 25;}</style>

        
            <link rel="stylesheet" href="https://substackcdn.com/bundle/theme/reader2.69c1077a2331d7ca4074.css" />
        

        <style></style>

        

        

        

        
    </head>

    <body class="pc-root">
        
            <script>
              if (window.matchMedia) {
                const match = window.matchMedia('(prefers-color-scheme: dark)');

                match.addEventListener('change', handleColorSchemeChange);
                handleColorSchemeChange(match);

                function handleColorSchemeChange(event) {
                  const localSetting = localStorage.colorScheme;
                  document.documentElement.classList.toggle('dark-mode', localSetting == null ?
                    event.matches :  localSetting === 'dark');
                }
              }
            </script>
        

        

        

        

        

        

        <div id="entry">
            <div style="--size-left-nav:232px;" class="reader-nav-root reader2-font-base"><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-reset"><div role="navigation" aria-label="Main navigation" aria-orientation="vertical" class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-fixed pc-reset sizing-border-box-DggLA4 sidebar-y6xw9g"><div class="pencraft pc-display-flex pc-flexDirection-column pc-height-64 pc-padding-8 pc-alignItems-flex-start pc-reset flex-auto-j3S2WA sizing-border-box-DggLA4"><button tabindex="0" type="button" aria-label="Home" data-href="/home" class="pencraft pc-display-flex pc-flexDirection-column pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 pc-borderRadius-md showFocus-sk_vEm pencraft buttonBase-GK1x3M"><div class="pencraft pc-display-flex pc-width-48 pc-height-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset"><svg role="img" style="height:28px;width:28px;" width="28" height="28" viewBox="0 0 20 20" fill="var(--color-nav-logo)" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M1.96484 0.624512H18.0354V2.70052H1.96484V0.624512Z" stroke="none"></path><path d="M1.96484 4.77655H18.0354V6.85254H1.96484V4.77655Z" stroke="none"></path><path d="M1.96484 8.92857V19.9505L10.0001 14.6347L18.0354 19.9505V8.92857H1.96484Z" stroke="none"></path></g></svg></div></button></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-padding-8 pc-justifyContent-flex-start pc-reset flex-grow-rzmknG"><a tabindex="0" matchSubpaths aria-label="Home" role="button" href="/home?" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" style="height:24px;width:24px;" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M10 18.1302C11.259 18.7392 12.741 18.7392 14 18.1302M2 11.0059V19.0976C2 20.7006 3.34315 22 5 22H19C20.6569 22 22 20.7006 22 19.0976V11.0059C22 10.4471 21.7503 9.91553 21.315 9.54809L12.6575 2.23854C12.2808 1.92049 11.7192 1.92049 11.3425 2.23854L2.68496 9.54809C2.24974 9.91553 2 10.4471 2 11.0059Z"></path></g></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Home</div></a><a tabindex="0" matchSubpaths native aria-label="Subscriptions" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" style="height:24px;width:24px;" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M1.99999 14L2.00001 18.5C2.00001 20.1569 3.34316 21.5 5.00001 21.5H19C20.6569 21.5 22 20.1569 22 18.5V14M1.99999 14H8.5L9.29342 16.3732C9.41681 16.7422 9.76236 16.9911 10.1515 16.9911H13.8485C14.2376 16.9911 14.5832 16.7422 14.7066 16.3732L15.5 14H22M1.99999 14L5.12 3.30286C5.34518 2.53079 6.05291 2 6.85714 2H17.1429C17.9471 2 18.6548 2.53079 18.88 3.30286L22 14"></path></g></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Subscriptions</div></a><a tabindex="0" matchSubpaths native aria-label="Chat" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" style="height:24px;width:24px;" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M3.57895 22.2631C4.63158 21.2105 4.98246 18.9298 4.63158 18.0526C4.55024 18.0246 3.94258 17.7935 3.86123 17.7644C2.70525 17.3507 2 16.215 2 14.9872V5.81496C2 4.60782 2.68342 3.49229 3.8249 3.09955C7.48196 1.84131 13.7925 0.807216 20.1374 3.07721C21.2934 3.49079 22 4.6271 22 5.85486V15.0271C22 16.2342 21.3154 17.3503 20.1739 17.7429C17.6947 18.5958 13.9964 19.3455 9.89474 19.0328C8.84211 21.1381 6.21053 22.2631 3.57895 22.2631Z"></path></g></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Chat</div></a><a tabindex="0" matchSubpaths native aria-label="Activity" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" style="height:24px;width:24px;" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M9.84924 21.9622C11.2032 22.6392 12.7968 22.6392 14.1507 21.9622M9.446 1.90703C8.59467 2.16583 6.63661 3.14927 5.61501 5.01263C4.33801 7.34183 4.70638 10.6565 4.33801 12.0002C3.96965 13.344 3.06102 14.3294 2.42252 15.8822C2.27112 16.2504 2.15563 16.5749 2.06751 16.8662C1.72354 18.0031 2.74693 18.9878 3.93475 18.9878H20.0652C21.2531 18.9878 22.2764 18.0031 21.9325 16.8662C21.8444 16.5749 21.7289 16.2504 21.5775 15.8822C20.939 14.3294 20.0303 13.344 19.662 12.0002C19.2936 10.6565 19.662 7.34183 18.385 5.01263C17.3634 3.14927 15.4053 2.16583 14.554 1.90703C12.8884 1.40469 11.1116 1.40469 9.446 1.90703Z"></path></g></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Activity</div></a><a tabindex="0" matchSubpaths aria-label="Explore" role="button" href="/explore?" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" style="height:24px;width:24px;" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg"><g><title></title><path d="M21 21L15.6702 15.6435M15.6702 15.6435C17.1101 14.1968 18 12.2023 18 10C18 5.58172 14.4183 2 10 2C5.58174 2 2.00002 5.58172 2.00002 10C2.00002 14.4183 5.58174 18 10 18C12.216 18 14.2215 17.099 15.6702 15.6435Z"></path></g></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Explore</div></a><a tabindex="0" matchSubpaths native aria-label="Profile" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-circle-user-round"><path d="M18 20a6 6 0 0 0-12 0"></path><circle cx="12" cy="10" r="4"></circle><circle cx="12" cy="12" r="10"></circle></svg></div><div class="pencraft pc-reset flex-grow-rzmknG color-secondary-ls1g8s line-height-20-t4M0El font-text-qe4AeH size-15-Psle70 weight-medium-fw81nC reset-IxiVJZ">Profile</div></a><div class="pencraft pc-display-flex pc-flexDirection-column pc-minWidth-48 pc-paddingTop-12 pc-paddingBottom-12 pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o">Create</button></div></div></div></div><div class="reader-nav-page"><div class="pencraft pc-display-flex pc-minWidth-0 pc-reset flex-grow-rzmknG root-tHtwM7"><div id="reader-nav-page-scroll" class="reader-nav-page-scroll"><div class="pencraft pc-display-flex pc-minWidth-0 pc-paddingLeft-20 pc-mobile-paddingLeft-0 pc-paddingRight-20 pc-mobile-paddingRight-0 pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-minWidth-0 pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-16 pc-paddingBottom-16 pc-paddingLeft-0 pc-mobile-paddingLeft-16 pc-position-relative pc-reset"><button type="button" aria-hidden="true" style="position:fixed;top:1px;left:1px;width:1px;height:0px;padding:0px;margin:-1px;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border-width:0;"></button><div class="pencraft pc-display-flex pc-position-relative pc-reset overflow-hidden-WdpwT6 pullX-4-uulEPF pullY-4-a9xmnt"><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH left-Tg8vqp"><div class="overlay-zrMCxn primary-lv_sOW"></div><button tabindex="0" type="button" disabled aria-hidden="true" class="pencraft pc-reset pencraft arrowButtonOverlaid-xLyA_z iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left"><path d="m15 18-6-6 6-6"></path></svg></button></div><div aria-label="Tabs" role="tablist" aria-orientation="horizontal" class="pencraft pc-display-flex pc-gap-8 pc-minWidth-0 pc-padding-4 pc-reset flex-grow-rzmknG cursor-default-flE2S1 row-gZz5wu"><button tabindex="0" type="button" id="headlessui-tabs-tab-P0-12" role="tab" aria-selected="true" data-headlessui-state="selected" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-mono-P_CW5x size_sm-G3LciD">Staff picks</button></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH right-i3oWGi"><div class="overlay-zrMCxn primary-lv_sOW"></div><button tabindex="0" type="button" disabled aria-hidden="true" class="pencraft pc-reset pencraft arrowButtonOverlaid-xLyA_z iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></button></div></div></div></div><div class="pencraft pc-display-flex pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div id="reader-nav-page-scroll" class="reader-nav-page-scroll"><div class="pencraft pc-display-flex pc-paddingLeft-24 pc-mobile-paddingLeft-0 pc-paddingRight-24 pc-mobile-paddingRight-0 pc-justifyContent-center pc-reset flex-grow-rzmknG"><div class="reader-nav-center"><div class="reader2-page reader2-font-base"><div class="reader2-page-body"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-20 pc-reset flex-grow-rzmknG"></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset reader2-inbox-sidebar"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset reader2-inbox-sidebar-fixed"><div class="pencraft pc-display-flex pc-paddingLeft-8 pc-paddingRight-8 pc-justifyContent-space-between pc-alignItems-center pc-reset"><div style="width:156px;height:24px;min-width:156px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-paddingLeft-12 pc-paddingRight-12 pc-reset"><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div><div class="pencraft pc-display-flex pc-paddingLeft-12 pc-paddingRight-12 pc-paddingTop-8 pc-paddingBottom-8 pc-justifyContent-space-between pc-alignItems-center pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-gap-12 pc-alignItems-center pc-reset"><div style="width:10px;height:20px;min-width:10px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu"></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-reset"><div style="width:120px;height:20px;min-width:120px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div><div style="width:80px;height:16px;min-width:80px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu bar-WF2dWN"></div></div></div><div style="width:32px;height:32px;min-width:32px;" class="pencraft pc-reset pc-borderRadius-sm placeholder-sSVwvu circle-qbcUfs"></div></div></div></div></div></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft cta-X9m2EB ctaBase-Dht55f buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_md-gCDS3o">Get app</button></div><div class="pencraft pc-display-flex pc-reset modalViewer-xO_gxg"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset container-K5XPov"></div><div inert role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><div style="left:auto;right:16px;bottom:16px;z-index:1001;transform:translateY(0px);" role="region" aria-label="Notification" class="pencraft pc-position-fixed pc-reset sizing-border-box-DggLA4"></div><div></div>
        </div>

        


        
        
        
        <script>window._preloads        = JSON.parse("{\"isEU\":false,\"language\":\"en\",\"country\":\"US\",\"userLocale\":{\"language\":\"en\",\"region\":\"US\",\"source\":\"accept-language\"},\"base_url\":\"https://substack.com\",\"stripe_publishable_key\":\"pk_live_51QfnARLDSWi1i85FBpvw6YxfQHljOpWXw8IKi5qFWEzvW8HvoD8cqTulR9UWguYbYweLvA16P7LN6WZsGdZKrNkE00uGbFaOE3\",\"captcha_site_key\":\"6LdYbsYZAAAAAIFIRh8X_16GoFRLIReh-e-q6qSa\",\"pub\":null,\"confirmedLogin\":false,\"hide_intro_popup\":true,\"block_auto_login\":false,\"domainInfo\":{\"isSubstack\":true,\"customDomain\":null},\"experimentFeatures\":{},\"experimentExposures\":{},\"siteConfigs\":{\"score_upsell_email\":\"control\",\"first_chat_email_enabled\":true,\"notes_video_max_duration_minutes\":5,\"reader-onboarding-promoted-pub\":737237,\"new_commenter_approval\":false,\"pub_update_opennode_api_key\":false,\"ios_trending_topic_note_badge\":\"control\",\"enable_user_report_review\":true,\"zendesk_automation_cancellations\":false,\"hide_book_a_meeting_button\":false,\"mfa_action_box_enabled\":false,\"publication_max_bylines\":35,\"no_contest_charge_disputes\":false,\"feed_posts_previously_seen_weight\":0.1,\"publication_tabs_reorder\":false,\"comp_expiry_email_new_copy\":\"NONE\",\"free_unlock_required\":false,\"traffic_rule_check_enabled\":false,\"amp_emails_enabled\":false,\"enable_post_summarization\":false,\"live_stream_host_warning_message\":\"\",\"bitcoin_enabled\":false,\"minimum_ios_os_version\":\"17.0.0\",\"show_entire_square_image\":false,\"hide_subscriber_count\":false,\"publication_author_display_override\":\"\",\"ios_webview_payments_enabled\":\"control\",\"generate_pdf_tax_report\":false,\"show_generic_post_importer\":false,\"enable_pledges_modal\":true,\"include_pdf_invoice\":false,\"app_upsell_after_posting_notes\":\"experiment\",\"notes_weight_watch_video\":5,\"use_post_podcast_import_batching\":true,\"enable_react_dashboard\":false,\"meetings_v1\":false,\"enable_videos_page\":false,\"exempt_from_gtm_filter\":false,\"group_sections_and_podcasts_in_menu\":false,\"boost_optin_modal_enabled\":true,\"standards_and_enforcement_features_enabled\":false,\"pub_creation_captcha_behavior\":\"risky_pubs_or_rate_limit\",\"post_blogspot_importer\":false,\"suggested_search_metadata_web_ui\":false,\"notes_weight_short_item_boost\":0.15,\"pub_tts_override\":\"default\",\"disable_monthly_subscriptions\":false,\"skip_welcome_email\":false,\"chat_reader_thread_notification_default\":false,\"scheduled_pinned_posts\":false,\"disable_redirect_outbound_utm_params\":false,\"reader_gift_referrals_enabled\":true,\"dont_show_guest_byline\":false,\"like_comments_enabled\":true,\"subscription_bar_all_debug_enabled\":false,\"temporal_livestream_ended_draft\":true,\"enable_author_note_email_toggle\":false,\"meetings_embed_publication_name\":false,\"fallback_to_archive_search_on_section_pages\":false,\"livekit_track_egress_custom_base_url\":\"http://livekit-egress-custom-recorder-participant-test.s3-website-us-east-1.amazonaws.com\",\"people_you_may_know_algorithm\":\"experiment\",\"welcome_screen_blurb_override\":\"\",\"live_stream_guest_overlay\":\"control\",\"like_posts_enabled\":true,\"ios_gutterless_feed\":\"control\",\"twitter_player_card_enabled\":true,\"feed_promoted_user\":false,\"writer_beta_android_enable_post_editor_v2\":false,\"show_note_stats_for_all_notes\":false,\"section_specific_csv_imports_enabled\":false,\"disable_podcast_feed_description_cta\":false,\"bypass_profile_substack_logo_detection\":false,\"use_preloaded_player_sources\":false,\"enable_tiktok_oauth\":false,\"list_pruning_enabled\":false,\"facebook_connect\":false,\"opt_in_to_sections_during_subscribe\":false,\"dpn_weight_share\":2,\"underlined_colored_links\":false,\"unified_presskit_enabled\":false,\"extract_stripe_receipt_url\":false,\"enable_aligned_images\":false,\"max_image_upload_mb\":64,\"enable_android_dms_writer_beta\":false,\"threads_suggested_ios_version\":null,\"pledges_disabled\":false,\"threads_minimum_ios_version\":812,\"hide_podcast_email_setup_link\":false,\"subscribe_captcha_behavior\":\"default\",\"publication_ban_sample_rate\":0,\"grant_viral_gifts_to_gift_recipients\":\"experiment\",\"ios_enable_publication_activity_tab\":false,\"custom_themes_substack_subscribe_modal\":false,\"share_viral_gift_as_link\":\"experiment\",\"opt_in_to_sections_during_subscribe_include_main_pub_newsletter\":false,\"continue_support_cta_in_newsletter_emails\":false,\"bloomberg_syndication_enabled\":false,\"lists_enabled\":false,\"ios_feed_media_content_mode\":\"fit\",\"generated_database_maintenance_mode\":false,\"allow_document_freeze\":false,\"subscription_bar_all_debug_subdomains\":null,\"podcast_main_feed_is_firehose\":false,\"pub_app_incentive_gift\":\"\",\"no_embed_redirect\":false,\"translate_mobile_app\":false,\"customized_email_from_name_for_new_follow_emails\":\"treatment\",\"spotify_open_access_sandbox_mode\":false,\"fullstory_enabled\":false,\"chat_reply_poll_interval\":3,\"dpn_weight_follow_or_subscribe\":3,\"speaker_focus_group_shot\":\"experiment\",\"updated_note_sharing_assets_enabled\":false,\"enable_reader_marketing_page\":false,\"force_pub_links_to_use_subdomain\":false,\"always_show_cookie_banner\":false,\"hide_media_download_option\":false,\"hide_post_restacks\":false,\"feed_item_source_debug_mode\":false,\"writer_beta_android_enable_post_editor\":false,\"thefp_enable_account_menu\":false,\"enable_user_status_ui\":false,\"publication_homepage_title_display_override\":\"\",\"pub_banned_word_list\":\"\",\"post_preview_highlight_byline\":false,\"4k_video\":false,\"enable_islands_section_intent_screen\":false,\"tfp_free_week_reg_wall\":false,\"post_metering_enabled\":false,\"notifications_disabled\":\"\",\"cross_post_notification_threshold\":1000,\"facebook_connect_prod_app\":true,\"feed_enable_live_streams\":false,\"force_into_pymk_ranking\":false,\"minimum_android_version\":756,\"live_stream_krisp_noise_suppression_enabled\":false,\"enable_transcription_translations\":false,\"ios_post_video_pager_alpha_enabled\":false,\"use_og_image_as_twitter_image_for_post_previews\":false,\"always_use_podcast_channel_art_as_episode_art_in_rss\":false,\"cookie_preference_middleware_enabled\":false,\"seo_tier_override\":\"NONE\",\"no_follow_links\":false,\"publisher_api_enabled\":false,\"zendesk_support_priority\":\"default\",\"enable_post_clips_stats\":false,\"enable_subscriber_referrals_awards\":true,\"ios_profile_themes_feed_permalink_enabled\":false,\"use_publication_language_for_transcription\":false,\"show_substack_funded_gifts_tooltip\":true,\"disable_ai_transcription\":false,\"thread_permalink_preview_min_ios_version\":4192,\"android_toggle_on_website_enabled\":false,\"internal_android_enable_post_editor\":false,\"edit_profile_feed_item\":false,\"updated_inbox_ui\":false,\"web_reader_podcasts_tab\":false,\"use_temporal_thumbnail_selection_workflow\":false,\"live_stream_creation_enabled\":false,\"disable_card_element_in_europe\":false,\"web_growth_item_promotion_threshold\":0,\"enable_web_typing_indicators\":false,\"web_vitals_sample_rate\":0,\"allow_live_stream_auto_takedown\":\"true\",\"search_ranker_variant\":\"control\",\"enable_progressive_speaker_focus_clips\":false,\"post_advanced_search\":\"control\",\"ai_image_generation_enabled\":true,\"disable_personal_substack_initialization\":false,\"section_specific_welcome_pages\":false,\"local_payment_methods\":\"control\",\"enable_linkedin_oauth\":true,\"posts_in_rss_feed\":20,\"post_rec_endpoint\":\"\",\"publisher_dashboard_section_selector\":false,\"reader_surveys_platform_question_order\":\"36,1,4,2,3,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35\",\"ios_toggle_on_website_enabled\":false,\"login_guard_app_link_in_email\":true,\"community_moderators_enabled\":false,\"monthly_sub_is_one_off\":false,\"unread_notes_activity_digest\":\"control\",\"display_cookie_settings\":false,\"welcome_page_query_params\":false,\"enable_free_podcast_urls\":false,\"comp_expiry_emails_disabled\":false,\"enable_description_on_polls\":false,\"use_microlink_for_instagram_embeds\":false,\"post_notification_batch_delay_ms\":30000,\"free_signup_confirmation_behavior\":\"with_email_validation\",\"ios_post_stats_for_admins\":false,\"live_stream_concurrent_viewer_count_drawer\":false,\"use_livestream_post_media_composition\":true,\"section_specific_preambles\":false,\"android_live_stream_multihost_enabled\":false,\"show_menu_on_posts\":false,\"app_upsell_follow_prompt\":\"control\",\"ios_post_subscribe_web_routing\":true,\"opt_into_all_trending_topics\":false,\"ios_writer_stats_public_launch_v2\":false,\"min_size_for_phishing_check\":1,\"enable_android_post_stats\":false,\"ios_chat_revamp_enabled\":false,\"app_onboarding_survey_email\":false,\"post_notification_batch_chunk_size\":100,\"thefp_enable_pullquote_alignment\":false,\"thefp_enable_pullquote_color\":false,\"republishing_enabled\":false,\"app_mode\":false,\"show_phone_banner\":false,\"live_stream_video_enhancer\":\"internal\",\"minimum_ios_version\":2200,\"enable_author_pages\":false,\"enable_decagon_chat\":true,\"first_month_upsell\":\"control\",\"enable_fedcm\":false,\"new_user_checklist_enabled\":\"use_follower_count\",\"android_enable_auto_gain_control\":false,\"enable_updated_webview_checkout\":false,\"show_attached_profile_for_pub_setting\":false,\"welcome_page_update_desktop_visuals_limited\":\"experiment\",\"rss_verification_code\":\"\",\"notification_post_emails\":\"experiment\",\"ios_profile_subdomain_chips\":true,\"chat_suppress_contributor_push_option_enabled\":false,\"live_stream_invite_ttl_seconds\":600,\"feed_ranking_per_post_clip_cap\":2,\"export_hooks_enabled\":false,\"audio_encoding_bitrate\":null,\"bestseller_pub_override\":false,\"extra_seats_coupon_type\":false,\"post_subdomain_universal_links\":false,\"post_import_max_file_size\":26214400,\"notes_weight_follow\":4,\"enable_post_to_post_link_data_event\":true,\"livekit_reconnect_slate_url\":\"https://mux-livestream-assets.s3.us-east-1.amazonaws.com/custom-disconnect-slate-tall.png\",\"exclude_from_pymk_suggestions\":false,\"publication_ranking_variant\":\"experiment\",\"disable_annual_subscriptions\":false,\"enable_android_dms\":false,\"feed_ranker_use_user_comment_reaction_cache\":true,\"pub_auto_moderation_enabled\":false,\"disable_live_stream_ai_trimming_by_default\":false,\"recipes_enabled\":false,\"disable_deletion\":false,\"ios_default_coupon_enabled\":false,\"notes_weight_read_post\":5,\"notes_weight_reply\":3,\"livekit_egress_custom_base_url\":\"http://livekit-egress-custom-recorder.s3-website-us-east-1.amazonaws.com\",\"clip_focused_video_upload_flow\":false,\"live_stream_max_guest_users\":2,\"enable_video_seo_data\":false,\"can_reimport_unsubscribed_users_with_2x_optin\":false,\"feed_posts_weight_subscribed\":0,\"included_in_demo_feed\":false,\"live_event_mixin\":\"\",\"review_incoming_email\":\"default\",\"app_install_prompts\":\"native_banner_if_supported\",\"enable_founding_gifts\":false,\"ios_chat_uikit\":false,\"enable_sponsorship_campaigns\":false,\"thread_permalink_preview_min_android_version\":2037,\"enable_fp_new_events_page\":true,\"default_thumbnail_time\":10,\"pub_ranking_weight_immediate_engagement\":1,\"pub_ranking_weight_retained_engagement\":1,\"load_test_unichat\":false,\"ios_reader_post_sharing_flow_v2\":\"experiment\",\"notes_read_post_baseline\":0,\"live_stream_head_alignment_guide\":false,\"show_open_post_as_pdf_button\":false,\"free_press_combo_subscribe_flow_enabled\":false,\"restack_with_image\":false,\"free_press_tabbed_subscribe_flow\":\"control\",\"gift_from_substack_modal\":\"experiment\",\"onboarding_badges_android\":\"control\",\"gifts_from_substack_feature_available\":true,\"disable_ai_clips\":false,\"thefp_enable_web_livestream_kicking\":false,\"enable_elevenlabs_voiceovers\":false,\"growth_sources_all_time\":false,\"android_upgrade_alert_dialog\":true,\"headline_testing_enabled\":true,\"translated_notifications_enabled\":false,\"show_simple_post_editor\":false,\"desktop_live_streaming_enabled\":false,\"search_ranker_query_augmentation\":\"enabled\",\"enable_publication_podcasts_page\":false,\"ios_payment_connection_enabled\":true,\"app_install_reminder_email\":\"experiment\",\"use_rotated_recommendations_request\":false,\"thefp_enable_dynamic_toaster\":false,\"thefp_enable_america_250\":true,\"ios_note_composer_settings_enabled\":false,\"android_v2_post_video_player_enabled\":false,\"enable_direct_message_request_bypass\":false,\"enable_apple_news_sync\":false,\"postsById_batch_size\":20,\"free_press_newsletter_promo_enabled\":false,\"enable_ios_livestream_stats\":false,\"disable_live_stream_reactions\":false,\"enable_high_follower_dm\":true,\"ios_welcome_video_profile_prompt\":false,\"clip_generation_3rd_party_vendor\":\"internal\",\"ios_notification_settings_enabled\":false,\"tone_down_sidebar_livestreams\":false,\"notes_weight_negative\":1,\"ios_discover_tab_min_installed_date\":\"2025-06-09T16:56:58+0000\",\"notes_weight_click_see_more\":2,\"enable_publish_youtube_connect_repeat_upsell\":false,\"edit_profile_theme_colors\":false,\"backend_enable_subscription_bar\":true,\"disable_clipping_for_readers\":false,\"android_enable_subscription_bar\":false,\"apple_fee_percent\":15,\"allow_anonymous_personal_pub_creation\":false,\"feed_posts_weight_reply\":3,\"feed_posts_weight_negative\":5,\"feed_posts_weight_like\":1.5,\"feed_posts_weight_share\":3,\"feed_posts_weight_save\":3,\"enable_press_kit_preview_modal\":false,\"dpn_weight_tap_clickbait_penalty\":0.5,\"feed_posts_weight_sign_up\":4,\"live_stream_video_degradation_preference\":\"maintainFramerate\",\"pause_app_badges\":false,\"android_enable_publication_activity_tab\":false,\"thefp_paywall_with_plans\":\"experiment\",\"notes_weight_like\":2,\"profile_feed_expanded_inventory\":false,\"phone_verification_fallback_to_twilio\":false,\"livekit_mux_latency_mode\":\"low\",\"feed_posts_weight_long_click\":1,\"feed_juiced_user\":0,\"vertical_video_player_in_feed_1\":\"experiment\",\"show_branded_intro_setting\":true,\"free_press_single_screen_subscribe_flow_enabled\":false,\"notes_click_see_more_baseline\":0.35,\"android_edit_user_links\":true,\"android_move_feed_tabs\":false,\"ios_inline_replies\":\"control\",\"android_enable_user_status_ui\":false,\"use_advanced_commerce_api_for_iap\":false,\"skip_free_preview_language_in_podcast_notes\":false,\"larger_wordmark_on_publication_homepage\":false,\"video_editor_full_screen\":false,\"enable_mobile_stats_for_admins\":false,\"ios_profile_themes_note_composer_enabled\":false,\"reduce_post_search_fuzziness\":\"treatment\",\"related_posts_web\":\"experiment\",\"notes_weight_click_item\":3,\"notes_weight_long_visit\":1,\"ios_note_sharing_assets\":\"control\",\"bypass_single_unlock_token_limit\":false,\"notes_watch_video_baseline\":0.08,\"add_section_and_tag_metadata\":false,\"daily_promoted_notes_enabled\":true,\"feed_ranker_use_user_feed_restack_comment_cache\":true,\"enable_islands_cms\":false,\"enable_livestream_combined_stats\":false,\"ios_social_subgroups_enabled\":false,\"ios_notification_preference_onboarding_copy\":\"control\",\"enable_drip_campaigns\":false,\"ios_offline_mode_enabled\":false,\"post_management_search_engine\":\"elasticsearch\",\"new_bestseller_leaderboard_feed_item_enabled\":false,\"feed_main_disabled\":false,\"enable_account_settings_revamp\":false,\"allowed_email_domains\":\"one\",\"thefp_enable_fp_recirc_block\":false,\"ios_web_subscription_payments\":\"experiment\",\"ios_full_search_results\":\"control\",\"enable_debug_logs_ios\":false,\"show_pub_content_on_profile_for_pub_id\":0,\"web_badge_popover_treatment\":\"lottie\",\"show_pub_content_on_profile\":false,\"livekit_track_egress\":true,\"video_tab_mixture_pattern\":\"npnnnn\",\"enable_theme_contexts\":false,\"onboarding_suggestions_search\":\"experiment\",\"feed_tuner_enabled\":false,\"livekit_mux_latency_mode_rtmp\":\"low\",\"notes_weight_follow_boost\":3,\"thefp_homepage_portrait_layout\":false,\"ios_post_bottom_share\":\"experiment\",\"fcm_high_priority\":false,\"android_enable_unified_composer\":\"control\",\"ios_user_status_sheet_subscribe_button\":\"experiment\",\"subscription_bar_top_selection_strategy_v2\":\"destination_wau_pub_score\",\"search_ranker_load_test_pct\":0,\"iap_announcement_blog_url\":\"\",\"android_onboarding_progress_persistence\":\"control\",\"use_theme_editor_v2\":false,\"dpn_weight_tap_bonus_subscribed\":3,\"thefp_email_paywall_with_plans\":\"experiment\",\"ios_custom_buttons_enabled\":true,\"ios_livestream_feedback\":false,\"founding_plan_upgrade_warning\":false,\"suggested_search_ranking_v1\":\"control\",\"dpn_weight_like\":3,\"ios_iap_opt_out_enabled\":false,\"skip_kafka_retry_messages\":false,\"related_notes_variations\":\"control\",\"android_view_post_share_assets_employees_only\":false,\"thefp_show_fixed_footer_paywall\":false,\"android_subscription_queue_experiment\":\"experiment\",\"ios_viral_gift_entry_points\":\"treatment\",\"ios_post_video_pager_enabled_v2\":\"experiment\",\"render_high_quality_clips\":true,\"ios_subscription_pogs\":\"experiment\",\"use_elasticsearch_for_category_tabs\":\"control\",\"dpn_weight_reply\":2,\"age_verification_uk_rollout_percentage\":0,\"android_enable_edit_profile_theme\":false,\"android_enable_view_profile_theme\":false,\"enable_refresh_token_deduplication\":true,\"dpn_weight_follow\":3,\"live_stream_audio_enhancer_v2\":\"auphonic\",\"age_verification_uk_enabled\":false,\"enable_speaker_focus_clips\":true,\"search_ranker_load_test_ranking_window\":2000,\"ios_new_post_sharing_flow_enabled\":false,\"ignore_video_in_notes_length_limit\":false,\"web_show_scores_on_sports_tab\":false,\"notes_weight_click_share\":3,\"direct_device_push_notifications\":false,\"allow_long_videos\":true,\"enable_livestream_rtmp_invites\":true,\"dpn_score_threshold\":0,\"thefp_enable_follow_module\":false,\"publication_user_invite_tier_2_only\":false,\"dpn_weight_follow_bonus\":0.5,\"ios_post_subscribe_follow_related\":\"control\",\"use_intro_clip_and_branded_intro_by_default\":false,\"community_profile_activity_feed\":false,\"new_user_subscribe_follow_prompt_override\":\"none\",\"ios_subscription_pogs_new_users\":\"experiment\",\"ios_subscription_pogs_old_users\":\"experiment\",\"android_subscription_queue_experiment_2\":\"experiment\",\"enable_viewing_all_livestream_viewers\":false,\"tabbed_notes_search\":\"control\",\"enable_clip_prompt_variant_filtering\":true,\"dpn_ranking_enabled\":true,\"sequential_retrieval_model_pct\":100,\"android_vertical_post_player_3\":\"experiment\",\"dpn_model_variant\":\"experiment\",\"add_byline_by_user_id_tier_2_only\":false,\"enable_outlier_subscriber_activity_item\":true,\"android_reader_share_assets_3\":\"control\",\"enable_apple_podcast_auto_publish\":false,\"dpn_weight_disable\":10,\"enable_dashboard_data_callout\":true,\"speaker_focus_hls_usage_enabled\":true,\"ios_reader_post_sharing_flow\":\"control\",\"dpn_weight_open\":2.5,\"dpn_suggested_content_title\":\"control\",\"dpn_weight_long_session\":1.5,\"live_stream_in_trending_topic_overrides\":\"\",\"android_vertical_post_player\":\"control\",\"enable_notes_admins\":false,\"ios_post_embed_card_enabled\":true,\"enable_suggested_searches\":true,\"saved_post_reactivation_push_notification\":\"experiment\",\"android_synchronous_push_notif_handling\":\"control\",\"disable_user_status_for_user\":false,\"a24_redemption_link\":\"\",\"podcast_subscribe_flow_app_upsell\":\"treatment\",\"trending_topics_module_long_term_experiment\":\"experiment\",\"share_asset_ordering\":\"control\",\"suggested_search_metadata_web_market_ui\":false,\"dpn_weight_restack\":2,\"dpn_weight_negative\":40,\"search_retrieval_variant\":\"control\",\"session_version_invalidation_enabled\":false,\"publisher_banner\":\"\",\"ios_enable_subscription_stories\":false,\"direct_device_push_notifications_ios\":\"experiment\",\"forced_featured_topic_id\":\"\",\"android_vertical_post_player_2\":\"control\",\"web_notes_trending_topics_enabled\":\"control\",\"dpn_weight_tap\":5,\"ios_live_stream_auto_gain_enabled\":false,\"client_support_for_image_quote_feed_item_type\":false,\"get_app_pill_welcome_page_v2\":\"experiment\",\"android_rank_share_destinations_experiment\":\"experiment\",\"android_note_share_assets\":\"control\",\"feed_permalink_referred_modal\":\"experiment\",\"ios_live_stream_pip_dismiss_v2\":\"control\",\"web_post_reading_share_nudge\":\"experiment\"},\"publicationSettings\":null,\"publicationUserSettings\":null,\"userSettings\":{\"user_id\":null,\"activity_likes_enabled\":true,\"artist_mode_enabled\":false,\"dashboard_nav_refresh_enabled\":false,\"hasDismissedSectionToNewsletterRename\":false,\"is_guest_post_enabled\":true,\"feed_web_nux_seen_at\":null,\"has_seen_select_to_restack_tooltip_nux\":false,\"invite_friends_nux_dismissed_at\":null,\"suggestions_feed_item_last_shown_at\":null,\"has_seen_select_to_restack_modal\":false,\"last_home_tab\":null,\"last_notification_alert_shown_at\":null,\"disable_reply_hiding\":false,\"newest_seen_chat_item_published_at\":null,\"explicitContentEnabled\":false,\"contactMatchingEnabled\":false,\"messageRequestLevel\":\"everyone\",\"liveStreamAcceptableInviteLevel\":\"everyone\",\"liveStreamAcceptableChatLevel\":\"everyone\",\"creditTokensTreatmentExposed\":false,\"appBadgeIncludesChat\":false,\"autoPlayVideo\":true,\"smart_delivery_enabled\":false,\"chatbotTermsLastAcceptedAt\":null,\"has_seen_notes_post_app_upsell\":false,\"substack_summer_nux_dismissed_at\":null,\"first_note_id\":null,\"show_concurrent_live_stream_viewers\":false,\"has_dismissed_fp_download_pdf_nux\":false,\"edit_profile_feed_item_dismissed_at\":null,\"mobile_permalink_app_upsell_seen_at\":null,\"new_user_checklist_enabled\":false,\"new_user_follow_subscribe_prompt_dismissed_at\":null,\"has_seen_youtube_shorts_auto_publish_announcement\":false,\"has_seen_publish_youtube_connect_upsell\":false,\"notificationQualityFilterEnabled\":true,\"hasSeenOnboardingNewslettersScreen\":false},\"subscriberCountDetails\":null,\"mux_env_key\":\"u42pci814i6011qg3segrcpp9\",\"sentry_environment\":\"production\",\"launchWelcomePage\":false,\"pendingInviteForActiveLiveStream\":null,\"abTestingId\":\"21a3b32b-b2a5-4400-a5e5-8379e5cd8a8a\",\"hideHeader\":true,\"hideFooter\":true,\"useDarkMode\":true,\"features\":{},\"browser\":{\"name\":\"Chrome\",\"version\":\"120.0.0.0\",\"major\":\"120\"},\"showCookieBanner\":false,\"disabledCookies\":[],\"dd_env\":\"prod\",\"dd_ti\":true}")</script>
        <script>window._analyticsConfig = JSON.parse("{\"properties\":{\"subdomain\":null,\"has_plans\":false,\"pub_community_enabled\":false,\"is_personal_publication\":false,\"is_subscribed\":false,\"is_free_subscribed\":false,\"is_author\":false,\"is_contributor\":false,\"is_admin\":false,\"is_founding\":false},\"localGaPixelId\":\"G-TLW0DF6G5V\",\"adwordsAccountId\":\"AW-316245675\",\"adwordsEventSendTo\":\"Tf76CKqcyL4DEKuN5pYB\"}")</script>

        
        
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2083.9fe47401.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/598.e0f754b2.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6253.7ef90e5b.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1907.06314015.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5725.f89fa29e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/400.f58ec134.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2658.434e8abb.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3793.7670135c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9305.4446497f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7562.fa3052cd.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3897.43992966.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2536.4ec76a84.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1729.d1dd27aa.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9063.b9bb8752.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5698.5cde0d38.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8497.c62520ac.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/689.c7c10643.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2263.bf3789ec.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2516.387658de.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/697.e6987c29.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1526.5380ec12.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/870.a3d1ad2d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7201.1a57f066.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8402.70f37023.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3327.7d38ec3f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3456.9fec4e8c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4601.cce8465b.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8700.e4d3b887.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7675.48136919.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1172.482f9195.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4840.7a4b31fc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1800.8dfa859c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6068.dad48f36.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6683.3fe2570e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4086.c87308fe.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4218.89d4d69e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1877.184c4a6f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2861.4fe3fda3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3925.53178e34.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8779.efcab556.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8978.ddea496d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8547.4880f2ac.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1462.624979db.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7806.7be556f1.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4056.a51cc600.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4108.2ed748c9.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8237.c16e8b02.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7353.f2175d70.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8771.35268a8e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5664.5941cd89.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7582.061a040f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3713.621f4255.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7880.068214df.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9830.d732da60.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7588.470e542f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2455.112e7f81.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6820.561b7a39.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5197.3ba7cb63.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9966.981fdbbe.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3672.4791d66e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3715.4f2d02ab.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6331.423f4dd0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/66.8b8b8b55.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5688.a55da2f2.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/195.14116008.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/184.9c596e46.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1749.d7ec76d0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8511.d0e67f2d.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4347.83001e70.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8782.b02a6b78.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7800.188f3a15.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7147.1ade4a0e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1219.37ad75aa.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1638.9963990f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8096.000ae3c7.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1601.df88a9e3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3948.794d0cd5.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4975.2fb4cb82.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/55.b1b85ee5.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7953.e053d699.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6449.d065ba11.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/reader2.cf908c13.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4347.83001e70.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1749.d7ec76d0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/184.9c596e46.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9713.e20f466f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/195.14116008.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7222.75062350.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5688.a55da2f2.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6331.423f4dd0.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9966.981fdbbe.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5197.3ba7cb63.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6820.561b7a39.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2455.112e7f81.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7880.068214df.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3713.621f4255.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5664.5941cd89.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8237.c16e8b02.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4108.2ed748c9.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8547.4880f2ac.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8779.efcab556.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2861.4fe3fda3.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1877.184c4a6f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4086.c87308fe.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6068.dad48f36.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1800.8dfa859c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/4840.7a4b31fc.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7675.48136919.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3327.7d38ec3f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7201.1a57f066.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1830.c43016d7.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1526.5380ec12.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/271.cb27bcc4.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2516.387658de.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2263.bf3789ec.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6869.3db4edc8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8497.c62520ac.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5698.5cde0d38.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7523.a8d4dc49.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1729.d1dd27aa.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/2536.4ec76a84.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3897.43992966.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3793.7670135c.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/400.f58ec134.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7957.817ce496.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/5725.f89fa29e.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1907.06314015.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/6253.7ef90e5b.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/598.e0f754b2.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9137.17769749.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/9935.32ea3b54.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/185.520016a8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/3948.794d0cd5.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1837.4d1d7fc8.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/8096.000ae3c7.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/1638.9963990f.js" charset="utf-8"></script>
            
                <script defer type="module" src="https://substackcdn.com/bundle/static/js/7800.188f3a15.js" charset="utf-8"></script>
            
        
        <script nomodule>
            (function() {
                var message = 'Your browser does not support modern JavaScript modules. Please upgrade your browser for the best experience.';
                var warningDiv = document.createElement('div');
                warningDiv.style.color = 'red';
                warningDiv.style.padding = '10px';
                warningDiv.style.margin = '10px 0';
                warningDiv.style.border = '1px solid red';
                warningDiv.style.backgroundColor = 'lightyellow';
                warningDiv.innerText = message;
                document.body.prepend(warningDiv);
            })();
        </script>

        
            <!-- Datadog Analytics -->
            <script>
              (function(h,o,u,n,d) {
                h=h[d]=h[d]||{q:[],onReady:function(c){h.q.push(c)}}
                d=o.createElement(u);d.async=1;d.src=n
                n=o.getElementsByTagName(u)[0];n.parentNode.insertBefore(d,n)
              })(window,document,'script','https://www.datadoghq-browser-agent.com/us1/v5/datadog-rum.js','DD_RUM')
              window.DD_RUM.onReady(function() {
                window.DD_RUM.init({
                  clientToken: 'puba71073f072643721169b68f352438710',
                  applicationId: '2e321b35-c76b-4073-8d04-cc9a10461793',
                  site: 'datadoghq.com',
                  service: 'substack-web',
                  env: window._preloads.dd_env,
                  version: 'd14c50c6ee04f127a4b5d4cf2403b9c95d541740',
                  sessionSampleRate: 1,
                  sessionReplaySampleRate: 100,
                  trackUserInteractions: window._preloads.dd_ti,
                  trackResources: true,
                  trackLongTasks: true,
                  defaultPrivacyLevel: 'mask-user-input',
                  allowedTracingUrls: [/https?:\/\/(.+\/.)?substack(cdn)?\.com/]
                });
              })
            </script>
            <!-- End Datadog Analytics -->

            <!-- Cloudflare Web Analytics -->
            <script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"token": "216309cffb464db4b0e02daf0b8e8060"}'></script>
            <!-- End Cloudflare Web Analytics -->
        

        <!-- Fallback tracking pixels -->
        

        

        <noscript>
    <style>
        #nojs-banner {
            position: fixed;
            bottom: 0;
            left: 0;
            padding: 16px 16px 16px 32px;
            width: 100%;
            box-sizing: border-box;
            background: red;
            color: white;
            font-family: -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            font-size: 13px;
            line-height: 13px;
        }
        #nojs-banner a {
            color: inherit;
            text-decoration: underline;
        }
    </style>

    <div id="nojs-banner">
        This site requires JavaScript to run correctly. Please <a href="https://enable-javascript.com/" target="_blank">turn on JavaScript</a> or unblock scripts
    </div>
</noscript>


        

        

        
        
    <script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9984c79ebda468ce',t:'MTc2MjA5ODU0NC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script></body>
</html>
</file>

<file path="debug/substack-puppeteer.html">
<!DOCTYPE html><html lang="en"><head>
        <meta charset="utf-8">
        <meta name="norton-safeweb-site-verification" content="24usqpep0ejc5w6hod3dulxwciwp0djs6c6ufp96av3t4whuxovj72wfkdjxu82yacb7430qjm8adbd5ezlt4592dq4zrvadcn9j9n-0btgdzpiojfzno16-fnsnu7xd">
        
        <link rel="preconnect" href="https://substackcdn.com">
        

        
            <title>Explore | Substack</title>
            
            <meta data-rh="true" name="theme-color" content="#ffffff"><meta data-rh="true" name="theme-color" content="#16171d"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="og:title" content="Explore | Substack"><meta data-rh="true" name="twitter:title" content="Explore | Substack"><meta data-rh="true" name="description" content="See the top posts on Substack today"><meta data-rh="true" property="og:description" content="See the top posts on Substack today"><meta data-rh="true" name="twitter:description" content="See the top posts on Substack today"><meta data-rh="true" property="og:image" content="https://substackcdn.com/image/fetch/$s_!RY_a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Freader%2Fbrowse-page-thumb.jpg"><meta data-rh="true" name="twitter:image" content="https://substackcdn.com/image/fetch/$s_!RY_a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Freader%2Fbrowse-page-thumb.jpg"><meta data-rh="true" name="twitter:card" content="summary">
            
            
        

        

        <style>
          @layer legacy, tailwind, pencraftReset, pencraft;
        </style>

        
        <link rel="preload" as="style" href="https://substackcdn.com/bundle/theme/reader2.69c1077a2331d7ca4074.css">
        
        
        

        
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8700.57a7ba5d.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/8447.9939e29c.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/1172.eae5b575.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/9163.16c4aa11.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6068.0d496b62.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4108.32661963.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4347.dbcb881f.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/55.177652a1.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/reader2.57927340.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4347.dbcb881f.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/4108.32661963.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/6068.0d496b62.css">
            
                <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/9163.16c4aa11.css">
            
        

        
        
        
        
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0, viewport-fit=cover">
        <meta name="author" content="Substack">
        <meta property="og:url" content="https://substack.com/browse/politics">
        
        
        <link rel="canonical" href="https://substack.com/browse/politics">
        

        

        

        
            
                <link rel="shortcut icon" href="https://substackcdn.com/icons/substack/favicon.ico" sizes="32x32">
            
        
            
                <link rel="icon" href="https://substackcdn.com/icons/substack/icon.svg" type="image/svg+xml">
            
        
            
                <link rel="apple-touch-icon" href="https://substackcdn.com/icons/substack/apple-touch-icon.png">
            
        
            
        
            
        
            
        

        

        

        
            <style>
    /* Cahuenga */
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 300;
        src: local('Cahuenga Light'), local('Cahuenga-Light'), url(https://substackcdn.com/fonts/Cahuenga-Light.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 300;
        src: local('Cahuenga Light Italic'), local('Cahuenga-Light-Italic'), url(https://substackcdn.com/fonts/Cahuenga-LightItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 400;
        src: local('Cahuenga Regular'), local('Cahuenga-Regular'), url(https://substackcdn.com/fonts/Cahuenga-Regular.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 400;
        src: local('Cahuenga Italic'), local('Cahuenga-Italic'), url(https://substackcdn.com/fonts/Cahuenga-Italic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 500;
        src: local('Cahuenga SemiBold'), local('Cahuenga-SemiBold'), url(https://substackcdn.com/fonts/Cahuenga-SemiBold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 500;
        src: local('Cahuenga SemiBold Italic'), local('Cahuenga-SemiBold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-SemiBoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 600;
        src: local('Cahuenga Bold'), local('Cahuenga-Bold'), url(https://substackcdn.com/fonts/Cahuenga-Bold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 600;
        src: local('Cahuenga Bold Italic'), local('Cahuenga-Bold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-BoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: normal;
        font-weight: 700;
        src: local('Cahuenga ExtraBold'), local('Cahuenga-ExtraBold'), url(https://substackcdn.com/fonts/Cahuenga-ExtraBold.woff2) format('woff2');
        font-display: fallback;
    }
    @font-face {
        font-family: 'Cahuenga';
        font-style: italic;
        font-weight: 700;
        src: local('Cahuenga ExtraBold Italic'), local('Cahuenga-ExtraBold-Italic'), url(https://substackcdn.com/fonts/Cahuenga-ExtraBoldItalic.woff2) format('woff2');
        font-display: fallback;
    }
</style>

        
        
          <style>
            @font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8on7mTNmnUHowCw.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onXmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onTmTNmnUHowCw.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:italic;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCt-xNNww_2s0amA9M8onrmTNmnUHo.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M9knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M2knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M3knjsS_ulYHs.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:400;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCr-xNNww_2s0amA9M5knjsS_ul.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3FafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3OafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3PafaPWnIIMrY.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20CF,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Spectral';font-style:normal;font-weight:600;font-display:fallback;src:url(https://fonts.gstatic.com/s/spectral/v13/rnCs-xNNww_2s0amA9vmtm3BafaPWnII.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}
            
          </style>
        
        

        <style>:root{--background_pop:#ff6719;--cover_bg_color:#FFFFFF;--background_pop_darken:#ff5600;--print_on_pop:#ffffff;--border_subtle:#f6e7df;--background_subtle:rgba(255, 232, 221, 0.4);--print_pop:#ff6719;--cover_print_primary:#363737;--cover_print_secondary:#757575;--cover_print_tertiary:#b6b6b6;--cover_border_color:#ff6719;--background_contrast_1:#f0f0f0;--background_contrast_2:#dddddd;--background_contrast_3:#b7b7b7;--background_contrast_4:#929292;--background_contrast_5:#515151;--color_theme_detail:#e6e6e6;--background_contrast_pop:rgba(255, 103, 25, 0.4);--input_background:#ffffff;--cover_input_background:#ffffff;--tooltip_background:#191919;--background_pop_rgb:255, 103, 25;--background_pop_rgb_pc:255 103 25;}</style>

        
            <link rel="stylesheet" href="https://substackcdn.com/bundle/theme/reader2.69c1077a2331d7ca4074.css">
        

        <style></style>

        

        

        

        
    <link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/async/9023.400114c3.css"><link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/async/7978.e25666b3.css"><link rel="stylesheet" type="text/css" href="https://substackcdn.com/bundle/static/css/async/4950.400114c3.css"></head>

    <body class="pc-root">
        
            <script type="text/javascript" async="" src="https://www.googletagmanager.com/gtag/js?id=AW-316245675&amp;l=localGaDataLayer&amp;cx=c&amp;gtm=4e5at1"></script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TLW0DF6G5V&amp;l=localGaDataLayer"></script><script async="" src="https://www.datadoghq-browser-agent.com/us1/v5/datadog-rum.js"></script><script>
              if (window.matchMedia) {
                const match = window.matchMedia('(prefers-color-scheme: dark)');

                match.addEventListener('change', handleColorSchemeChange);
                handleColorSchemeChange(match);

                function handleColorSchemeChange(event) {
                  const localSetting = localStorage.colorScheme;
                  document.documentElement.classList.toggle('dark-mode', localSetting == null ?
                    event.matches :  localSetting === 'dark');
                }
              }
            </script>
        

        

        

        

        

        

        <div id="entry"><div style="--size-left-nav: var(--size-80);" class="reader-nav-root reader2-font-base"><div class="pencraft pc-display-flex pc-reset flex-auto-j3S2WA container-fi9IrN"><div role="banner" aria-label="Page header" class="pencraft pc-display-flex pc-zIndex-1 pc-gap-20 pc-paddingLeft-20 pc-paddingRight-20 pc-alignItems-center pc-justifyContent-flex-end pc-reset flex-grow-rzmknG border-bottom-detail-k1F6C4 sizing-border-box-DggLA4 nav-ptYSWX"><button tabindex="0" type="button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_md-gCDS3o rounded-SYxRdz">Sign in</button><button tabindex="0" type="button" class="pencraft pc-reset pencraft buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-RfbeYt size_md-gCDS3o">Create account</button></div></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-position-relative pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-padding-8 pc-position-fixed pc-reset sizing-border-box-DggLA4 sidebar-y6xw9g" role="navigation" aria-label="Main navigation" aria-orientation="vertical"><div id="" class="pencraft pc-display-flex pc-flexDirection-column pc-height-64 pc-padding-8 pc-alignItems-center pc-reset flex-auto-j3S2WA sizing-border-box-DggLA4"><button tabindex="0" type="button" aria-label="Home" data-href="/home" class="pencraft pc-display-flex pc-flexDirection-column pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 pc-borderRadius-md showFocus-sk_vEm pencraft buttonBase-GK1x3M"><div class="pencraft pc-display-flex pc-width-48 pc-height-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset"><svg role="img" width="28" height="28" viewBox="0 0 20 20" fill="var(--color-nav-logo)" stroke-width="1.8" stroke="none" xmlns="http://www.w3.org/2000/svg" style="height: 28px; width: 28px;"><g><title></title><path d="M1.96484 0.624512H18.0354V2.70052H1.96484V0.624512Z" stroke="none"></path><path d="M1.96484 4.77655H18.0354V6.85254H1.96484V4.77655Z" stroke="none"></path><path d="M1.96484 8.92857V19.9505L10.0001 14.6347L18.0354 19.9505V8.92857H1.96484Z" stroke="none"></path></g></svg></div></button></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-8 pc-padding-8 pc-justifyContent-flex-start pc-reset flex-grow-rzmknG"><div><a tabindex="0" matchsubpaths="true" aria-label="Home" role="button" href="/home?" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" style="height: 24px; width: 24px;"><g><title></title><path d="M10 18.1302C11.259 18.7392 12.741 18.7392 14 18.1302M2 11.0059V19.0976C2 20.7006 3.34315 22 5 22H19C20.6569 22 22 20.7006 22 19.0976V11.0059C22 10.4471 21.7503 9.91553 21.315 9.54809L12.6575 2.23854C12.2808 1.92049 11.7192 1.92049 11.3425 2.23854L2.68496 9.54809C2.24974 9.91553 2 10.4471 2 11.0059Z"></path></g></svg></div></a></div><div><a tabindex="0" matchsubpaths="true" native="true" aria-label="Subscriptions" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" style="height: 24px; width: 24px;"><g><title></title><path d="M1.99999 14L2.00001 18.5C2.00001 20.1569 3.34316 21.5 5.00001 21.5H19C20.6569 21.5 22 20.1569 22 18.5V14M1.99999 14H8.5L9.29342 16.3732C9.41681 16.7422 9.76236 16.9911 10.1515 16.9911H13.8485C14.2376 16.9911 14.5832 16.7422 14.7066 16.3732L15.5 14H22M1.99999 14L5.12 3.30286C5.34518 2.53079 6.05291 2 6.85714 2H17.1429C17.9471 2 18.6548 2.53079 18.88 3.30286L22 14"></path></g></svg></div></a></div><div><a tabindex="0" matchsubpaths="true" native="true" aria-label="Chat" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" style="height: 24px; width: 24px;"><g><title></title><path d="M3.57895 22.2631C4.63158 21.2105 4.98246 18.9298 4.63158 18.0526C4.55024 18.0246 3.94258 17.7935 3.86123 17.7644C2.70525 17.3507 2 16.215 2 14.9872V5.81496C2 4.60782 2.68342 3.49229 3.8249 3.09955C7.48196 1.84131 13.7925 0.807216 20.1374 3.07721C21.2934 3.49079 22 4.6271 22 5.85486V15.0271C22 16.2342 21.3154 17.3503 20.1739 17.7429C17.6947 18.5958 13.9964 19.3455 9.89474 19.0328C8.84211 21.1381 6.21053 22.2631 3.57895 22.2631Z"></path></g></svg></div></a></div><div><a tabindex="0" matchsubpaths="true" native="true" aria-label="Activity" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" style="height: 24px; width: 24px;"><g><title></title><path d="M9.84924 21.9622C11.2032 22.6392 12.7968 22.6392 14.1507 21.9622M9.446 1.90703C8.59467 2.16583 6.63661 3.14927 5.61501 5.01263C4.33801 7.34183 4.70638 10.6565 4.33801 12.0002C3.96965 13.344 3.06102 14.3294 2.42252 15.8822C2.27112 16.2504 2.15563 16.5749 2.06751 16.8662C1.72354 18.0031 2.74693 18.9878 3.93475 18.9878H20.0652C21.2531 18.9878 22.2764 18.0031 21.9325 16.8662C21.8444 16.5749 21.7289 16.2504 21.5775 15.8822C20.939 14.3294 20.0303 13.344 19.662 12.0002C19.2936 10.6565 19.662 7.34183 18.385 5.01263C17.3634 3.14927 15.4053 2.16583 14.554 1.90703C12.8884 1.40469 11.1116 1.40469 9.446 1.90703Z"></path></g></svg></div></a></div><div><a tabindex="0" matchsubpaths="true" aria-label="Explore" role="button" href="/explore?" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg role="img" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke-width="2" stroke="var(--color-fg-primary)" stroke-linecap="round" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" style="height: 24px; width: 24px;"><g><title></title><path d="M21 21L15.6702 15.6435M15.6702 15.6435C17.1101 14.1968 18 12.2023 18 10C18 5.58172 14.4183 2 10 2C5.58174 2 2.00002 5.58172 2.00002 10C2.00002 14.4183 5.58174 18 10 18C12.216 18 14.2215 17.099 15.6702 15.6435Z"></path></g></svg></div></a></div><div><a tabindex="0" matchsubpaths="true" native="true" aria-label="Profile" class="pencraft pc-display-flex pc-gap-4 pc-minWidth-48 pc-minHeight-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset bg-primary-zk6FDl hover-KQSFex animate-XFJxE4 cursor-pointer-LYORKw userSelect-none-oDUy26 pc-borderRadius-md pressable-lg-kV7yq8 showFocus-sk_vEm"><div class="pencraft pc-display-flex pc-height-48 pc-minWidth-48 pc-justifyContent-center pc-alignItems-center pc-position-relative pc-reset icon-lyqgV5 inactive-zdmjOL"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-circle-user-round"><path d="M18 20a6 6 0 0 0-12 0"></path><circle cx="12" cy="10" r="4"></circle><circle cx="12" cy="12" r="10"></circle></svg></div></a></div><button tabindex="0" type="button" aria-label="Create" class="pencraft pc-reset pencraft iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_primary-RfbeYt"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-plus"><path d="M5 12h14"></path><path d="M12 5v14"></path></svg></button></div></div></div><div class="reader-nav-page logged-out-top-nav"><div class="pencraft pc-display-flex pc-minWidth-0 pc-reset flex-grow-rzmknG root-tHtwM7"><div id="reader-nav-page-scroll" class="reader-nav-page-scroll"><div class="pencraft pc-display-flex pc-minWidth-0 pc-paddingLeft-20 pc-mobile-paddingLeft-0 pc-paddingRight-20 pc-mobile-paddingRight-0 pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-minWidth-0 pc-position-relative pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset"><div class="pencraft pc-display-flex pc-flexDirection-column pc-paddingTop-16 pc-paddingBottom-16 pc-paddingLeft-0 pc-mobile-paddingLeft-16 pc-position-relative pc-reset"><div class="pencraft pc-display-flex pc-position-relative pc-reset overflow-hidden-WdpwT6 pullX-4-uulEPF pullY-4-a9xmnt"><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH left-Tg8vqp"><div class="overlay-zrMCxn primary-lv_sOW"></div><button tabindex="0" type="button" disabled="" aria-hidden="true" class="pencraft pc-reset pencraft arrowButtonOverlaid-xLyA_z iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-left"><path d="m15 18-6-6 6-6"></path></svg></button></div><div aria-label="Tabs" role="tablist" aria-orientation="horizontal" class="pencraft pc-display-flex pc-gap-8 pc-minWidth-0 pc-padding-4 pc-reset flex-grow-rzmknG cursor-default-flE2S1 row-gZz5wu"><button tabindex="0" type="button" id="headlessui-tabs-tab-P0-18" role="tab" aria-selected="true" data-headlessui-state="selected" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_primary-mono-P_CW5x size_sm-G3LciD">Staff picks</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-19" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Culture</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-20" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Technology</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-21" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Business</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-22" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">U.S. Politics</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-23" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Finance</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-24" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Food &amp; Drink</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-25" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Sports</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-26" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Art &amp; Illustration</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-27" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">World Politics</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-28" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Health Politics</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-29" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">News</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-30" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Fashion &amp; Beauty</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-31" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Music</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-32" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Faith &amp; Spirituality</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-33" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Climate &amp; Environment</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-34" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Science</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-35" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Literature</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-36" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Fiction</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-37" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Health &amp; Wellness</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-38" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Design</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-39" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Travel</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-40" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Parenting</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-41" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Philosophy</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-42" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Comics</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-43" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">International</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-44" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Crypto</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-45" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">History</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-46" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Humor</button><button tabindex="-1" type="button" id="headlessui-tabs-tab-P0-47" role="tab" aria-selected="false" data-headlessui-state="" class="pencraft pc-reset pencraft tab-ntEFBb buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_sm-G3LciD">Education</button></div><div class="pencraft pc-display-flex pc-alignItems-center pc-reset arrowButtonContainer-O4uSiH arrowButtonOverlaidContainer-t10AyH right-i3oWGi"><div class="overlay-zrMCxn primary-lv_sOW"></div><button tabindex="0" type="button" disabled="" aria-hidden="true" class="pencraft pc-reset pencraft arrowButtonOverlaid-xLyA_z iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_sm-G3LciD priority_quaternary-kpMibu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-chevron-right"><path d="m9 18 6-6-6-6"></path></svg></button></div></div></div></div><div class="pencraft pc-display-flex pc-reset flex-grow-rzmknG"><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset flex-grow-rzmknG"><div id="reader-nav-page-scroll" class="reader-nav-page-scroll"><div class="pencraft pc-display-flex pc-paddingLeft-24 pc-mobile-paddingLeft-0 pc-paddingRight-24 pc-mobile-paddingRight-0 pc-justifyContent-center pc-reset flex-grow-rzmknG"><div class="reader-nav-center"><div class="reader2-page reader2-font-base"><div class="reader2-page-body"><div class="pencraft pc-display-flex pc-flexDirection-column pc-gap-20 pc-reset flex-grow-rzmknG"></div></div></div></div></div></div></div></div></div></div></div></div></div><div class="pencraft pc-display-flex pc-reset"><button tabindex="0" type="button" class="pencraft pc-reset pencraft cta-X9m2EB ctaBase-Dht55f buttonBase-GK1x3M buttonText-X0uSmG buttonStyle-r7yGCK priority_secondary-S63h9o size_md-gCDS3o">Get app</button></div><div class="pencraft pc-display-flex pc-reset modalViewer-xO_gxg"></div><div class="pencraft pc-display-flex pc-flexDirection-column pc-reset container-K5XPov"></div><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><div class="reader-onboarding-modal-container"></div></div></div></div></div></div></div><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><div inert="" role="dialog" class="modal typography out gone reader-onboarding-modal wide popup"><div class="modal-table"><div class="modal-row"><div class="modal-cell modal-content"><div class="container"><button tabindex="0" type="button" aria-label="X" data-testid="close-modal" class="pencraft pc-reset pencraft modal-btn modal-exit-btn no-margin iconButton-mq_Et5 iconButtonBase-dJGHgN buttonBase-GK1x3M buttonStyle-r7yGCK size_md-gCDS3o priority_tertiary-rlke8z"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="secondary" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-x"><path d="M18 6 6 18"></path><path d="m6 6 12 12"></path></svg></button><div class="reader-onboarding-modal-container"></div></div></div></div></div></div><div style="left: auto; right: 16px; bottom: 16px; z-index: 1001; transform: translateY(0px);" role="region" aria-label="Notification" class="pencraft pc-position-fixed pc-reset sizing-border-box-DggLA4"></div><div></div>
            
        </div>

        


        
        
        
        <script>window._preloads        = JSON.parse("{\"isEU\":false,\"language\":\"en\",\"country\":\"US\",\"userLocale\":{\"language\":\"en\",\"region\":\"US\",\"source\":\"accept-language\"},\"base_url\":\"https://substack.com\",\"stripe_publishable_key\":\"pk_live_51QfnARLDSWi1i85FBpvw6YxfQHljOpWXw8IKi5qFWEzvW8HvoD8cqTulR9UWguYbYweLvA16P7LN6WZsGdZKrNkE00uGbFaOE3\",\"captcha_site_key\":\"6LdYbsYZAAAAAIFIRh8X_16GoFRLIReh-e-q6qSa\",\"pub\":null,\"confirmedLogin\":false,\"hide_intro_popup\":true,\"block_auto_login\":false,\"domainInfo\":{\"isSubstack\":true,\"customDomain\":null},\"experimentFeatures\":{},\"experimentExposures\":{},\"siteConfigs\":{\"score_upsell_email\":\"control\",\"first_chat_email_enabled\":true,\"notes_video_max_duration_minutes\":5,\"reader-onboarding-promoted-pub\":737237,\"new_commenter_approval\":false,\"pub_update_opennode_api_key\":false,\"ios_trending_topic_note_badge\":\"control\",\"enable_user_report_review\":true,\"zendesk_automation_cancellations\":false,\"hide_book_a_meeting_button\":false,\"mfa_action_box_enabled\":false,\"publication_max_bylines\":35,\"no_contest_charge_disputes\":false,\"feed_posts_previously_seen_weight\":0.1,\"publication_tabs_reorder\":false,\"comp_expiry_email_new_copy\":\"NONE\",\"free_unlock_required\":false,\"traffic_rule_check_enabled\":false,\"amp_emails_enabled\":false,\"enable_post_summarization\":false,\"live_stream_host_warning_message\":\"\",\"bitcoin_enabled\":false,\"minimum_ios_os_version\":\"17.0.0\",\"show_entire_square_image\":false,\"hide_subscriber_count\":false,\"publication_author_display_override\":\"\",\"ios_webview_payments_enabled\":\"control\",\"generate_pdf_tax_report\":false,\"show_generic_post_importer\":false,\"enable_pledges_modal\":true,\"include_pdf_invoice\":false,\"app_upsell_after_posting_notes\":\"experiment\",\"notes_weight_watch_video\":5,\"use_post_podcast_import_batching\":true,\"enable_react_dashboard\":false,\"meetings_v1\":false,\"enable_videos_page\":false,\"exempt_from_gtm_filter\":false,\"group_sections_and_podcasts_in_menu\":false,\"boost_optin_modal_enabled\":true,\"standards_and_enforcement_features_enabled\":false,\"pub_creation_captcha_behavior\":\"risky_pubs_or_rate_limit\",\"post_blogspot_importer\":false,\"suggested_search_metadata_web_ui\":false,\"notes_weight_short_item_boost\":0.15,\"pub_tts_override\":\"default\",\"disable_monthly_subscriptions\":false,\"skip_welcome_email\":false,\"chat_reader_thread_notification_default\":false,\"scheduled_pinned_posts\":false,\"disable_redirect_outbound_utm_params\":false,\"reader_gift_referrals_enabled\":true,\"dont_show_guest_byline\":false,\"like_comments_enabled\":true,\"subscription_bar_all_debug_enabled\":false,\"temporal_livestream_ended_draft\":true,\"enable_author_note_email_toggle\":false,\"meetings_embed_publication_name\":false,\"fallback_to_archive_search_on_section_pages\":false,\"livekit_track_egress_custom_base_url\":\"http://livekit-egress-custom-recorder-participant-test.s3-website-us-east-1.amazonaws.com\",\"people_you_may_know_algorithm\":\"experiment\",\"welcome_screen_blurb_override\":\"\",\"live_stream_guest_overlay\":\"control\",\"like_posts_enabled\":true,\"ios_gutterless_feed\":\"control\",\"twitter_player_card_enabled\":true,\"feed_promoted_user\":false,\"writer_beta_android_enable_post_editor_v2\":false,\"show_note_stats_for_all_notes\":false,\"section_specific_csv_imports_enabled\":false,\"disable_podcast_feed_description_cta\":false,\"bypass_profile_substack_logo_detection\":false,\"use_preloaded_player_sources\":false,\"enable_tiktok_oauth\":false,\"list_pruning_enabled\":false,\"facebook_connect\":false,\"opt_in_to_sections_during_subscribe\":false,\"dpn_weight_share\":2,\"underlined_colored_links\":false,\"unified_presskit_enabled\":false,\"extract_stripe_receipt_url\":false,\"enable_aligned_images\":false,\"max_image_upload_mb\":64,\"enable_android_dms_writer_beta\":false,\"threads_suggested_ios_version\":null,\"pledges_disabled\":false,\"threads_minimum_ios_version\":812,\"hide_podcast_email_setup_link\":false,\"subscribe_captcha_behavior\":\"default\",\"publication_ban_sample_rate\":0,\"grant_viral_gifts_to_gift_recipients\":\"experiment\",\"ios_enable_publication_activity_tab\":false,\"custom_themes_substack_subscribe_modal\":false,\"share_viral_gift_as_link\":\"experiment\",\"opt_in_to_sections_during_subscribe_include_main_pub_newsletter\":false,\"continue_support_cta_in_newsletter_emails\":false,\"bloomberg_syndication_enabled\":false,\"lists_enabled\":false,\"ios_feed_media_content_mode\":\"fit\",\"generated_database_maintenance_mode\":false,\"allow_document_freeze\":false,\"subscription_bar_all_debug_subdomains\":null,\"podcast_main_feed_is_firehose\":false,\"pub_app_incentive_gift\":\"\",\"no_embed_redirect\":false,\"translate_mobile_app\":false,\"customized_email_from_name_for_new_follow_emails\":\"treatment\",\"spotify_open_access_sandbox_mode\":false,\"fullstory_enabled\":false,\"chat_reply_poll_interval\":3,\"dpn_weight_follow_or_subscribe\":3,\"speaker_focus_group_shot\":\"experiment\",\"updated_note_sharing_assets_enabled\":false,\"enable_reader_marketing_page\":false,\"force_pub_links_to_use_subdomain\":false,\"always_show_cookie_banner\":false,\"hide_media_download_option\":false,\"hide_post_restacks\":false,\"feed_item_source_debug_mode\":false,\"writer_beta_android_enable_post_editor\":false,\"thefp_enable_account_menu\":false,\"enable_user_status_ui\":false,\"publication_homepage_title_display_override\":\"\",\"pub_banned_word_list\":\"\",\"post_preview_highlight_byline\":false,\"4k_video\":false,\"enable_islands_section_intent_screen\":false,\"tfp_free_week_reg_wall\":false,\"post_metering_enabled\":false,\"notifications_disabled\":\"\",\"cross_post_notification_threshold\":1000,\"facebook_connect_prod_app\":true,\"feed_enable_live_streams\":false,\"force_into_pymk_ranking\":false,\"minimum_android_version\":756,\"live_stream_krisp_noise_suppression_enabled\":false,\"enable_transcription_translations\":false,\"ios_post_video_pager_alpha_enabled\":false,\"use_og_image_as_twitter_image_for_post_previews\":false,\"always_use_podcast_channel_art_as_episode_art_in_rss\":false,\"cookie_preference_middleware_enabled\":false,\"seo_tier_override\":\"NONE\",\"no_follow_links\":false,\"publisher_api_enabled\":false,\"zendesk_support_priority\":\"default\",\"enable_post_clips_stats\":false,\"enable_subscriber_referrals_awards\":true,\"ios_profile_themes_feed_permalink_enabled\":false,\"use_publication_language_for_transcription\":false,\"show_substack_funded_gifts_tooltip\":true,\"disable_ai_transcription\":false,\"thread_permalink_preview_min_ios_version\":4192,\"android_toggle_on_website_enabled\":false,\"internal_android_enable_post_editor\":false,\"edit_profile_feed_item\":false,\"updated_inbox_ui\":false,\"web_reader_podcasts_tab\":false,\"use_temporal_thumbnail_selection_workflow\":false,\"live_stream_creation_enabled\":false,\"disable_card_element_in_europe\":false,\"web_growth_item_promotion_threshold\":0,\"enable_web_typing_indicators\":false,\"web_vitals_sample_rate\":0,\"allow_live_stream_auto_takedown\":\"true\",\"search_ranker_variant\":\"control\",\"enable_progressive_speaker_focus_clips\":false,\"post_advanced_search\":\"control\",\"ai_image_generation_enabled\":true,\"disable_personal_substack_initialization\":false,\"section_specific_welcome_pages\":false,\"local_payment_methods\":\"control\",\"enable_linkedin_oauth\":true,\"posts_in_rss_feed\":20,\"post_rec_endpoint\":\"\",\"publisher_dashboard_section_selector\":false,\"reader_surveys_platform_question_order\":\"36,1,4,2,3,5,6,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35\",\"ios_toggle_on_website_enabled\":false,\"login_guard_app_link_in_email\":true,\"community_moderators_enabled\":false,\"monthly_sub_is_one_off\":false,\"unread_notes_activity_digest\":\"control\",\"display_cookie_settings\":false,\"welcome_page_query_params\":false,\"enable_free_podcast_urls\":false,\"comp_expiry_emails_disabled\":false,\"enable_description_on_polls\":false,\"use_microlink_for_instagram_embeds\":false,\"post_notification_batch_delay_ms\":30000,\"free_signup_confirmation_behavior\":\"with_email_validation\",\"ios_post_stats_for_admins\":false,\"live_stream_concurrent_viewer_count_drawer\":false,\"use_livestream_post_media_composition\":true,\"section_specific_preambles\":false,\"android_live_stream_multihost_enabled\":false,\"show_menu_on_posts\":false,\"app_upsell_follow_prompt\":\"control\",\"ios_post_subscribe_web_routing\":true,\"opt_into_all_trending_topics\":false,\"ios_writer_stats_public_launch_v2\":false,\"min_size_for_phishing_check\":1,\"enable_android_post_stats\":false,\"ios_chat_revamp_enabled\":false,\"app_onboarding_survey_email\":false,\"post_notification_batch_chunk_size\":100,\"thefp_enable_pullquote_alignment\":false,\"thefp_enable_pullquote_color\":false,\"republishing_enabled\":false,\"app_mode\":false,\"show_phone_banner\":false,\"live_stream_video_enhancer\":\"internal\",\"minimum_ios_version\":2200,\"enable_author_pages\":false,\"enable_decagon_chat\":true,\"first_month_upsell\":\"control\",\"enable_fedcm\":false,\"new_user_checklist_enabled\":\"use_follower_count\",\"android_enable_auto_gain_control\":false,\"enable_updated_webview_checkout\":false,\"show_attached_profile_for_pub_setting\":false,\"welcome_page_update_desktop_visuals_limited\":\"experiment\",\"rss_verification_code\":\"\",\"notification_post_emails\":\"experiment\",\"ios_profile_subdomain_chips\":true,\"chat_suppress_contributor_push_option_enabled\":false,\"live_stream_invite_ttl_seconds\":600,\"feed_ranking_per_post_clip_cap\":2,\"export_hooks_enabled\":false,\"audio_encoding_bitrate\":null,\"bestseller_pub_override\":false,\"extra_seats_coupon_type\":false,\"post_subdomain_universal_links\":false,\"post_import_max_file_size\":26214400,\"notes_weight_follow\":4,\"enable_post_to_post_link_data_event\":true,\"livekit_reconnect_slate_url\":\"https://mux-livestream-assets.s3.us-east-1.amazonaws.com/custom-disconnect-slate-tall.png\",\"exclude_from_pymk_suggestions\":false,\"publication_ranking_variant\":\"experiment\",\"disable_annual_subscriptions\":false,\"enable_android_dms\":false,\"feed_ranker_use_user_comment_reaction_cache\":true,\"pub_auto_moderation_enabled\":false,\"disable_live_stream_ai_trimming_by_default\":false,\"recipes_enabled\":false,\"disable_deletion\":false,\"ios_default_coupon_enabled\":false,\"notes_weight_read_post\":5,\"notes_weight_reply\":3,\"livekit_egress_custom_base_url\":\"http://livekit-egress-custom-recorder.s3-website-us-east-1.amazonaws.com\",\"clip_focused_video_upload_flow\":false,\"live_stream_max_guest_users\":2,\"enable_video_seo_data\":false,\"can_reimport_unsubscribed_users_with_2x_optin\":false,\"feed_posts_weight_subscribed\":0,\"included_in_demo_feed\":false,\"live_event_mixin\":\"\",\"review_incoming_email\":\"default\",\"app_install_prompts\":\"native_banner_if_supported\",\"enable_founding_gifts\":false,\"ios_chat_uikit\":false,\"enable_sponsorship_campaigns\":false,\"thread_permalink_preview_min_android_version\":2037,\"enable_fp_new_events_page\":true,\"default_thumbnail_time\":10,\"pub_ranking_weight_immediate_engagement\":1,\"pub_ranking_weight_retained_engagement\":1,\"load_test_unichat\":false,\"ios_reader_post_sharing_flow_v2\":\"experiment\",\"notes_read_post_baseline\":0,\"live_stream_head_alignment_guide\":false,\"show_open_post_as_pdf_button\":false,\"free_press_combo_subscribe_flow_enabled\":false,\"restack_with_image\":false,\"free_press_tabbed_subscribe_flow\":\"control\",\"gift_from_substack_modal\":\"experiment\",\"onboarding_badges_android\":\"control\",\"gifts_from_substack_feature_available\":true,\"disable_ai_clips\":false,\"thefp_enable_web_livestream_kicking\":false,\"enable_elevenlabs_voiceovers\":false,\"growth_sources_all_time\":false,\"android_upgrade_alert_dialog\":true,\"headline_testing_enabled\":true,\"translated_notifications_enabled\":false,\"show_simple_post_editor\":false,\"desktop_live_streaming_enabled\":false,\"search_ranker_query_augmentation\":\"enabled\",\"enable_publication_podcasts_page\":false,\"ios_payment_connection_enabled\":true,\"app_install_reminder_email\":\"experiment\",\"use_rotated_recommendations_request\":false,\"thefp_enable_dynamic_toaster\":false,\"thefp_enable_america_250\":true,\"ios_note_composer_settings_enabled\":false,\"android_v2_post_video_player_enabled\":false,\"enable_direct_message_request_bypass\":false,\"enable_apple_news_sync\":false,\"postsById_batch_size\":20,\"free_press_newsletter_promo_enabled\":false,\"enable_ios_livestream_stats\":false,\"disable_live_stream_reactions\":false,\"enable_high_follower_dm\":true,\"ios_welcome_video_profile_prompt\":false,\"clip_generation_3rd_party_vendor\":\"internal\",\"ios_notification_settings_enabled\":false,\"tone_down_sidebar_livestreams\":false,\"notes_weight_negative\":1,\"ios_discover_tab_min_installed_date\":\"2025-06-09T16:56:58+0000\",\"notes_weight_click_see_more\":2,\"enable_publish_youtube_connect_repeat_upsell\":false,\"edit_profile_theme_colors\":false,\"backend_enable_subscription_bar\":true,\"disable_clipping_for_readers\":false,\"android_enable_subscription_bar\":false,\"apple_fee_percent\":15,\"allow_anonymous_personal_pub_creation\":false,\"feed_posts_weight_reply\":3,\"feed_posts_weight_negative\":5,\"feed_posts_weight_like\":1.5,\"feed_posts_weight_share\":3,\"feed_posts_weight_save\":3,\"enable_press_kit_preview_modal\":false,\"dpn_weight_tap_clickbait_penalty\":0.5,\"feed_posts_weight_sign_up\":4,\"live_stream_video_degradation_preference\":\"maintainFramerate\",\"pause_app_badges\":false,\"android_enable_publication_activity_tab\":false,\"thefp_paywall_with_plans\":\"experiment\",\"notes_weight_like\":2,\"profile_feed_expanded_inventory\":false,\"phone_verification_fallback_to_twilio\":false,\"livekit_mux_latency_mode\":\"low\",\"feed_posts_weight_long_click\":1,\"feed_juiced_user\":0,\"vertical_video_player_in_feed_1\":\"experiment\",\"show_branded_intro_setting\":true,\"free_press_single_screen_subscribe_flow_enabled\":false,\"notes_click_see_more_baseline\":0.35,\"android_edit_user_links\":true,\"android_move_feed_tabs\":false,\"ios_inline_replies\":\"control\",\"android_enable_user_status_ui\":false,\"use_advanced_commerce_api_for_iap\":false,\"skip_free_preview_language_in_podcast_notes\":false,\"larger_wordmark_on_publication_homepage\":false,\"video_editor_full_screen\":false,\"enable_mobile_stats_for_admins\":false,\"ios_profile_themes_note_composer_enabled\":false,\"reduce_post_search_fuzziness\":\"treatment\",\"related_posts_web\":\"experiment\",\"notes_weight_click_item\":3,\"notes_weight_long_visit\":1,\"ios_note_sharing_assets\":\"control\",\"bypass_single_unlock_token_limit\":false,\"notes_watch_video_baseline\":0.08,\"add_section_and_tag_metadata\":false,\"daily_promoted_notes_enabled\":true,\"feed_ranker_use_user_feed_restack_comment_cache\":true,\"enable_islands_cms\":false,\"enable_livestream_combined_stats\":false,\"ios_social_subgroups_enabled\":false,\"ios_notification_preference_onboarding_copy\":\"control\",\"enable_drip_campaigns\":false,\"ios_offline_mode_enabled\":false,\"post_management_search_engine\":\"elasticsearch\",\"new_bestseller_leaderboard_feed_item_enabled\":false,\"feed_main_disabled\":false,\"enable_account_settings_revamp\":false,\"allowed_email_domains\":\"one\",\"thefp_enable_fp_recirc_block\":false,\"ios_web_subscription_payments\":\"experiment\",\"ios_full_search_results\":\"control\",\"enable_debug_logs_ios\":false,\"show_pub_content_on_profile_for_pub_id\":0,\"web_badge_popover_treatment\":\"lottie\",\"show_pub_content_on_profile\":false,\"livekit_track_egress\":true,\"video_tab_mixture_pattern\":\"npnnnn\",\"enable_theme_contexts\":false,\"onboarding_suggestions_search\":\"experiment\",\"feed_tuner_enabled\":false,\"livekit_mux_latency_mode_rtmp\":\"low\",\"notes_weight_follow_boost\":3,\"thefp_homepage_portrait_layout\":false,\"ios_post_bottom_share\":\"experiment\",\"fcm_high_priority\":false,\"android_enable_unified_composer\":\"control\",\"ios_user_status_sheet_subscribe_button\":\"experiment\",\"subscription_bar_top_selection_strategy_v2\":\"destination_wau_pub_score\",\"search_ranker_load_test_pct\":0,\"iap_announcement_blog_url\":\"\",\"android_onboarding_progress_persistence\":\"control\",\"use_theme_editor_v2\":false,\"dpn_weight_tap_bonus_subscribed\":3,\"thefp_email_paywall_with_plans\":\"experiment\",\"ios_custom_buttons_enabled\":true,\"ios_livestream_feedback\":false,\"founding_plan_upgrade_warning\":false,\"suggested_search_ranking_v1\":\"control\",\"dpn_weight_like\":3,\"ios_iap_opt_out_enabled\":false,\"skip_kafka_retry_messages\":false,\"related_notes_variations\":\"control\",\"android_view_post_share_assets_employees_only\":false,\"thefp_show_fixed_footer_paywall\":false,\"android_subscription_queue_experiment\":\"experiment\",\"ios_viral_gift_entry_points\":\"treatment\",\"ios_post_video_pager_enabled_v2\":\"experiment\",\"render_high_quality_clips\":true,\"ios_subscription_pogs\":\"experiment\",\"use_elasticsearch_for_category_tabs\":\"control\",\"dpn_weight_reply\":2,\"age_verification_uk_rollout_percentage\":0,\"android_enable_edit_profile_theme\":false,\"android_enable_view_profile_theme\":false,\"enable_refresh_token_deduplication\":true,\"dpn_weight_follow\":3,\"live_stream_audio_enhancer_v2\":\"auphonic\",\"age_verification_uk_enabled\":false,\"enable_speaker_focus_clips\":true,\"search_ranker_load_test_ranking_window\":2000,\"ios_new_post_sharing_flow_enabled\":false,\"ignore_video_in_notes_length_limit\":false,\"web_show_scores_on_sports_tab\":false,\"notes_weight_click_share\":3,\"direct_device_push_notifications\":false,\"allow_long_videos\":true,\"enable_livestream_rtmp_invites\":true,\"dpn_score_threshold\":0,\"thefp_enable_follow_module\":false,\"publication_user_invite_tier_2_only\":false,\"dpn_weight_follow_bonus\":0.5,\"ios_post_subscribe_follow_related\":\"control\",\"use_intro_clip_and_branded_intro_by_default\":false,\"community_profile_activity_feed\":false,\"new_user_subscribe_follow_prompt_override\":\"none\",\"ios_subscription_pogs_new_users\":\"experiment\",\"ios_subscription_pogs_old_users\":\"experiment\",\"android_subscription_queue_experiment_2\":\"experiment\",\"enable_viewing_all_livestream_viewers\":false,\"tabbed_notes_search\":\"control\",\"enable_clip_prompt_variant_filtering\":true,\"dpn_ranking_enabled\":true,\"sequential_retrieval_model_pct\":100,\"android_vertical_post_player_3\":\"experiment\",\"dpn_model_variant\":\"experiment\",\"add_byline_by_user_id_tier_2_only\":false,\"enable_outlier_subscriber_activity_item\":true,\"android_reader_share_assets_3\":\"control\",\"enable_apple_podcast_auto_publish\":false,\"dpn_weight_disable\":10,\"enable_dashboard_data_callout\":true,\"speaker_focus_hls_usage_enabled\":true,\"ios_reader_post_sharing_flow\":\"control\",\"dpn_weight_open\":2.5,\"dpn_suggested_content_title\":\"control\",\"dpn_weight_long_session\":1.5,\"live_stream_in_trending_topic_overrides\":\"\",\"android_vertical_post_player\":\"control\",\"enable_notes_admins\":false,\"ios_post_embed_card_enabled\":true,\"enable_suggested_searches\":true,\"saved_post_reactivation_push_notification\":\"experiment\",\"android_synchronous_push_notif_handling\":\"control\",\"disable_user_status_for_user\":false,\"a24_redemption_link\":\"\",\"podcast_subscribe_flow_app_upsell\":\"treatment\",\"trending_topics_module_long_term_experiment\":\"experiment\",\"share_asset_ordering\":\"control\",\"suggested_search_metadata_web_market_ui\":false,\"dpn_weight_restack\":2,\"dpn_weight_negative\":40,\"search_retrieval_variant\":\"control\",\"session_version_invalidation_enabled\":false,\"publisher_banner\":\"\",\"ios_enable_subscription_stories\":false,\"direct_device_push_notifications_ios\":\"experiment\",\"forced_featured_topic_id\":\"\",\"android_vertical_post_player_2\":\"control\",\"web_notes_trending_topics_enabled\":\"control\",\"dpn_weight_tap\":5,\"ios_live_stream_auto_gain_enabled\":false,\"client_support_for_image_quote_feed_item_type\":false,\"get_app_pill_welcome_page_v2\":\"experiment\",\"android_rank_share_destinations_experiment\":\"experiment\",\"android_note_share_assets\":\"control\",\"feed_permalink_referred_modal\":\"experiment\",\"ios_live_stream_pip_dismiss_v2\":\"control\",\"web_post_reading_share_nudge\":\"experiment\"},\"publicationSettings\":null,\"publicationUserSettings\":null,\"userSettings\":{\"user_id\":null,\"activity_likes_enabled\":true,\"artist_mode_enabled\":false,\"dashboard_nav_refresh_enabled\":false,\"hasDismissedSectionToNewsletterRename\":false,\"is_guest_post_enabled\":true,\"feed_web_nux_seen_at\":null,\"has_seen_select_to_restack_tooltip_nux\":false,\"invite_friends_nux_dismissed_at\":null,\"suggestions_feed_item_last_shown_at\":null,\"has_seen_select_to_restack_modal\":false,\"last_home_tab\":null,\"last_notification_alert_shown_at\":null,\"disable_reply_hiding\":false,\"newest_seen_chat_item_published_at\":null,\"explicitContentEnabled\":false,\"contactMatchingEnabled\":false,\"messageRequestLevel\":\"everyone\",\"liveStreamAcceptableInviteLevel\":\"everyone\",\"liveStreamAcceptableChatLevel\":\"everyone\",\"creditTokensTreatmentExposed\":false,\"appBadgeIncludesChat\":false,\"autoPlayVideo\":true,\"smart_delivery_enabled\":false,\"chatbotTermsLastAcceptedAt\":null,\"has_seen_notes_post_app_upsell\":false,\"substack_summer_nux_dismissed_at\":null,\"first_note_id\":null,\"show_concurrent_live_stream_viewers\":false,\"has_dismissed_fp_download_pdf_nux\":false,\"edit_profile_feed_item_dismissed_at\":null,\"mobile_permalink_app_upsell_seen_at\":null,\"new_user_checklist_enabled\":false,\"new_user_follow_subscribe_prompt_dismissed_at\":null,\"has_seen_youtube_shorts_auto_publish_announcement\":false,\"has_seen_publish_youtube_connect_upsell\":false,\"notificationQualityFilterEnabled\":true,\"hasSeenOnboardingNewslettersScreen\":false},\"subscriberCountDetails\":null,\"mux_env_key\":\"u42pci814i6011qg3segrcpp9\",\"sentry_environment\":\"production\",\"launchWelcomePage\":false,\"pendingInviteForActiveLiveStream\":null,\"abTestingId\":\"34b08fc5-9901-4b97-9b5c-aaaa0f45f14e\",\"hideHeader\":true,\"hideFooter\":true,\"useDarkMode\":true,\"features\":{},\"browser\":{\"name\":\"Chrome\",\"version\":\"120.0.0.0\",\"major\":\"120\"},\"showCookieBanner\":false,\"disabledCookies\":[],\"dd_env\":\"prod\",\"dd_ti\":true}")</script>
        <script>window._analyticsConfig = JSON.parse("{\"properties\":{\"subdomain\":null,\"has_plans\":false,\"pub_community_enabled\":false,\"is_personal_publication\":false,\"is_subscribed\":false,\"is_free_subscribed\":false,\"is_author\":false,\"is_contributor\":false,\"is_admin\":false,\"is_founding\":false},\"localGaPixelId\":\"G-TLW0DF6G5V\",\"adwordsAccountId\":\"AW-316245675\",\"adwordsEventSendTo\":\"Tf76CKqcyL4DEKuN5pYB\"}")</script>

        
        
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2083.9fe47401.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/598.e0f754b2.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6253.7ef90e5b.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1907.06314015.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5725.f89fa29e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/400.f58ec134.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2658.434e8abb.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3793.7670135c.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9305.4446497f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7562.fa3052cd.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3897.43992966.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2536.4ec76a84.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1729.d1dd27aa.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9063.b9bb8752.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5698.5cde0d38.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8497.c62520ac.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/689.c7c10643.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2263.bf3789ec.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2516.387658de.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/697.e6987c29.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1526.5380ec12.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/870.a3d1ad2d.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7201.1a57f066.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8402.70f37023.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3327.7d38ec3f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3456.9fec4e8c.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4601.cce8465b.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8700.e4d3b887.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7675.48136919.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1172.482f9195.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4840.7a4b31fc.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1800.8dfa859c.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6068.dad48f36.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6683.3fe2570e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4086.c87308fe.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4218.89d4d69e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1877.184c4a6f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2861.4fe3fda3.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3925.53178e34.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8779.efcab556.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8978.ddea496d.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8547.4880f2ac.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1462.624979db.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7806.7be556f1.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4056.a51cc600.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4108.2ed748c9.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8237.c16e8b02.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7353.f2175d70.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8771.35268a8e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5664.5941cd89.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7582.061a040f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3713.621f4255.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7880.068214df.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9830.d732da60.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7588.470e542f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2455.112e7f81.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6820.561b7a39.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5197.3ba7cb63.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9966.981fdbbe.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3672.4791d66e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3715.4f2d02ab.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6331.423f4dd0.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/66.8b8b8b55.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5688.a55da2f2.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/195.14116008.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/184.9c596e46.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1749.d7ec76d0.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8511.d0e67f2d.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4347.83001e70.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8782.b02a6b78.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7800.188f3a15.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7147.1ade4a0e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1219.37ad75aa.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1638.9963990f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8096.000ae3c7.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1601.df88a9e3.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3948.794d0cd5.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4975.2fb4cb82.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/55.b1b85ee5.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7953.e053d699.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6449.d065ba11.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/reader2.cf908c13.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4347.83001e70.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1749.d7ec76d0.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/184.9c596e46.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9713.e20f466f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/195.14116008.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7222.75062350.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5688.a55da2f2.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6331.423f4dd0.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9966.981fdbbe.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5197.3ba7cb63.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6820.561b7a39.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2455.112e7f81.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7880.068214df.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3713.621f4255.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5664.5941cd89.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8237.c16e8b02.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4108.2ed748c9.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8547.4880f2ac.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8779.efcab556.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2861.4fe3fda3.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1877.184c4a6f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4086.c87308fe.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6068.dad48f36.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1800.8dfa859c.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/4840.7a4b31fc.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7675.48136919.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3327.7d38ec3f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7201.1a57f066.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1830.c43016d7.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1526.5380ec12.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/271.cb27bcc4.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2516.387658de.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2263.bf3789ec.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6869.3db4edc8.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8497.c62520ac.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5698.5cde0d38.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7523.a8d4dc49.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1729.d1dd27aa.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/2536.4ec76a84.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3897.43992966.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3793.7670135c.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/400.f58ec134.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7957.817ce496.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/5725.f89fa29e.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1907.06314015.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/6253.7ef90e5b.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/598.e0f754b2.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9137.17769749.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/9935.32ea3b54.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/185.520016a8.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/3948.794d0cd5.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1837.4d1d7fc8.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/8096.000ae3c7.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/1638.9963990f.js" charset="utf-8"></script>
            
                <script defer="" type="module" src="https://substackcdn.com/bundle/static/js/7800.188f3a15.js" charset="utf-8"></script>
            
        
        <script nomodule="">
            (function() {
                var message = 'Your browser does not support modern JavaScript modules. Please upgrade your browser for the best experience.';
                var warningDiv = document.createElement('div');
                warningDiv.style.color = 'red';
                warningDiv.style.padding = '10px';
                warningDiv.style.margin = '10px 0';
                warningDiv.style.border = '1px solid red';
                warningDiv.style.backgroundColor = 'lightyellow';
                warningDiv.innerText = message;
                document.body.prepend(warningDiv);
            })();
        </script>

        
            <!-- Datadog Analytics -->
            <script>
              (function(h,o,u,n,d) {
                h=h[d]=h[d]||{q:[],onReady:function(c){h.q.push(c)}}
                d=o.createElement(u);d.async=1;d.src=n
                n=o.getElementsByTagName(u)[0];n.parentNode.insertBefore(d,n)
              })(window,document,'script','https://www.datadoghq-browser-agent.com/us1/v5/datadog-rum.js','DD_RUM')
              window.DD_RUM.onReady(function() {
                window.DD_RUM.init({
                  clientToken: 'puba71073f072643721169b68f352438710',
                  applicationId: '2e321b35-c76b-4073-8d04-cc9a10461793',
                  site: 'datadoghq.com',
                  service: 'substack-web',
                  env: window._preloads.dd_env,
                  version: 'd14c50c6ee04f127a4b5d4cf2403b9c95d541740',
                  sessionSampleRate: 1,
                  sessionReplaySampleRate: 100,
                  trackUserInteractions: window._preloads.dd_ti,
                  trackResources: true,
                  trackLongTasks: true,
                  defaultPrivacyLevel: 'mask-user-input',
                  allowedTracingUrls: [/https?:\/\/(.+\/.)?substack(cdn)?\.com/]
                });
              })
            </script>
            <!-- End Datadog Analytics -->

            <!-- Cloudflare Web Analytics -->
            <script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;216309cffb464db4b0e02daf0b8e8060&quot;}"></script>
            <!-- End Cloudflare Web Analytics -->
        

        <!-- Fallback tracking pixels -->
        

        

        <noscript>
    <style>
        #nojs-banner {
            position: fixed;
            bottom: 0;
            left: 0;
            padding: 16px 16px 16px 32px;
            width: 100%;
            box-sizing: border-box;
            background: red;
            color: white;
            font-family: -apple-system, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            font-size: 13px;
            line-height: 13px;
        }
        #nojs-banner a {
            color: inherit;
            text-decoration: underline;
        }
    </style>

    <div id="nojs-banner">
        This site requires JavaScript to run correctly. Please <a href="https://enable-javascript.com/" target="_blank">turn on JavaScript</a> or unblock scripts
    </div>
</noscript>


        

        

        
        
    <script>(function(){function c(){var b=a.contentDocument||a.contentWindow.document;if(b){var d=b.createElement('script');d.innerHTML="window.__CF$cv$params={r:'9984d0050cc9901b',t:'MTc2MjA5ODg4OC4wMDAwMDA='};var a=document.createElement('script');a.nonce='';a.src='/cdn-cgi/challenge-platform/scripts/jsd/main.js';document.getElementsByTagName('head')[0].appendChild(a);";b.getElementsByTagName('head')[0].appendChild(d)}}if(document.body){var a=document.createElement('iframe');a.height=1;a.width=1;a.style.position='absolute';a.style.top=0;a.style.left=0;a.style.border='none';a.style.visibility='hidden';document.body.appendChild(a);if('loading'!==document.readyState)c();else if(window.addEventListener)document.addEventListener('DOMContentLoaded',c);else{var e=document.onreadystatechange||function(){};document.onreadystatechange=function(b){e(b);'loading'!==document.readyState&&(document.onreadystatechange=e,c())}}}})();</script><iframe height="1" width="1" style="position: absolute; top: 0px; left: 0px; border: none; visibility: hidden;"></iframe>

<div id="P0-2" data-floating-ui-portal=""></div><div id="P0-5" data-floating-ui-portal=""></div><div id="P0-8" data-floating-ui-portal=""></div><div id="P0-11" data-floating-ui-portal=""></div><div id="P0-14" data-floating-ui-portal=""></div><div id="P0-17" data-floating-ui-portal=""></div><iframe height="0" width="0" style="display: none; visibility: hidden;"></iframe><script type="text/javascript" async="" src="https://googleads.g.doubleclick.net/pagead/viewthroughconversion/316245675/?random=1762098890012&amp;cv=11&amp;fst=1762098890012&amp;bg=ffffff&amp;guid=ON&amp;async=1&amp;en=gtag.config&amp;gtm=45be5at1v887153041za200zb880777354zd880777354xec&amp;gcd=13l3l3l3l1l1&amp;dma=0&amp;tag_exp=101509157~103116026~103200004~103233427~104527907~104528501~104573694~104684208~104684211~104948813~105322303~115480709~115583767~115938466~115938469~116217636~116217638~116253087~116253089~116254370&amp;u_w=800&amp;u_h=600&amp;url=https%3A%2F%2Fsubstack.com%2Fbrowse%2Fpolitics&amp;frm=0&amp;tiba=Explore%20%7C%20Substack&amp;hn=www.googleadservices.com&amp;npa=0&amp;auid=2095092980.1762098890&amp;uaa=&amp;uab=&amp;uafvl=&amp;uamb=0&amp;uam=&amp;uap=&amp;uapv=&amp;uaw=0&amp;data=event%3Dgtag.config&amp;rfmt=3&amp;fmt=4"></script></body></html>
</file>

<file path="docs/ARCHITECTURE.md">
# Newsletter Control Center – Production Architecture v2.0

## System Overview

The Newsletter Control Center is a production-grade newsletter intelligence platform that ingests content from two Gmail accounts, processes it through chunking and embedding pipelines, and provides RAG-powered search capabilities. The system uses Gmail History API for efficient incremental updates, content-hash deduplication to handle cross-inbox duplicates, and a robust checkpoint system for failure recovery.

## Data Flow

```mermaid

flowchart LR

  subgraph Sources

    A1[Gmail: johnsfnewsletters@gmail.com]

    A2[Gmail: nsm@internationalintrigue.io]

    noteA[[Labels in both inboxes:<br/>• Paid $ (read-only)<br/>• Ingested (visual-only, app-applied post-success)]]:::note

  end

  subgraph Jobs

    B1[Cloud Run Job: ncc-ingest-gmail\n• Read last_history_id (per inbox)\n• Gmail History API deltas\n• Fallback: in:anywhere newer_than:30d\n• Skip if gmail_id exists in DB\n• Skip heavy work if content_hash seen (cross-inbox)\n• Write raw + parts + labels + is_paid\n• Update last_history_id; JSON logs]

    B2[Cloud Run Job: ncc-process-chunk-embed\n• PARSE → PUBLISHER → CHUNK → EMBED with checkpoints\n• Chunk ~1200 / overlap ~200\n• Vertex text-embedding-004 (768d)\n• On DONE: apply label "Ingested" (idempotent)]

  end

  subgraph BigQuery (ncc_production)

    C1[(control.ingest_state\ninbox, last_history_id, last_success_at)]

    C2[(control.processing_status\ngmail_message_id, stage, error, updated_at)]

    C3[(core.raw_emails\n+ content_hash, list_id, reply_to, is_paid)]

    C4[(core.email_labels\nor labels[])]

    C5[(core.publishers\nUNIQUE(service,site_id))]

    C6[(core.publisher_aliases)]

    C7[(core.chunks\n+ char_start, char_end)]

    C8[(core.chunk_embeddings\nmodel, dim=768, embedding)]

  end

  subgraph Retrieval & AI (later)

    D1[BigQuery Vector Search]

    D2[LLM Answering Layer\n(strict citations, cost logging)]

  end

  A1 --> B1

  A2 --> B1

  B1 --> C1

  B1 --> C3

  B1 --> C4

  B1 --> C2

  B2 --> C7

  B2 --> C8

  C7 --> D1

  C8 --> D1

  D1 --> D2

  classDef note fill:#eef,stroke:#99f,color:#222;

```

## Publisher Canonicalization

```mermaid

flowchart TD

  H[List-ID host?] -->|yes| P1[service=substack, site_id=<subdomain>.substack.com]

  H -->|no| R[Reply-To host recognizable?] -->|yes| P2[service=<svc>, site_id=<host>]

  R -->|no| L[Primary canonical link host?] -->|yes| P3[service=custom, site_id=<host>]

  L -->|no| F[From root domain] --> P4[service=custom, site_id=<root>]

  P1 --> U[UNIQUE(service, site_id)]

  P2 --> U

  P3 --> U

  P4 --> U

```

## Tables

### Control Tables (control.*)

- **ingest_state**: Tracks last Gmail History ID per inbox for incremental sync.
- **processing_status**: Tracks each message through stages (PARSED → PUBLISHERED → CHUNKED → EMBEDDED → DONE) with error logging and resume.

### Core Tables (core.*)

- **raw_emails**: Original email content + content_hash for dedupe, list_id/reply_to for service detection, is_paid from Gmail labels.
- **email_labels**: Normalized label storage (IDs + names) for filtering and categorization (or store labels[] on raw_emails).
- **publishers**: Canonical registry with UNIQUE(service, site_id) preventing duplicates.
- **publisher_aliases**: Optional mapping of variations → canonical publishers for merges.
- **chunks**: Deterministic chunks (~1200 chars, ~200 overlap) with char_start/char_end.
- **chunk_embeddings**: 768-dim vectors from Vertex AI text-embedding-004 with model + dim.

## Migration Path from v1 to v2

### Phase 1: Non-Breaking Additions

- Create control.* tables alongside existing tables.
- Add content_hash to existing messages.
- Add service and site_id to publishers.
- Run the new pipeline in parallel for validation.

### Phase 2: Cutover

- Switch from date-based scanning to Gmail History API.
- Enable content_hash deduplication (cross-inbox).
- Implement new publisher canonicalization logic.
- Switch to control.processing_status for state management.

### Phase 3: Cleanup

- Archive the old discovery system.
- Remove duplicate publishers via UNIQUE(service, site_id) (plus optional aliases).
- Drop deprecated columns/tables.
- Archive test/experimental scripts.

## Runbook

See [docs/RUNBOOK.md](./RUNBOOK.md) for operational procedures.
</file>

<file path="docs/RUNBOOK.md">
# Production Health Check Runbook

## Overview

The `/healthz` endpoint provides a production health check for the newsletter control center pipeline. It monitors:

1. **Job Execution Times**: Last successful runs for 4 critical jobs:
   - `ncc-ingest-me`
   - `ncc-ingest-other`
   - `ncc-chunks`
   - `ncc-embeddings`

2. **Pipeline Coverage**: Last 24h statistics:
   - Raw emails ingested
   - Emails chunked
   - Chunks created
   - Chunks embedded
   - Chunk coverage percentage
   - Embedding coverage percentage

## Health Check Logic

### Success Criteria (200 OK)

- ✅ All 4 jobs succeeded within last 120 minutes
- ✅ Last 24h chunk coverage == 100%
- ✅ Last 24h embedding coverage == 100%

### Failure Criteria (500 Internal Server Error)

- ❌ Any job hasn't succeeded in last 120 minutes
- ❌ Chunk coverage < 100%
- ❌ Embedding coverage < 100%

## Endpoint

**URL**: `https://ncc-jobs-runner-d6cqllgv7a-uc.a.run.app/health-check`

**Note**: The `/healthz` path is reserved by Cloud Run. We use `/health-check` as the production endpoint.

**Method**: `GET`

**Response Format**:
```json
{
  "ok": true,
  "details": {
    "jobs": [
      {
        "job": "ncc-ingest-me",
        "lastSuccessTime": "2025-11-05T18:27:30.642Z",
        "status": "success"
      },
      ...
    ],
    "reconcile": {
      "rawEmails": 51,
      "emailsChunked": 10,
      "chunks": 119,
      "chunksEmbedded": 119,
      "chunkCoverage": 19.61,
      "embeddingCoverage": 100.0
    }
  }
}
```

## Common Scenarios

### 🟢 Green (All Healthy)

**Symptoms**:
- Status: 200 OK
- `ok: true`
- All jobs show `status: "success"`
- Coverage at 100%

**Action**: None required.

### 🟡 Yellow (Partial Degradation)

**Symptoms**:
- Status: 500
- `ok: false`
- One or more jobs `status: "stale"` or `status: "missing"`
- Coverage still 100%

**Common Causes**:
- Scheduled job missed its run window
- Job execution failed silently
- Network/timeout issues

**Diagnosis**:
```bash
# Check job execution logs
gcloud run jobs executions list --job=ncc-ingest-me --region=us-central1 --project=newsletter-control-center --limit=5

# View recent logs
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=ncc-ingest-me" --limit=100 --project=newsletter-control-center --freshness=1h

# Check scheduler status
gcloud scheduler jobs describe schedule-ncc-ingest-me-0710 --location=us-central1 --project=newsletter-control-center
```

**Remediation**:
```bash
# Manually trigger the job
gcloud run jobs execute ncc-ingest-me --region=us-central1 --project=newsletter-control-center

# Or use the runner API
curl -X POST https://ncc-jobs-runner-d6cqllgv7a-uc.a.run.app/run \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type: application/json" \
  -d '{"job": "ncc-chunks"}'
```

### 🔴 Red (Pipeline Degradation)

**Symptoms**:
- Status: 500
- `ok: false`
- `chunkCoverage < 100%` OR `embeddingCoverage < 100%`

**Common Causes**:
- Chunking job not processing all emails
- Embedding job lagging behind
- BigQuery write failures
- Data quality issues

**Diagnosis**:
```bash
# Check reconcile report
npm run report:reconcile

# Check recent chunk creation
bq query --use_legacy_sql=false "
  SELECT COUNT(*) as chunks_created
  FROM \`newsletter-control-center.ncc_production.chunks\`
  WHERE gmail_message_id IN (
    SELECT gmail_message_id
    FROM \`newsletter-control-center.ncc_production.raw_emails\`
    WHERE ingested_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
  )
"

# Check embedding status
bq query --use_legacy_sql=false "
  SELECT COUNT(*) as embedded
  FROM \`newsletter-control-center.ncc_production.chunk_embeddings\`
  WHERE chunk_id IN (
    SELECT chunk_id
    FROM \`newsletter-control-center.ncc_production.chunks\`
    WHERE gmail_message_id IN (
      SELECT gmail_message_id
      FROM \`newsletter-control-center.ncc_production.raw_emails\`
      WHERE ingested_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
    )
  )
"
```

**Remediation**:
```bash
# Manually trigger chunking
gcloud run jobs execute ncc-chunks --region=us-central1 --project=newsletter-control-center

# Manually trigger embeddings
gcloud run jobs execute ncc-embeddings --region=us-central1 --project=newsletter-control-center

# Check for errors in logs
gcloud logging read "resource.type=cloud_run_job AND severity>=ERROR" --limit=50 --project=newsletter-control-center --freshness=1h
```

## Monitoring & Alerts

### Notification Channels

Notification channels are created in the Cloud Console, not by scripts:

1. Go to **Monitoring > Alerting > Notification Channels**
2. Click **"Add New"** and select **"Email"**
3. Enter the email address: `john@internationalintrigue.io`
4. Set display name: `Email alerts` (or use `MONITORING_EMAIL_CHANNEL_NAME` env var)

**Environment Variables**:
- `MONITORING_EMAIL_CHANNEL_NAME`: Default `"Email alerts"` - used to find channel by display name
- `MONITORING_EMAIL_CHANNEL_ID`: Override - full resource name or channel ID (required if listing channels fails due to permissions)

### Uptime Check

- **Name**: `ncc-health-check`
- **Type**: HTTPS
- **Frequency**: Every 1 minute
- **Endpoint**: `/health-check`

**Note**: Uptime Checks can only hit public endpoints. To enable `/health-check` for Uptime Checks, set `RUNNER_HEALTH_PUBLIC=true` before deploying. **Warning**: This makes the entire Cloud Run service publicly accessible (IAM is service-level, not route-level). The service will still require authentication for `/run` and other endpoints via application logic, but the service itself will be publicly invokable.

### Alert Policy

- **Name**: `NCC Health Alert`
- **Condition**: Uptime check fails 2 of 2 times within 10 minutes
- **Notification**: Email channel (configured in Console)
- **Auto-close**: 30 minutes after resolution

### Managing Alerts

**View Alert Policy**:
```bash
gcloud monitoring policies list --project=newsletter-control-center --filter="displayName:NCC Health Alert"
```

**Temporarily Disable Alert**:
```bash
# Get policy name
POLICY_NAME=$(gcloud monitoring policies list --project=newsletter-control-center --format="value(name)" --filter="displayName:NCC Health Alert")

# Disable
gcloud monitoring policies update $POLICY_NAME --project=newsletter-control-center --no-enabled

# Re-enable later
gcloud monitoring policies update $POLICY_NAME --project=newsletter-control-center --enabled
```

### Setup & Verification

**Preview alert setup**:
```bash
MONITORING_EMAIL_CHANNEL_NAME="Email alerts" npm run ops:alert:plan
```

**Apply alert setup** (requires channel exists in Console):
```bash
MONITORING_EMAIL_CHANNEL_NAME="Email alerts" npm run ops:alert:apply
```

**Verify health endpoint**:
```bash
ts-node scripts/ops/verify-health.ts
# Expected output: HEALTH OK (exit 0)
```

**Full verification sequence**:
```bash
# 1. Verify health endpoint
ts-node scripts/ops/verify-health.ts

# 2. Preview alert changes
MONITORING_EMAIL_CHANNEL_NAME="Email alerts" npm run ops:alert:plan

# 3. Apply alert setup
MONITORING_EMAIL_CHANNEL_NAME="Email alerts" npm run ops:alert:apply

# 4. Snapshot deployment state
npm run cloud:snapshot
```

**Silence for Maintenance**:
1. Disable the alert policy (see above)
2. Or update the health check to return 200 during maintenance (not recommended)

## Local Testing

**Test locally**:
```bash
npm run dev
# In another terminal:
curl http://localhost:8080/health-check
```

**Verify production endpoint**:
```bash
ts-node scripts/ops/verify-health.ts
```

**Mock health check** (for development):
The health check will work locally but will query actual Cloud Run jobs and BigQuery. For testing without real data, you can modify `src/ops/health.ts` temporarily.

## Troubleshooting

### Health Check Returns 500 but Jobs Look Fine

1. Check if jobs are using the correct service account
2. Verify BigQuery permissions
3. Check for timezone issues (jobs use UTC timestamps)

### Health Check Times Out

1. Check BigQuery query performance
2. Verify network connectivity
3. Check Cloud Run service logs:
   ```bash
   gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=ncc-jobs-runner" --limit=50 --project=newsletter-control-center
   ```

### False Positives

If chunk/embedding coverage is legitimately < 100% (e.g., during initial ingestion), you may need to:
1. Adjust the health check thresholds
2. Add a grace period for new data
3. Exclude specific time windows

## Related Documentation

- [Cloud Run Jobs Documentation](https://cloud.google.com/run/docs/creating-jobs)
- [Cloud Monitoring Uptime Checks](https://cloud.google.com/monitoring/uptime-checks)
- [Cloud Monitoring Alert Policies](https://cloud.google.com/monitoring/alerts)
</file>

<file path="newsletter-search/public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="newsletter-search/public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="newsletter-search/public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="newsletter-search/public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="newsletter-search/public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path="newsletter-search/src/app/api/newsletter/[id]/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { BigQuery } from '@google-cloud/bigquery';

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

const bigquery = new BigQuery({ projectId: PROJECT_ID });

export async function GET(
  request: NextRequest,
  { params }: { params: Promise<{ id: string }> }
) {
  try {
    const { id } = await params;
    
    const sqlQuery = `
      SELECT 
        id,
        sender,
        subject,
        sent_date,
        received_date,
        body_text,
        body_html,
        is_vip,
        publisher_name,
        source_type,
        word_count,
        has_attachments
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      WHERE id = '${id.replace(/'/g, "''")}'
      LIMIT 1
    `;

    const [rows] = await bigquery.query(sqlQuery);
    
    if (rows.length === 0) {
      return NextResponse.json(
        { error: 'Newsletter not found' },
        { status: 404 }
      );
    }

    return NextResponse.json(rows[0]);

  } catch (error) {
    console.error('Newsletter fetch error:', error);
    return NextResponse.json(
      { error: 'Failed to fetch newsletter', details: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}
</file>

<file path="newsletter-search/src/app/newsletter/[id]/page.tsx">
'use client';

import { useState, useEffect } from 'react';
import Link from 'next/link';
import { getBestCleanedContent } from '@/lib/newsletter-cleaning';

interface Newsletter {
  id: string;
  sender: string;
  subject: string;
  sent_date: any; // BigQueryTimestamp object
  received_date: any; // BigQueryTimestamp object
  body_text: string;
  body_html: string | null;
  is_vip: boolean;
  publisher_name: string;
  source_type: string;
  word_count: number;
  has_attachments: boolean;
}

export default function NewsletterDetail({ params }: { params: Promise<{ id: string }> }) {
  const [newsletter, setNewsletter] = useState<Newsletter | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState('');
  const [searchQuery, setSearchQuery] = useState<string>('');

  useEffect(() => {
    const fetchNewsletter = async () => {
      try {
        const { id } = await params;
        const response = await fetch(`/api/newsletter/${id}`);
        const data = await response.json();
        
        if (!response.ok) {
          throw new Error(data.error || 'Failed to fetch newsletter');
        }
        
        setNewsletter(data);
      } catch (err) {
        setError(err instanceof Error ? err.message : 'Failed to fetch newsletter');
      } finally {
        setLoading(false);
      }
    };

    fetchNewsletter();
  }, [params]);

  useEffect(() => {
    const urlParams = new URLSearchParams(window.location.search);
    const query = urlParams.get('q') || '';
    setSearchQuery(query);
  }, []);

  const formatDate = (dateInput: any) => {
    // Case 1: NULL values
    if (!dateInput) return 'N/A';
    
    // Case 2: BigQueryTimestamp objects (what BigQuery actually returns)
    if (dateInput && typeof dateInput === 'object' && dateInput.value) {
      try {
        const date = new Date(dateInput.value);
        if (isNaN(date.getTime())) {
          return 'Invalid Date';
        }
        return date.toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'long',
          day: 'numeric'
        });
      } catch (error) {
        return 'Invalid Date';
      }
    }
    
    // Case 3: Date OBJECTS
    if (dateInput instanceof Date) {
      if (isNaN(dateInput.getTime())) {
        return 'Invalid Date';
      }
      return dateInput.toLocaleDateString('en-US', {
        year: 'numeric',
        month: 'long',
        day: 'numeric'
      });
    }
    
    // Case 4: String dates
    if (typeof dateInput === 'string') {
      let date: Date;
      if (dateInput.includes('T')) {
        // ISO format with time
        date = new Date(dateInput);
      } else if (dateInput.includes('-')) {
        // Date only format (YYYY-MM-DD)
        date = new Date(dateInput + 'T00:00:00');
      } else {
        // Try parsing as-is
        date = new Date(dateInput);
      }
      
      if (isNaN(date.getTime())) {
        return 'Invalid Date';
      }
      
      return date.toLocaleDateString('en-US', {
        year: 'numeric',
        month: 'long',
        day: 'numeric'
      });
    }
    
    // Fallback for unexpected types
    return 'Invalid Date';
  };

  const calculateReadTime = (wordCount: number) => {
    const wordsPerMinute = 200;
    const minutes = Math.ceil(wordCount / wordsPerMinute);
    return minutes;
  };

  const formatContent = (content: string, searchQuery?: string) => {
    if (!content) return [];
    
    // Content is already cleaned when we get it
    const cleanedContent = content;
    
    // Split into paragraphs based on double line breaks
    const paragraphs = cleanedContent
      .split(/\n\s*\n/)
      .map(p => p.trim())
      .filter(p => p.length > 0);
    
    // Highlight search terms if provided
    if (searchQuery && searchQuery.trim()) {
      const query = searchQuery.trim();
      const regex = new RegExp(`(${query.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')})`, 'gi');
      
      return paragraphs.map(paragraph => {
        const sentences = paragraph.split(/(?<=[.!?])\s+/);
        return sentences.map(sentence => {
          if (regex.test(sentence)) {
            return sentence.replace(regex, '<span class="highlight-sentence">$1</span>');
          }
          return sentence;
        }).join(' ');
      });
    }
    
    return paragraphs;
  };

  if (loading) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <div className="text-center">
          <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-blue-600 mx-auto mb-4"></div>
          <p className="text-gray-600">Loading newsletter...</p>
        </div>
      </div>
    );
  }

  if (error) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <div className="text-center">
          <div className="bg-red-50 border border-red-200 text-red-700 px-6 py-4 rounded-lg mb-4">
            {error}
          </div>
          <Link 
            href="/"
            className="text-blue-600 hover:text-blue-800 hover:underline"
          >
            ← Back to Search
          </Link>
        </div>
      </div>
    );
  }

  if (!newsletter) {
    return (
      <div className="min-h-screen bg-gray-50 flex items-center justify-center">
        <div className="text-center">
          <p className="text-gray-600 mb-4">Newsletter not found</p>
          <Link 
            href="/"
            className="text-blue-600 hover:text-blue-800 hover:underline"
          >
            ← Back to Search
          </Link>
        </div>
      </div>
    );
  }

  // Get content - prefer body_text, fallback to stripped HTML
  // Use the shared cleaning utility to get best cleaned content
  const getContent = () => {
    if (!newsletter) return '';
    return getBestCleanedContent(newsletter.body_text || '', newsletter.body_html || '');
  };


  const content = getContent();
  const paragraphs = formatContent(content, searchQuery);
  const readTime = calculateReadTime(newsletter.word_count);

  return (
    <div className="min-h-screen">
      {/* Navigation */}
      <div className="sticky top-0 z-10">
        <div className="max-w-4xl mx-auto px-6 py-4">
          <Link 
            href="/"
            className="text-gray-600 hover:text-gray-900 hover:underline text-sm font-medium"
          >
            ← Back to Search
          </Link>
        </div>
      </div>

      {/* Article Container */}
      <div className="max-w-4xl mx-auto px-6 py-8">
        {/* Article Header */}
        <article>
          <header className="mb-8">
            <h1 className="text-4xl font-bold leading-tight mb-6">
              {newsletter.subject}
            </h1>
            
            <div className="flex items-center space-x-6 text-gray-600 mb-6">
              <div className="flex items-center space-x-2">
                <span className="font-medium text-gray-700">{newsletter.publisher_name}</span>
                {newsletter.is_vip && (
                  <span className="inline-flex items-center px-2 py-1 rounded-full text-xs font-medium bg-yellow-100 text-yellow-800">
                    VIP
                  </span>
                )}
              </div>
              <span>•</span>
              <time className="text-gray-500">
                {formatDate(newsletter.sent_date)}
              </time>
              <span>•</span>
              <span className="text-gray-500">
                {readTime} min read
              </span>
              <span>•</span>
              <span className="text-gray-500">
                {newsletter.word_count.toLocaleString()} words
              </span>
            </div>
          </header>

          {/* Article Content */}
          <div className="article-content">
            <div className="prose prose-lg max-w-none">
              {paragraphs.length > 0 ? (
                paragraphs.map((paragraph, index) => (
                  <p 
                    key={index} 
                    className="mb-6 leading-relaxed"
                    dangerouslySetInnerHTML={{ __html: paragraph }}
                  />
                ))
              ) : (
                <p className="text-gray-500 italic">No content available</p>
              )}
            </div>
          </div>
        </article>
      </div>
    </div>
  );
}
</file>

<file path="newsletter-search/src/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

/* Reading-optimized typography */
.prose {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  font-size: 18px;
  line-height: 1.7;
  color: #2d3748;
  max-width: 65ch;
  margin: 0 auto;
}

.prose p {
  margin-bottom: 1.5em;
  text-align: justify;
  hyphens: auto;
}

.prose h1 {
  font-size: 2.5rem;
  font-weight: 700;
  line-height: 1.2;
  margin-bottom: 1rem;
  color: #1a202c;
}

.prose h2 {
  font-size: 2rem;
  font-weight: 600;
  line-height: 1.3;
  margin-top: 2rem;
  margin-bottom: 1rem;
  color: #1a202c;
}

.prose h3 {
  font-size: 1.5rem;
  font-weight: 600;
  line-height: 1.4;
  margin-top: 1.5rem;
  margin-bottom: 0.75rem;
  color: #1a202c;
}

/* Article-specific styling */
article {
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  background-color: #fafafa;
  min-height: 100vh;
}

article h1 {
  font-size: 2.5rem;
  font-weight: 700;
  line-height: 1.2;
  color: #1a202c;
  margin-bottom: 1.5rem;
}

article header {
  border-bottom: 1px solid #e2e8f0;
  padding-bottom: 2rem;
  margin-bottom: 3rem;
  background-color: white;
  padding: 2rem;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

/* Sticky navigation */
.sticky {
  backdrop-filter: blur(8px);
  background-color: rgba(255, 255, 255, 0.95);
  border-bottom: 1px solid #e2e8f0;
}

/* Content highlighting */
.highlight-sentence {
  background-color: #fef3c7;
  padding: 2px 4px;
  border-radius: 3px;
  font-weight: 500;
}

/* Article content container */
.article-content {
  background-color: white;
  padding: 3rem;
  border-radius: 8px;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
  margin: 0 auto;
  max-width: 65ch;
}

/* Responsive design */
@media (max-width: 768px) {
  .prose {
    font-size: 16px;
    line-height: 1.6;
  }
  
  article h1 {
    font-size: 2rem;
  }
  
  .prose h2 {
    font-size: 1.75rem;
  }
  
  .prose h3 {
    font-size: 1.25rem;
  }
  
  .article-content {
    padding: 1.5rem;
    margin: 0 1rem;
  }
  
  article header {
    padding: 1.5rem;
    margin: 0 1rem 2rem 1rem;
  }
}
</file>

<file path="newsletter-search/src/app/layout.tsx">
import type { Metadata } from "next";
import { Geist, Geist_Mono } from "next/font/google";
import "./globals.css";

const geistSans = Geist({
  variable: "--font-geist-sans",
  subsets: ["latin"],
});

const geistMono = Geist_Mono({
  variable: "--font-geist-mono",
  subsets: ["latin"],
});

export const metadata: Metadata = {
  title: "Newsletter Search",
  description: "Search and browse your newsletter collection",
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
</file>

<file path="newsletter-search/src/app/page-semantic.tsx">
'use client';

import { useState } from 'react';
import Link from 'next/link';

interface SemanticResult {
  query: string;
  answer: string;
  citations: Array<{
    chunk_id: string;
    citation: string;
    publisher: string;
    date: any;
    subject: string;
  }>;
  chunks_used: number;
  cost_usd: number;
  chunks: Array<{
    chunk_id: string;
    subject: string;
    publisher: string;
    score: number;
  }>;
}

export default function SemanticSearchPage() {
  const [query, setQuery] = useState('');
  const [results, setResults] = useState<SemanticResult | null>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');

  const searchSemantic = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!query.trim()) return;

    setLoading(true);
    setError('');
    setResults(null);

    try {
      const response = await fetch('/api/intelligence/query', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ query }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || data.message || 'Search failed');
      }

      setResults(data);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Search failed');
      setResults(null);
    } finally {
      setLoading(false);
    }
  };

  const formatDate = (dateInput: any) => {
    if (!dateInput) return 'Date unknown';
    
    if (dateInput && typeof dateInput === 'object' && dateInput.value) {
      try {
        return new Date(dateInput.value).toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'short',
          day: 'numeric'
        });
      } catch {
        return 'Date unknown';
      }
    }
    
    if (typeof dateInput === 'string') {
      try {
        return new Date(dateInput).toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'short',
          day: 'numeric'
        });
      } catch {
        return 'Date unknown';
      }
    }
    
    return 'Date unknown';
  };

  return (
    <div className="min-h-screen bg-gray-50">
      <div className="container mx-auto px-4 py-8 max-w-4xl">
        <div className="mb-8">
          <h1 className="text-4xl font-bold text-gray-900 mb-2">Semantic Newsletter Search</h1>
          <p className="text-gray-600">Ask questions and get intelligent answers from 69,673 newsletters</p>
        </div>

        {/* Search Form */}
        <form onSubmit={searchSemantic} className="bg-white p-6 rounded-lg shadow-md mb-6">
          <div className="flex gap-4">
            <input
              type="text"
              value={query}
              onChange={(e) => setQuery(e.target.value)}
              placeholder="Ask a question... (e.g., 'What are the latest developments in AI regulation?')"
              className="flex-1 px-4 py-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500"
              disabled={loading}
            />
            <button
              type="submit"
              disabled={loading || !query.trim()}
              className="px-6 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 disabled:opacity-50 disabled:cursor-not-allowed font-medium"
            >
              {loading ? 'Searching...' : 'Search'}
            </button>
          </div>
        </form>

        {/* Error Message */}
        {error && (
          <div className="bg-red-50 border border-red-200 text-red-700 px-4 py-3 rounded-lg mb-6">
            <strong>Error:</strong> {error}
          </div>
        )}

        {/* Loading State */}
        {loading && (
          <div className="bg-white rounded-lg shadow-md p-8 text-center">
            <div className="animate-pulse">
              <div className="h-4 bg-gray-200 rounded w-3/4 mb-4"></div>
              <div className="h-4 bg-gray-200 rounded w-1/2 mb-4"></div>
              <div className="h-4 bg-gray-200 rounded w-5/6"></div>
            </div>
            <p className="mt-4 text-gray-500">Searching 938,601 chunks...</p>
          </div>
        )}

        {/* Results */}
        {results && !loading && (
          <div className="space-y-6">
            {/* AI Answer */}
            <div className="bg-white rounded-lg shadow-md p-6">
              <h2 className="text-2xl font-bold text-gray-900 mb-4">Answer</h2>
              <div className="prose max-w-none text-gray-700 whitespace-pre-wrap">
                {results.answer}
              </div>
              <div className="mt-4 pt-4 border-t border-gray-200 text-sm text-gray-500">
                Based on {results.chunks_used} relevant chunks • Cost: ${results.cost_usd.toFixed(4)}
              </div>
            </div>

            {/* Citations */}
            {results.citations && results.citations.length > 0 && (
              <div className="bg-white rounded-lg shadow-md p-6">
                <h3 className="text-xl font-semibold text-gray-900 mb-4">Sources</h3>
                <div className="space-y-3">
                  {results.citations.map((citation, idx) => (
                    <div key={idx} className="border-l-4 border-blue-500 pl-4 py-2">
                      <div className="font-medium text-gray-900">{citation.citation}</div>
                      <div className="text-sm text-gray-500 mt-1">
                        {formatDate(citation.date)}
                      </div>
                    </div>
                  ))}
                </div>
              </div>
            )}

            {/* Top Chunks */}
            {results.chunks && results.chunks.length > 0 && (
              <div className="bg-white rounded-lg shadow-md p-6">
                <h3 className="text-xl font-semibold text-gray-900 mb-4">
                  Relevant Newsletters ({results.chunks.length})
                </h3>
                <div className="space-y-3">
                  {results.chunks.slice(0, 5).map((chunk, idx) => (
                    <div key={chunk.chunk_id} className="border border-gray-200 rounded p-3 hover:bg-gray-50">
                      <div className="flex justify-between items-start">
                        <div className="flex-1">
                          <div className="font-medium text-gray-900">{chunk.subject}</div>
                          <div className="text-sm text-gray-500">{chunk.publisher}</div>
                        </div>
                        <div className="text-sm text-gray-400 ml-4">
                          {(chunk.score * 100).toFixed(0)}% match
                        </div>
                      </div>
                    </div>
                  ))}
                </div>
              </div>
            )}
          </div>
        )}

        {/* No Results State */}
        {!loading && !results && !error && query && (
          <div className="bg-white p-6 rounded-lg shadow-md text-center">
            <p className="text-gray-500">Enter a question above to search.</p>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="newsletter-search/src/lib/newsletter-cleaning.ts">
/**
 * Newsletter Content Cleaning Utility
 * 
 * This module provides functions to clean newsletter content by:
 * - Removing tracking URLs and email artifacts
 * - Stripping HTML tags and preserving text content
 * - Removing sponsored/advertisement content
 * - Fixing spacing issues for readability
 * - Selecting the best content source (body_text vs body_html)
 */

/**
 * Calculate content weight to determine the best content source
 */
export function calculateContentWeight(content: string): number {
  if (!content || content.trim().length === 0) return 0;
  
  let weight = content.length;
  
  // Penalize content that's mostly URLs
  const urlPattern = /https?:\/\/[^\s]+/g;
  const urls = content.match(urlPattern) || [];
  const urlLength = urls.reduce((sum, url) => sum + url.length, 0);
  
  if (urlLength > content.length * 0.5) {
    weight = weight * 0.1;
  }
  
  // Bonus for having proper sentence structure
  if (content.match(/[.!?]\s+[A-Z]/g)) {
    weight = weight * 1.2;
  }
  
  return weight;
}

/**
 * Strip HTML tags and decode entities
 */
export function stripHtml(html: string): string {
  if (!html) return '';
  
  // Remove style/script blocks
  let text = html
    .replace(/<style[^>]*>[\s\S]*?<\/style>/gi, ' ')
    .replace(/<script[^>]*>[\s\S]*?<\/script>/gi, ' ')
    .replace(/<noscript[^>]*>[\s\S]*?<\/noscript>/gi, ' ');
  
  // Remove HTML comments
  text = text.replace(/<!--[\s\S]*?-->/g, ' ');
  
  // Add spaces around ALL tags before removing them
  text = text.replace(/</g, ' <').replace(/>/g, '> ');
  
  // Remove all HTML tags
  text = text.replace(/<[^>]*>/g, ' ');
  
  // Decode HTML entities
  text = text
    .replace(/&nbsp;/g, ' ')
    .replace(/&amp;/g, '&')
    .replace(/&lt;/g, '<')
    .replace(/&gt;/g, '>')
    .replace(/&quot;/g, '"')
    .replace(/&#39;/g, "'")
    .replace(/&mdash;/g, ' — ')
    .replace(/&ndash;/g, ' – ')
    .replace(/&hellip;/g, '...')
    .replace(/&ldquo;/g, '"')
    .replace(/&rdquo;/g, '"')
    .replace(/&lsquo;/g, "'")
    .replace(/&rsquo;/g, "'")
    .replace(/&#\d+;/g, ' ')
    .replace(/&[a-z]+;/g, ' ');
  
  // Normalize whitespace (multiple spaces to single space)
  text = text.replace(/[ \t\n\r]+/g, ' ').trim();
  
  return text;
}

/**
 * Clean newsletter content by removing artifacts and fixing spacing
 */
export function cleanNewsletterContent(content: string): string {
  if (!content) return '';
  
  return content
    // Remove sponsored content
    .replace(/A message from [^.\n]{0,100}\./gim, '')
    .replace(/Presented by [^.\n]{0,100}\./gim, '')
    .replace(/Sponsored by [^.\n]{0,100}\./gim, '')
    .replace(/In partnership with [^.\n]{0,100}\./gim, '')
    .replace(/Advertisement[\s\S]{0,200}\.?/gim, '')
    .replace(/[A-Z][a-z]+'s new local partnership.*?See how\./gims, '')
    .replace(/Improving public transportation requires partnership.*?Learn more\./gims, '')
    .replace(/Want more [A-Z][a-z]+ content\? Check out.*?and more!/gims, '')
    .replace(/Want to help [A-Z][a-z]+ grow\? Become a member\./gims, '')
    .replace(/Support your local newsroom.*?and more!/gims, '')
    .replace(/Sponsored event listings.*?(?=\d\.|$)/gims, '')
    .replace(/Advertise with us\..*?(?=\d\.|$)/gims, '')
    .replace(/Don't miss out.*?(?=\d\.|$)/gims, '')
    .replace(/Learn how to ride\./gim, '')
    .replace(/See how\./gim, '')
    .replace(/Learn more\./gim, '')
    .replace(/Sign up now to get.*?inbox\./gims, '')
    .replace(/To stop receiving.*?preferences\./gims, '')
    .replace(/Was this email forwarded.*?preferences\./gims, '')
    .replace(/Interested in advertising.*?axios\.com\./gims, '')
    .replace(/A message from [^.]*\./gims, '')
    .replace(/Presented by [^.]*\./gims, '')
    .replace(/Sponsored by [^.]*\./gims, '')
    .replace(/Public-private partnerships.*?transit networks.*?Learn more\./gims, '')
    .replace(/Want to help.*?Become a member\./gims, '')
    .replace(/Axios thanks our partners.*?newsletters\./gims, '')
    .replace(/Advertise with us\..*$/gims, '')
    .replace(/Axios, PO Box.*$/gims, '')
    .replace(/To stop receiving.*$/gims, '')
    .replace(/unsubscribe or manage.*$/gims, '')
    .replace(/Want more.*?more!.*$/gims, '')
    
    // Remove image references
    .replace(/View image:.*?(?=\n|$)/gim, '')
    .replace(/\[image:.*?\]/gim, '')
    .replace(/<img[^>]*>/gim, '')
    .replace(/Image source.*?(?=\n|$)/gim, '')
    
    // Remove "View this post" links
    .replace(/View this post on the web at\s+https?:\/\/[^\s]+/gim, '')
    .replace(/View this email in your browser.*?(?=\n|$)/gim, '')
    .replace(/Read online.*?(?=\n|$)/gim, '')
    
    // Remove tracking URLs
    .replace(/https?:\/\/(newsletter-tracking|tracking|click-tracking|link-tracking|redirect-tracking)\.[^\s]+/gim, '')
    .replace(/https?:\/\/[a-zA-Z0-9.-]*\.com\/redirect[^\s]+/gim, '')
    .replace(/https?:\/\/(t\.co|bit\.ly|tinyurl\.com|ow\.ly|buff\.ly|goo\.gl)[^\s]+/gim, '')
    .replace(/https?:\/\/email\.(semafor|axios|bloomberg)\.[^\s]+/gim, '')
    .replace(/https?:\/\/mailchi\.mp[^\s]+/gim, '')
    
    // Remove very long URLs (over 60 chars, likely tracking)
    .replace(/https?:\/\/[^\s]{60,}/gim, '')
    
    // Remove unsubscribe boilerplate
    .replace(/Unsubscribe From This List\s+https?:\/\/[^\s]+/gim, '')
    .replace(/\[Unsubscribe\]/gim, '')
    .replace(/\[Update preferences\]/gim, '')
    .replace(/\[View in browser\]/gim, '')
    .replace(/\[Forward to a friend\]/gim, '')
    .replace(/This email was sent to.*?(?=\n|$)/gim, '')
    .replace(/You received this email because.*?(?=\n|$)/gim, '')
    .replace(/To unsubscribe.*?(?=\n|$)/gim, '')
    .replace(/If you no longer wish to receive.*?(?=\n|$)/gim, '')
    .replace(/Click here to unsubscribe.*?(?=\n|$)/gim, '')
    .replace(/Tracking pixel.*?(?=\n|$)/gim, '')
    
    // Remove email separators
    .replace(/-{10,}/g, '')
    .replace(/\*{10,}/g, '')
    .replace(/\u2014{3,}/g, '')
    .replace(/Presented by:.*?(?=\n\n|\n[A-Z][a-z])/gim, '')
    .replace(/Presented by\n.*?(?=\n\n|\n[A-Z][a-z])/gim, '')
    .replace(/In partnership with.*?(?=\n\n|\n[A-Z][a-z])/gim, '')
    
    // Clean up HTML entities
    .replace(/&zwnj;/g, '')
    .replace(/&nbsp;/g, ' ')
    .replace(/&middot;/g, ' · ')
    .replace(/&amp;/g, '&')
    .replace(/&lt;/g, '<')
    .replace(/&gt;/g, '>')
    .replace(/&quot;/g, '"')
    .replace(/&#39;/g, "'")
    .replace(/&[a-z]+;/g, ' ')
    
    // Remove orphaned artifacts
    .replace(/\[here\]/gim, '')
    .replace(/\[.*?\]\(\s*\)/gim, '')
    .replace(/\(Caption:\)/gim, '')
    .replace(/^(\d+|\d+[A-Z]|\d+\.)$/gm, '')
    .replace(/^View in browser$/gm, '')
    .replace(/^Sign up here.*$/gm, '')
    .replace(/^Don't keep us a secret.*$/gm, '')
    .replace(/Sponsorship has no influence.*$/gim, '')
    .replace(/Was this email forwarded to you\?.*$/gim, '')
    .replace(/Follow [A-Z][a-z]+ on social media:.*$/gim, '')
    
    // Add spacing between words
    .replace(/([a-z])([0-9])/g, '$1 $2')
    .replace(/([0-9])([A-Z])/g, '$1 $2')
    .replace(/([a-z])([A-Z])/g, '$1 $2')
    .replace(/([.!?])([A-Z])/g, '$1 $2')
    .replace(/([.!?])([a-z])/g, '$1 $2')
    .replace(/([.!?])([0-9])/g, '$1 $2')
    
    // Fix punctuation spacing
    .replace(/ : /g, ': ')
    .replace(/ \. /g, '. ')
    .replace(/ , /g, ', ')
    .replace(/ ; /g, '; ')
    .replace(/ \( /g, ' (')
    .replace(/ \) /g, ') ')
    .replace(/ \[ /g, ' [')
    .replace(/ \] /g, '] ')
    
    // Fix specific spacing issues
    .replace(/(\d+) \. (\d)/g, '$1.$2')
    .replace(/(\d+) \./g, '$1.')
    .replace(/\. (\d)/g, '.$1')
    .replace(/([a-z]) \. com/gi, '$1.com')
    .replace(/([a-z]) \. ([a-z])/gi, '$1.$2')
    .replace(/Data: ([A-Z][a-z]+) \. com/gi, 'Data: $1.com')
    
    // Clean up final whitespace
    .replace(/ {2,}/g, ' ')
    .replace(/\n\s*\n\s*\n+/g, '\n\n')
    .replace(/^\s+|\s+$/gm, '')
    .trim();
}

/**
 * Get the best cleaned content from a newsletter
 * Automatically selects between body_text and body_html based on content quality
 */
export function getBestCleanedContent(bodyText: string, bodyHtml: string): string {
  const textWeight = calculateContentWeight(bodyText);
  const htmlWeight = calculateContentWeight(stripHtml(bodyHtml));
  
  // Choose the field with higher weight
  const bestContent = htmlWeight > textWeight ? stripHtml(bodyHtml) : bodyText;
  
  // Apply cleaning
  return cleanNewsletterContent(bestContent);
}
</file>

<file path="newsletter-search/.gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path="newsletter-search/eslint.config.mjs">
import { defineConfig, globalIgnores } from "eslint/config";
import nextVitals from "eslint-config-next/core-web-vitals";
import nextTs from "eslint-config-next/typescript";

const eslintConfig = defineConfig([
  ...nextVitals,
  ...nextTs,
  // Override default ignores of eslint-config-next.
  globalIgnores([
    // Default ignores of eslint-config-next:
    ".next/**",
    "out/**",
    "build/**",
    "next-env.d.ts",
  ]),
]);

export default eslintConfig;
</file>

<file path="newsletter-search/next.config.ts">
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
</file>

<file path="newsletter-search/postcss.config.mjs">
const config = {
  plugins: {
    "@tailwindcss/postcss": {},
  },
};

export default config;
</file>

<file path="newsletter-search/README.md">
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
</file>

<file path="scripts/publishers/add-manual-override-fields.ts">
/**
 * Add manual override fields to publishers table
 * This allows manual adjustment of quality scores
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'publishers';

async function addManualOverrideFields() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  const dataset = bigquery.dataset(DATASET_ID);
  const table = dataset.table(TABLE_ID);

  console.log('Adding manual override fields to publishers table...\n');

  try {
    // Check if fields already exist
    const [metadata] = await table.getMetadata();
    const existingFields = metadata.schema?.fields?.map(f => f.name) || [];
    
    const fieldsToAdd = [
      {
        name: 'manual_quality_score_override',
        type: 'FLOAT64',
        mode: 'NULLABLE',
        description: 'Manual quality score override (0-100). If set, overrides calculated quality_score.'
      },
      {
        name: 'manual_override_reason',
        type: 'STRING',
        mode: 'NULLABLE',
        description: 'Reason for manual override (e.g., "Expert knowledge", "High-value source")'
      },
      {
        name: 'manual_override_updated_at',
        type: 'TIMESTAMP',
        mode: 'NULLABLE',
        description: 'When manual override was last updated'
      },
      {
        name: 'manual_override_updated_by',
        type: 'STRING',
        mode: 'NULLABLE',
        description: 'Who updated the manual override (e.g., user email or identifier)'
      },
      {
        name: 'manual_individual_signal_overrides',
        type: 'JSON',
        mode: 'NULLABLE',
        description: 'JSON object with manual overrides for individual signals: {citation_signal: 0.8, subscriber_signal: 0.9, ...}'
      }
    ];

    const fieldsToAddFiltered = fieldsToAdd.filter(f => !existingFields.includes(f.name));

    if (fieldsToAddFiltered.length === 0) {
      console.log('✅ All manual override fields already exist.\n');
      return;
    }

    console.log(`Adding ${fieldsToAddFiltered.length} new fields...\n`);

    // Add fields one by one using ALTER TABLE
    for (const field of fieldsToAddFiltered) {
      try {
        const alterQuery = `
          ALTER TABLE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
          ADD COLUMN IF NOT EXISTS ${field.name} ${field.type}
          ${field.mode === 'REQUIRED' ? 'NOT NULL' : ''}
        `;

        await bigquery.query(alterQuery);
        console.log(`   ✅ Added field: ${field.name} (${field.type})`);
      } catch (error: any) {
        // Check if field already exists (might have been added between check and add)
        if (error.message?.includes('already exists') || error.message?.includes('Duplicate column')) {
          console.log(`   ⚠️  Field ${field.name} already exists, skipping`);
        } else {
          throw error;
        }
      }
    }

    console.log('\n✅ Manual override fields added successfully!\n');
    console.log('Fields added:');
    fieldsToAddFiltered.forEach(f => {
      console.log(`   - ${f.name}: ${f.description}`);
    });

  } catch (error: any) {
    console.error('❌ Error adding manual override fields:', error.message);
    throw error;
  }
}

addManualOverrideFields()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/calculate-citations-pattern-based-robust.ts">
/**
 * Pattern-Based Citation Detection (Robust Version)
 * 
 * Enhanced with:
 * - Error handling and retry logic
 * - Query timeouts
 * - Progress tracking
 * - Resume capability
 * - Memory-efficient processing
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import * as fs from 'fs';
import * as path from 'path';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const CHUNKS_TABLE = 'chunks';

// Configuration
const BATCH_SIZE = 10000;
const MAX_RETRIES = 3;
const RETRY_DELAY_MS = 2000; // 2 seconds
const QUERY_TIMEOUT_MS = 300000; // 5 minutes per query
const PROGRESS_FILE = path.join(__dirname, '..', '..', '.citation-progress.json');

interface CitationMatch {
  discovery_id: string;
  newsletter_name: string;
  citation_count: number;
  citing_publishers: string[];
}

interface ProgressState {
  lastProcessedOffset: number;
  citations: { [identifier: string]: {
    identifier: string;
    newsletter_name: string;
    source: 'discovered' | 'publisher';
    citing_publishers: string[];
  }};
  startTime: string;
  lastUpdate: string;
}

/**
 * Load progress from file
 */
function loadProgress(): ProgressState | null {
  try {
    if (fs.existsSync(PROGRESS_FILE)) {
      const content = fs.readFileSync(PROGRESS_FILE, 'utf8');
      return JSON.parse(content);
    }
  } catch (error) {
    console.error('⚠️  Error loading progress file:', error);
  }
  return null;
}

/**
 * Save progress to file
 */
function saveProgress(state: ProgressState): void {
  try {
    state.lastUpdate = new Date().toISOString();
    fs.writeFileSync(PROGRESS_FILE, JSON.stringify(state, null, 2));
  } catch (error) {
    console.error('⚠️  Error saving progress file:', error);
  }
}

/**
 * Sleep helper for retries
 */
function sleep(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms));
}

/**
 * Execute BigQuery query with timeout and retry
 */
async function queryWithRetry(
  bigquery: BigQuery,
  query: string,
  options: { timeout?: number; maxRetries?: number } = {}
): Promise<any[]> {
  const timeout = options.timeout || QUERY_TIMEOUT_MS;
  const maxRetries = options.maxRetries || MAX_RETRIES;
  
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      // Use Promise.race to add timeout wrapper around query
      const queryPromise = bigquery.query({ query });
      const timeoutPromise = new Promise<never>((_, reject) => 
        setTimeout(() => reject(new Error(`Query timeout after ${timeout}ms`)), timeout)
      );

      const result = await Promise.race([queryPromise, timeoutPromise]);
      // BigQuery query returns [rows, metadata] tuple
      if (Array.isArray(result) && result.length >= 1) {
        return result[0] || [];
      }
      // If result is not array, return empty array (shouldn't happen)
      return [];
    } catch (error: any) {
      const errorMsg = error.message || String(error);
      
      // Don't retry on certain errors (syntax errors, etc.)
      if (errorMsg.includes('Syntax error') || errorMsg.includes('Invalid query')) {
        throw new Error(`Query syntax error: ${errorMsg}`);
      }
      
      console.error(`   ⚠️  Query attempt ${attempt}/${maxRetries} failed:`, errorMsg.substring(0, 200));
      
      if (attempt < maxRetries) {
        const delay = RETRY_DELAY_MS * attempt; // Exponential backoff
        console.log(`   ⏳ Retrying in ${delay}ms...`);
        await sleep(delay);
      } else {
        throw new Error(`Query failed after ${maxRetries} attempts: ${errorMsg}`);
      }
    }
  }
  
  throw new Error('Query failed: unknown error');
}

async function calculateCitationsPatternBased() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  const startTime = new Date().toISOString();

  console.log('📊 Calculating citations using pattern-based detection (Robust Version)...\n');
  console.log('Features:');
  console.log('  ✅ Error handling & retry logic');
  console.log('  ✅ Query timeouts (5 min per query)');
  console.log('  ✅ Progress tracking & resume capability');
  console.log('  ✅ Memory-efficient batch processing\n');

  // Check for existing progress
  const existingProgress = loadProgress();
  if (existingProgress) {
    console.log('⚠️  Found existing progress file!');
    console.log(`   Last processed offset: ${existingProgress.lastProcessedOffset}`);
    console.log(`   Citations found so far: ${Object.keys(existingProgress.citations).length}`);
    console.log(`   Started: ${existingProgress.startTime}`);
    console.log(`   Last update: ${existingProgress.lastUpdate}\n`);
    console.log('Do you want to:');
    console.log('  1. Resume from last position');
    console.log('  2. Start fresh (delete progress file)\n');
    // For now, auto-resume
    console.log('   Auto-resuming from last position...\n');
  }

  try {
    // Step 1: Get ALL newsletters/publishers with URLs
    // Combine discovered_newsletters with publishers table to get full coverage
    console.log('Step 1: Fetching all newsletters/publishers with URLs...');
    console.log('   (Checking discovered_newsletters + publishers table)\n');
    
    // Get from discovered_newsletters
    const discoveredNewsletters = await queryWithRetry(bigquery, `
      SELECT 
        discovery_id,
        newsletter_name,
        newsletter_url,
        canonical_url
      FROM \`${PROJECT_ID}.${DATASET_ID}.discovered_newsletters\`
      WHERE newsletter_url IS NOT NULL
        AND is_relevant = TRUE
        AND needs_review = FALSE
    `);
    
    if (!Array.isArray(discoveredNewsletters)) {
      throw new Error(`Expected array from query, got: ${typeof discoveredNewsletters}`);
    }
    
    // Also get from publishers table and infer URLs from email domains
    const allPublishers = await queryWithRetry(bigquery, `
      SELECT 
        publisher_id,
        publisher_name,
        newsletter_url,
        primary_email,
        email_domains
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
    `);
    
    // Infer URLs from email domains for publishers without explicit URLs
    const publishersWithUrls: any[] = [];
    const publishersWithInferredUrls: any[] = [];
    
    for (const publisher of allPublishers) {
      if (publisher.newsletter_url) {
        publishersWithUrls.push(publisher);
      } else if (publisher.primary_email) {
        // Infer URL from email domain
        const email = publisher.primary_email.toLowerCase();
        let inferredUrl: string | null = null;
        
        // Extract subdomain from email (e.g., "morningbrew@substack.com" -> "morningbrew.substack.com")
        if (email.includes('@substack.com')) {
          const subdomain = email.split('@')[0];
          inferredUrl = `https://${subdomain}.substack.com`;
        } else if (email.includes('@beehiiv.com')) {
          const subdomain = email.split('@')[0];
          inferredUrl = `https://${subdomain}.beehiiv.com`;
        } else if (email.includes('@ghost.org')) {
          // Ghost uses custom domains, can't infer easily
          // Skip for now
        }
        
        if (inferredUrl) {
          publishersWithInferredUrls.push({
            ...publisher,
            newsletter_url: inferredUrl,
            is_inferred: true,
          });
        }
      }
    }
    
    console.log(`   Found ${discoveredNewsletters.length} from discovered_newsletters`);
    console.log(`   Found ${publishersWithUrls.length} from publishers table (with explicit URLs)`);
    console.log(`   Found ${publishersWithInferredUrls.length} from publishers table (inferred from emails)\n`);
    
    if (discoveredNewsletters.length === 0 && publishersWithUrls.length === 0 && publishersWithInferredUrls.length === 0) {
      console.log('⚠️  No newsletters found with URLs.\n');
      return;
    }

    // Step 2: Create lookup map from all sources
    console.log('Step 2: Creating newsletter URL lookup map...');
    
    const newsletterUrlMap = new Map<string, {
      identifier: string; // discovery_id or publisher_id
      newsletter_name: string;
      source: 'discovered' | 'publisher';
    }>();
    
    // Add from discovered_newsletters
    for (const newsletter of discoveredNewsletters) {
      if (!newsletter.newsletter_url) continue;
      try {
        const url = new URL(newsletter.newsletter_url);
        const normalizedUrl = url.hostname.toLowerCase().replace(/^www\./, '');
        newsletterUrlMap.set(normalizedUrl, {
          identifier: newsletter.discovery_id,
          newsletter_name: newsletter.newsletter_name,
          source: 'discovered',
        });
      } catch {
        // Invalid URL, skip
      }
    }
    
    // Add from publishers table (explicit URLs)
    for (const publisher of publishersWithUrls) {
      if (!publisher.newsletter_url) continue;
      try {
        const url = new URL(publisher.newsletter_url);
        const normalizedUrl = url.hostname.toLowerCase().replace(/^www\./, '');
        // Only add if not already in map (discovered_newsletters takes precedence)
        if (!newsletterUrlMap.has(normalizedUrl)) {
          newsletterUrlMap.set(normalizedUrl, {
            identifier: publisher.publisher_id,
            newsletter_name: publisher.publisher_name,
            source: 'publisher',
          });
        }
      } catch {
        // Invalid URL, skip
      }
    }
    
    // Add from publishers table (inferred URLs from email domains)
    for (const publisher of publishersWithInferredUrls) {
      if (!publisher.newsletter_url) continue;
      try {
        const url = new URL(publisher.newsletter_url);
        const normalizedUrl = url.hostname.toLowerCase().replace(/^www\./, '');
        // Only add if not already in map
        if (!newsletterUrlMap.has(normalizedUrl)) {
          newsletterUrlMap.set(normalizedUrl, {
            identifier: publisher.publisher_id,
            newsletter_name: publisher.publisher_name,
            source: 'publisher',
          });
        }
      } catch {
        // Invalid URL, skip
      }
    }
    
    console.log(`   Created lookup map for ${newsletterUrlMap.size} newsletters/publishers\n`);

    // Step 3: Get total count of chunks with URLs (optimized: only scan chunks with http)
    console.log('Step 3: Counting chunks that might contain URLs...');
    console.log('   (Scanning chunks with "http" to optimize - most URLs will be in these)\n');
    
    const countResult = await queryWithRetry(bigquery, `
      SELECT COUNT(*) as total
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
      WHERE c.chunk_text LIKE '%http%'
    `);
    
    if (!Array.isArray(countResult) || countResult.length === 0) {
      throw new Error('Failed to get chunk count from BigQuery');
    }
    
    const totalChunks = parseInt(countResult[0]?.total) || 0;
    console.log(`   Found ${totalChunks.toLocaleString()} chunks with URLs (out of full corpus)\n`);
    console.log(`   This will search these chunks for newsletter URL citations...\n`);
    
    if (totalChunks === 0) {
      console.log('⚠️  No chunks with URLs found.\n');
      return;
    }

    // Initialize progress state
    const progress: ProgressState = existingProgress || {
      lastProcessedOffset: 0,
      citations: {},
      startTime: startTime,
      lastUpdate: startTime,
    };

    // Restore citation map from progress
    const citationMap = new Map<string, {
      identifier: string;
      newsletter_name: string;
      source: 'discovered' | 'publisher';
      citing_publishers: Set<string>;
    }>();

    for (const [identifier, data] of Object.entries(progress.citations)) {
      citationMap.set(identifier, {
        identifier: data.identifier || identifier,
        newsletter_name: data.newsletter_name,
        source: data.source || 'discovered',
        citing_publishers: new Set(data.citing_publishers),
      });
    }

    // Step 4: Process chunks in batches
    console.log('Step 4: Processing chunks in batches...\n');
    
    const totalBatches = Math.ceil(totalChunks / BATCH_SIZE);
    const startOffset = progress.lastProcessedOffset;
    const startBatch = Math.floor(startOffset / BATCH_SIZE) + 1;

    console.log(`   Processing ${totalBatches} batches (starting from batch ${startBatch})...\n`);

    for (let offset = startOffset; offset < totalChunks; offset += BATCH_SIZE) {
      const batchNum = Math.floor(offset / BATCH_SIZE) + 1;
      const batchStartTime = Date.now();

      try {
        console.log(`   Processing batch ${batchNum}/${totalBatches} (offset ${offset.toLocaleString()})...`);
        
        // Search chunks with URLs for newsletter URL patterns
        // Extract any newsletter URL patterns from chunk text
        // Filter in application logic (more efficient than WHERE clause regex)
        const urlQuery = `
          SELECT 
            c.newsletter_id,
            c.publisher_name as citing_publisher,
            REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.substack\\.com') as substack_subdomain,
            REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.beehiiv\\.com') as beehiiv_subdomain,
            REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.ghost\\.org') as ghost_subdomain,
            REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.tinyletter\\.com') as tinyletter_subdomain
          FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
          WHERE c.chunk_text LIKE '%http%'
          LIMIT ${BATCH_SIZE} OFFSET ${offset}
        `;

        const urlChunks = await queryWithRetry(bigquery, urlQuery);
        
        let processedCount = 0;
        let citationsFoundInBatch = 0;

        for (const chunk of urlChunks) {
          if (!chunk.citing_publisher) continue;
          
          processedCount++;
          
          const platforms = [
            { subdomain: chunk.substack_subdomain, domain: 'substack.com' },
            { subdomain: chunk.beehiiv_subdomain, domain: 'beehiiv.com' },
            { subdomain: chunk.ghost_subdomain, domain: 'ghost.org' },
            { subdomain: chunk.tinyletter_subdomain, domain: 'tinyletter.com' },
          ];
          
          for (const platform of platforms) {
            if (!platform.subdomain) continue;
            
            const normalizedUrl = `${platform.subdomain}.${platform.domain}`;
            const newsletterInfo = newsletterUrlMap.get(normalizedUrl);
            
            if (newsletterInfo) {
              // Don't count self-citations
              if (newsletterInfo.newsletter_name === chunk.citing_publisher) continue;
              
              if (!citationMap.has(newsletterInfo.identifier)) {
                citationMap.set(newsletterInfo.identifier, {
                  identifier: newsletterInfo.identifier,
                  newsletter_name: newsletterInfo.newsletter_name,
                  source: newsletterInfo.source,
                  citing_publishers: new Set(),
                });
                citationsFoundInBatch++;
              }
              
              const entry = citationMap.get(newsletterInfo.identifier)!;
              entry.citing_publishers.add(chunk.citing_publisher);
            }
          }
        }

        const batchTime = ((Date.now() - batchStartTime) / 1000).toFixed(1);
        console.log(`     ✅ Processed ${processedCount} chunks in ${batchTime}s`);
        console.log(`     📊 Found ${citationsFoundInBatch} new newsletters with citations`);
        console.log(`     📈 Total: ${citationMap.size} newsletters with citations\n`);

        // Update progress
        progress.lastProcessedOffset = offset + urlChunks.length;
        progress.citations = Object.fromEntries(
          Array.from(citationMap.entries()).map(([id, data]) => [
            id,
            {
              identifier: data.identifier,
              newsletter_name: data.newsletter_name,
              source: data.source,
              citing_publishers: Array.from(data.citing_publishers),
            },
          ])
        );
        saveProgress(progress);

      } catch (error: any) {
        console.error(`\n❌ Error processing batch ${batchNum}:`, error.message);
        console.error(`   Progress saved up to offset ${offset}`);
        console.error(`   You can resume by running the script again\n`);
        throw error;
      }
    }

    // Clean up progress file on success
    if (fs.existsSync(PROGRESS_FILE)) {
      fs.unlinkSync(PROGRESS_FILE);
      console.log('✅ Progress file cleaned up\n');
    }

    // Step 5: Aggregate results
    console.log('Step 5: Aggregating citation counts...');
    const citations: CitationMatch[] = [];

    for (const [identifier, data] of citationMap.entries()) {
      citations.push({
        discovery_id: data.identifier, // Keep field name for compatibility
        newsletter_name: data.newsletter_name,
        citation_count: data.citing_publishers.size,
        citing_publishers: Array.from(data.citing_publishers),
      });
    }

    citations.sort((a, b) => b.citation_count - a.citation_count);

    console.log(`\n📊 Citation Analysis Results:\n`);
    console.log(`   Total newsletters with citations: ${citations.length}`);
    console.log(`   Total citations: ${citations.reduce((sum, c) => sum + c.citation_count, 0)}`);

    if (citations.length > 0) {
      const avg = citations.reduce((sum, c) => sum + c.citation_count, 0) / citations.length;
      console.log(`   Average citations per newsletter: ${avg.toFixed(1)}\n`);

      console.log('🏆 Top 10 Most Cited Newsletters:');
      citations.slice(0, 10).forEach((c, idx) => {
        console.log(`   ${idx + 1}. ${c.citation_count} citations: ${c.newsletter_name}`);
      });
    }

    console.log('\n✅ Pattern-based citation analysis complete!\n');

    // Step 6: Update publishers table with citation counts
    console.log('Step 6: Updating publishers table with citation counts...\n');
    await updatePublishersWithCitations(bigquery, citations);

  } catch (error: any) {
    console.error('\n❌ Fatal error:', error.message);
    console.error('   Progress has been saved. You can resume by running the script again.\n');
    process.exit(1);
  }
}

/**
 * Update publishers table with citation counts
 */
async function updatePublishersWithCitations(bigquery: BigQuery, citations: CitationMatch[]) {
  console.log(`   Updating ${citations.length} publishers with citation data...\n`);

  let updatedCount = 0;
  let notFoundCount = 0;

  for (const citation of citations) {
    try {
      // Try to find publisher by discovery_id first
      if (citation.discovery_id) {
        // Check if this is a discovery_id (UUID format) or publisher_id
        // discovery_ids are UUIDs, publisher_ids might be different format
        // Try discovery_id match first (most common case)
        const isUUID = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i.test(citation.discovery_id);
        
        if (isUUID) {
          // Update via discovery_id link using MERGE
          const mergeQuery = `
            MERGE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` AS target
            USING (
              SELECT 
                @discovery_id AS discovery_id,
                @citation_count AS citation_count,
                @citing_publishers AS citing_publishers
            ) AS source
            ON target.discovery_id = source.discovery_id
            WHEN MATCHED THEN
              UPDATE SET
                citation_count = source.citation_count,
                citing_publishers = source.citing_publishers,
                updated_at = CURRENT_TIMESTAMP()
          `;

          const [job] = await bigquery.createQueryJob({
            query: mergeQuery,
            params: {
              discovery_id: citation.discovery_id,
              citation_count: citation.citation_count,
              citing_publishers: citation.citing_publishers.length > 0 
                ? citation.citing_publishers 
                : [],
            },
          });
          
          const [rows] = await job.getQueryResults();
          if (rows && rows.length > 0) {
            updatedCount++;
          } else {
            notFoundCount++;
          }
        } else {
          // It's a publisher_id - use MERGE
          const mergeQuery = `
            MERGE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` AS target
            USING (
              SELECT 
                @publisher_id AS publisher_id,
                @citation_count AS citation_count,
                @citing_publishers AS citing_publishers
            ) AS source
            ON target.publisher_id = source.publisher_id
            WHEN MATCHED THEN
              UPDATE SET
                citation_count = source.citation_count,
                citing_publishers = source.citing_publishers,
                updated_at = CURRENT_TIMESTAMP()
          `;

          const [job] = await bigquery.createQueryJob({
            query: mergeQuery,
            params: {
              publisher_id: citation.discovery_id,
              citation_count: citation.citation_count,
              citing_publishers: citation.citing_publishers.length > 0 
                ? citation.citing_publishers 
                : [],
            },
          });
          
          const [rows] = await job.getQueryResults();
          if (rows && rows.length > 0) {
            updatedCount++;
          } else {
            notFoundCount++;
          }
        }
      }
      
      // Always try matching by newsletter name (as fallback or primary method)
      // This handles cases where discovery_id isn't linked yet
      const nameMatchQuery = `
        UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
        SET 
          citation_count = @citation_count,
          citing_publishers = @citing_publishers,
          updated_at = CURRENT_TIMESTAMP()
        WHERE publisher_name = @newsletter_name
           OR LOWER(publisher_name) = LOWER(@newsletter_name)
      `;

      try {
        const [job] = await bigquery.createQueryJob({
          query: nameMatchQuery,
          params: {
            newsletter_name: citation.newsletter_name,
            citation_count: citation.citation_count,
            citing_publishers: citation.citing_publishers.length > 0 
              ? citation.citing_publishers 
              : [],
          },
        });
        
        await job.getQueryResults();
        
        // Verify update by checking if publisher exists and was updated
        const checkQuery = `
          SELECT citation_count
          FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          WHERE (publisher_name = @newsletter_name
             OR LOWER(publisher_name) = LOWER(@newsletter_name))
            AND citation_count = @citation_count
        `;
        const [checkRows] = await bigquery.query({
          query: checkQuery,
          params: { 
            newsletter_name: citation.newsletter_name,
            citation_count: citation.citation_count,
          },
        });
        
        if (checkRows && checkRows.length > 0) {
          if (updatedCount === 0) {
            // Only count if we didn't already update via discovery_id
            updatedCount++;
          }
        } else {
          // Check if publisher exists at all
          const existsQuery = `
            SELECT publisher_name
            FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
            WHERE publisher_name = @newsletter_name
               OR LOWER(publisher_name) = LOWER(@newsletter_name)
          `;
          const [existsRows] = await bigquery.query({
            query: existsQuery,
            params: { newsletter_name: citation.newsletter_name },
          });
          
          if (!existsRows || existsRows.length === 0) {
            notFoundCount++;
            console.log(`     ⚠️  Not found: ${citation.newsletter_name}`);
          } else {
            // Found but update didn't work - might be a data issue
            updatedCount++;
          }
        }
      } catch (error: any) {
        // If UPDATE failed, try to see if publisher exists
        notFoundCount++;
        console.log(`     ⚠️  Error updating ${citation.newsletter_name}: ${error.message}`);
      }
      
      // Also try matching by newsletter name even if we have a discovery_id
      // (in case publisher exists but discovery_id not linked yet)
      if (citation.discovery_id && notFoundCount > 0) {
        // Try name match as fallback
        const nameMatchQuery = `
          MERGE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` AS target
          USING (
            SELECT 
              @newsletter_name AS newsletter_name,
              @citation_count AS citation_count,
              @citing_publishers AS citing_publishers
          ) AS source
          ON target.publisher_name = source.newsletter_name
             OR LOWER(target.publisher_name) = LOWER(source.newsletter_name)
          WHEN MATCHED THEN
            UPDATE SET
              citation_count = source.citation_count,
              citing_publishers = source.citing_publishers,
              updated_at = CURRENT_TIMESTAMP()
        `;

        try {
          const [job] = await bigquery.createQueryJob({
            query: nameMatchQuery,
            params: {
              newsletter_name: citation.newsletter_name,
              citation_count: citation.citation_count,
              citing_publishers: citation.citing_publishers.length > 0 
                ? citation.citing_publishers 
                : [],
            },
          });
          
          const [rows] = await job.getQueryResults();
          if (rows && rows.length > 0) {
            updatedCount++;
            notFoundCount--; // Found via name match
          }
        } catch (error) {
          // Ignore errors in fallback
        }
      }
    } catch (error: any) {
      console.error(`     ❌ Error updating ${citation.newsletter_name}:`, error.message);
      notFoundCount++;
    }
  }

  console.log(`\n   ✅ Updated ${updatedCount} publishers`);
  if (notFoundCount > 0) {
    console.log(`   ⚠️  ${notFoundCount} citations could not be matched to publishers`);
    console.log(`      (These may be discovered newsletters not yet linked to publishers)\n`);
  }
}

calculateCitationsPatternBased()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/calculate-citations-pattern-based.ts">
/**
 * Pattern-Based Citation Detection
 * 
 * Instead of searching for publisher names (too noisy), we:
 * 1. Extract URLs from chunks (newsletter links)
 * 2. Extract citation phrases ("via X", "from X", etc.)
 * 3. Match to publishers table
 * 4. Count actual citations
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const CHUNKS_TABLE = 'chunks';

interface CitationMatch {
  discovery_id: string;
  newsletter_name: string;
  citation_count: number;
  citing_publishers: string[];
}

async function calculateCitationsPatternBased() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Calculating citations using pattern-based detection...\n');
  console.log('This approach:');
  console.log('  1. Extracts URLs from chunks (newsletter links)');
  console.log('  2. Extracts citation phrases ("via X", "from X")');
  console.log('  3. Matches to publishers table');
  console.log('  4. Counts actual citations (not false positives)\n');

  // Step 1: Get all newsletters with URLs from discovered_newsletters
  // We'll match URLs in chunks to these newsletters, then link to publishers later
  console.log('Step 1: Fetching newsletters with URLs from discovered_newsletters...');
  
  const [discoveredNewsletters] = await bigquery.query({
    query: `
      SELECT 
        discovery_id,
        newsletter_name,
        newsletter_url,
        canonical_url
      FROM \`${PROJECT_ID}.${DATASET_ID}.discovered_newsletters\`
      WHERE newsletter_url IS NOT NULL
        AND is_relevant = TRUE
        AND needs_review = FALSE
    `,
  });
  
  console.log(`   Found ${discoveredNewsletters.length} relevant newsletters with URLs\n`);

  if (discoveredNewsletters.length === 0) {
    console.log('⚠️  No newsletters found with URLs.\n');
    return;
  }

  // Step 2: Extract URLs from chunks using BigQuery regex
  console.log('Step 2: Extracting newsletter URLs from chunks...');
  console.log('   This will find chunks containing newsletter URLs and match them to publishers...\n');
  
  // Create a map of newsletter URLs to discovery IDs for fast lookup
  const newsletterUrlMap = new Map<string, {
    discovery_id: string;
    newsletter_name: string;
  }>(); // normalized_url -> {discovery_id, newsletter_name}
  
  for (const newsletter of discoveredNewsletters) {
    if (!newsletter.newsletter_url) continue;
    try {
      const url = new URL(newsletter.newsletter_url);
      const normalizedUrl = url.hostname.toLowerCase().replace(/^www\./, '');
      newsletterUrlMap.set(normalizedUrl, {
        discovery_id: newsletter.discovery_id,
        newsletter_name: newsletter.newsletter_name,
      });
    } catch {
      // Invalid URL, skip
    }
  }
  
  console.log(`   Created lookup map for ${newsletterUrlMap.size} newsletters\n`);

  // Step 3: Query chunks with newsletter URLs and match in SQL
  console.log('Step 3: Querying chunks with newsletter URLs...');
  
  // Build a query that extracts URLs and matches them to newsletters
  // We'll process in batches to avoid memory issues
  const BATCH_SIZE = 10000;
  const citationMap = new Map<string, {
    discovery_id: string;
    newsletter_name: string;
    citing_publishers: Set<string>;
  }>();

  // Get total count first (check for platform domains)
  const [countResult] = await bigquery.query({
    query: `
      SELECT COUNT(*) as total
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
      WHERE c.chunk_text LIKE '%substack%'
         OR c.chunk_text LIKE '%beehiiv%'
         OR c.chunk_text LIKE '%ghost%'
         OR c.chunk_text LIKE '%tinyletter%'
    `,
  });
  
  const totalChunks = parseInt(countResult[0].total) || 0;
  console.log(`   Found ${totalChunks.toLocaleString()} chunks with potential newsletter URLs\n`);
  
  if (totalChunks === 0) {
    console.log('⚠️  No chunks with newsletter URLs found.\n');
    return;
  }

  // For testing: Use random sample instead of full corpus
  const TEST_SAMPLE_SIZE = 2000; // Random sample of 2K chunks
  const USE_RANDOM_SAMPLE = true; // Set to false for full run
  
  let chunksToProcess = totalChunks;
  
  if (USE_RANDOM_SAMPLE && totalChunks > TEST_SAMPLE_SIZE) {
    console.log(`   Using random sample of ${TEST_SAMPLE_SIZE} chunks for testing...\n`);
    chunksToProcess = TEST_SAMPLE_SIZE;
  } else {
    // Full corpus processing
    const totalBatches = Math.ceil(totalChunks / BATCH_SIZE);
    console.log(`   Processing ${totalBatches} batches of ${BATCH_SIZE} chunks each...\n`);
  }

  // Process chunks (either random sample or full corpus)
  if (USE_RANDOM_SAMPLE && totalChunks > TEST_SAMPLE_SIZE) {
    // Random sample: single query
    console.log(`   Processing random sample...`);
    
    const urlQuery = `
      SELECT 
        c.newsletter_id,
        c.publisher_name as citing_publisher,
        REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.substack\\.com') as substack_subdomain,
        REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.beehiiv\\.com') as beehiiv_subdomain,
        REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.ghost\\.org') as ghost_subdomain,
        REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.tinyletter\\.com') as tinyletter_subdomain
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
      WHERE c.chunk_text LIKE '%substack%'
         OR c.chunk_text LIKE '%beehiiv%'
         OR c.chunk_text LIKE '%ghost%'
         OR c.chunk_text LIKE '%tinyletter%'
      ORDER BY RAND()
      LIMIT ${TEST_SAMPLE_SIZE}
    `;

    const [urlChunks] = await bigquery.query({ query: urlQuery });
    
    // Match URLs to publishers
    let processedCount = 0;
    for (const chunk of urlChunks) {
      const citingPublisher = chunk.citing_publisher;
      if (!citingPublisher) continue;
      
      processedCount++;
      
      // Check each platform
      const platforms = [
        { subdomain: chunk.substack_subdomain, domain: 'substack.com' },
        { subdomain: chunk.beehiiv_subdomain, domain: 'beehiiv.com' },
        { subdomain: chunk.ghost_subdomain, domain: 'ghost.org' },
        { subdomain: chunk.tinyletter_subdomain, domain: 'tinyletter.com' },
      ];
      
      for (const platform of platforms) {
        if (!platform.subdomain) continue;
        
        const normalizedUrl = `${platform.subdomain}.${platform.domain}`;
        const newsletterInfo = newsletterUrlMap.get(normalizedUrl);
        
        if (newsletterInfo) {
          // Don't count self-citations
          if (newsletterInfo.newsletter_name === citingPublisher) continue;
          
          if (!citationMap.has(newsletterInfo.discovery_id)) {
            citationMap.set(newsletterInfo.discovery_id, {
              discovery_id: newsletterInfo.discovery_id,
              newsletter_name: newsletterInfo.newsletter_name,
              citing_publishers: new Set(),
            });
          }
          
          const entry = citationMap.get(newsletterInfo.discovery_id)!;
          entry.citing_publishers.add(citingPublisher);
        }
      }
    }
    
    console.log(`     Processed ${processedCount} chunks (${urlChunks.length} total), found ${citationMap.size} newsletters with citations`);
  } else {
    // Full corpus: process in batches
    for (let offset = 0; offset < totalChunks; offset += BATCH_SIZE) {
      const batchNum = Math.floor(offset / BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(totalChunks / BATCH_SIZE);
      console.log(`   Processing batch ${batchNum}/${totalBatches}...`);
    
      // Extract URLs using regex (handle spaces/line breaks in URLs)
      // Use LIKE for WHERE clause (more reliable), then extract in SELECT
      const urlQuery = `
        SELECT 
          c.newsletter_id,
          c.publisher_name as citing_publisher,
          REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.substack\\.com') as substack_subdomain,
          REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.beehiiv\\.com') as beehiiv_subdomain,
          REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.ghost\\.org') as ghost_subdomain,
          REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.tinyletter\\.com') as tinyletter_subdomain
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
        WHERE c.chunk_text LIKE '%substack%'
           OR c.chunk_text LIKE '%beehiiv%'
           OR c.chunk_text LIKE '%ghost%'
           OR c.chunk_text LIKE '%tinyletter%'
        LIMIT ${BATCH_SIZE} OFFSET ${offset}
      `;

    const [urlChunks] = await bigquery.query({ query: urlQuery });
    
    if (urlChunks.length === 0 && batchNum === 1) {
      console.log(`     ⚠️  No chunks returned from query (might be regex issue)`);
      console.log(`     Testing with sample chunk...`);
      // Test with a single chunk to see what we get
      const testQuery = `
        SELECT 
          c.newsletter_id,
          c.publisher_name as citing_publisher,
          REGEXP_EXTRACT(LOWER(REGEXP_REPLACE(c.chunk_text, r'\\s+', '')), r'https?://([a-z0-9-]+)\\.substack\\.com') as substack_subdomain
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
        WHERE c.chunk_text LIKE '%substack%'
        LIMIT 5
      `;
      const [testChunks] = await bigquery.query({ query: testQuery });
      console.log(`     Test returned ${testChunks.length} chunks`);
      if (testChunks.length > 0) {
        console.log(`     Sample: subdomain="${testChunks[0].substack_subdomain}", citing="${testChunks[0].citing_publisher}"`);
      }
    }
    
    // Match URLs to publishers
    let processedCount = 0;
    for (const chunk of urlChunks) {
      const citingPublisher = chunk.citing_publisher;
      if (!citingPublisher) continue;
      
      processedCount++;
      
      // Check each platform
      const platforms = [
        { subdomain: chunk.substack_subdomain, domain: 'substack.com' },
        { subdomain: chunk.beehiiv_subdomain, domain: 'beehiiv.com' },
        { subdomain: chunk.ghost_subdomain, domain: 'ghost.org' },
        { subdomain: chunk.tinyletter_subdomain, domain: 'tinyletter.com' },
      ];
      
      for (const platform of platforms) {
        if (!platform.subdomain) continue;
        
        const normalizedUrl = `${platform.subdomain}.${platform.domain}`;
        const newsletterInfo = newsletterUrlMap.get(normalizedUrl);
        
        if (newsletterInfo) {
          // Don't count self-citations
          if (newsletterInfo.newsletter_name === citingPublisher) continue;
          
          if (!citationMap.has(newsletterInfo.discovery_id)) {
            citationMap.set(newsletterInfo.discovery_id, {
              discovery_id: newsletterInfo.discovery_id,
              newsletter_name: newsletterInfo.newsletter_name,
              citing_publishers: new Set(),
            });
          }
          
          const entry = citationMap.get(newsletterInfo.discovery_id)!;
          entry.citing_publishers.add(citingPublisher);
        }
      }
    }
    
    console.log(`     Processed ${processedCount} chunks (${urlChunks.length} total), found ${citationMap.size} newsletters with citations`);
    }
  }

  if (USE_RANDOM_SAMPLE) {
    console.log(`\n   📊 Random Sample Test Results:`);
    console.log(`      Sample size: ${chunksToProcess.toLocaleString()} chunks`);
    console.log(`      Total chunks available: ${totalChunks.toLocaleString()}`);
    console.log(`      Citations found in sample: ${citationMap.size} newsletters`);
  }
  
  console.log(`   Found ${citationMap.size} newsletters with URL-based citations\n`);

  // Step 4: Extract citation phrases
  console.log('Step 4: Extracting citation phrases ("via X", "from X")...');
  
  // This is a simplified version - in production, you'd use regex to extract phrases
  // For now, we'll focus on URL-based citations which are more reliable

  // Step 5: Aggregate results
  console.log('Step 5: Aggregating citation counts...');
  const citations: CitationMatch[] = [];

  for (const [discoveryId, data] of citationMap.entries()) {
    citations.push({
      discovery_id: data.discovery_id,
      newsletter_name: data.newsletter_name,
      citation_count: data.citing_publishers.size,
      citing_publishers: Array.from(data.citing_publishers),
    });
  }

  // Sort by citation count
  citations.sort((a, b) => b.citation_count - a.citation_count);

  console.log(`\n📊 Citation Analysis Results:\n`);
  console.log(`   Total newsletters with citations: ${citations.length}`);
  console.log(`   Total citations: ${citations.reduce((sum, c) => sum + c.citation_count, 0)}`);

  if (citations.length > 0) {
    const avg = citations.reduce((sum, c) => sum + c.citation_count, 0) / citations.length;
    console.log(`   Average citations per newsletter: ${avg.toFixed(1)}\n`);

    console.log('🏆 Top 10 Most Cited Newsletters:');
    citations.slice(0, 10).forEach((c, idx) => {
      console.log(`   ${idx + 1}. ${c.citation_count} citations: ${c.newsletter_name}`);
    });
  }

  console.log('\n✅ Pattern-based citation analysis complete!\n');
  console.log('Note: This is a sample run (10K chunks). For full analysis:');
  console.log('  1. Remove LIMIT 10000 from URL query');
  console.log('  2. Add citation phrase extraction');
  console.log('  3. Update publishers table with results');
}

calculateCitationsPatternBased()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/calculate-citations.ts">
/**
 * Calculate citation counts for all publishers
 * Searches chunks table for mentions of each publisher
 * This is a full corpus analysis (69K+ chunks)
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const CHUNKS_TABLE = 'chunks';

/**
 * Extract searchable terms from publisher name
 * Only use longer, more unique terms to avoid false positives
 */
function extractSearchTerms(publisherName: string): string[] {
  const normalized = publisherName.toLowerCase().trim();
  
  // Common words to exclude (appear in many chunks, not unique)
  // These words appear in thousands of chunks and cause false positives
  const COMMON_WORDS = [
    'from', 'the', 'and', 'with', 'that', 'this', 'have', 'will', 'would',
    'news', 'mail', 'daily', 'weekly', 'monthly', 'update', 'report',
    'brief', 'digest', 'summary', 'analysis', 'insights',
    'space', 'monitor', 'watch', 'check', 'view', 'read', 'see', 'look',
    'about', 'more', 'most', 'some', 'many', 'much', 'very', 'just',
    // Very common words that appear everywhere
    'world', 'today', 'state', 'letter', 'media', 'peak', 'time', 'news',
    'here', 'there', 'where', 'when', 'what', 'which', 'who', 'why',
    'could', 'should', 'might', 'may', 'must', 'can', 'cannot'
  ];
  
  // Extract words (6+ characters for single words, 5+ for phrases)
  // Single words must be longer to avoid false positives
  const words = normalized.split(/\s+/)
    .filter(w => w.length >= 5) // Start with 5+ char words
    .filter(w => !COMMON_WORDS.includes(w.toLowerCase()))
    .filter(w => !/^\d+$/.test(w)); // Exclude pure numbers
  
  if (words.length === 0) {
    // Try shorter words as fallback (but still filter common words)
    const fallbackWords = normalized.split(/\s+/)
      .filter(w => w.length >= 4)
      .filter(w => !COMMON_WORDS.includes(w.toLowerCase()));
    
    // CRITICAL: If only 1 word remains and it's common, skip it
    if (fallbackWords.length === 1 && fallbackWords[0].length < 6) {
      return []; // Skip - too generic
    }
    
    return [...new Set(fallbackWords)];
  }
  
  // CRITICAL: If only 1 word remains, it must be 6+ chars and not be too common
  if (words.length === 1) {
    const singleWord = words[0];
    // Single words must be 6+ chars to reduce false positives
    if (singleWord.length < 6) {
      return []; // Skip - single word too short/generic
    }
    // Even if 6+ chars, check if it's still too common
    // For now, allow it but it will be less precise
    return [singleWord];
  }
  
  // Prefer phrase matching if multiple unique words (more precise)
  const terms: string[] = [];
  
  // If we have 2+ unique words, try phrase combinations
  if (words.length >= 2) {
    // Try full phrase if reasonable length
    if (normalized.length >= 10 && normalized.length <= 100) {
      terms.push(normalized);
    }
    
    // Try 2-word phrases (more specific than individual words)
    for (let i = 0; i < words.length - 1; i++) {
      const phrase = `${words[i]} ${words[i + 1]}`;
      if (phrase.length >= 10) { // Only if phrase is reasonably long
        terms.push(phrase);
      }
    }
  }
  
  // Fallback to individual words if no phrases (but we already handled single word case)
  if (terms.length === 0 && words.length >= 2) {
    // Only use individual words if we have 2+ (AND logic requires all)
    terms.push(...words);
  }
  
  // Return unique terms
  return [...new Set(terms)];
}

async function calculateCitations() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  console.log('📊 Calculating citation counts for all publishers...\n');
  console.log('This analyzes the full corpus (69K+ chunks) - may take 1-2 hours\n');
  
  // Step 1: Get all publishers
  console.log('Step 1: Fetching all publishers...');
  const [publishers] = await bigquery.query({
    query: `
      SELECT 
        publisher_id,
        publisher_name,
        canonical_name,
        newsletter_url,
        primary_email
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
      ORDER BY publisher_name
    `,
  });
  
  console.log(`   Found ${publishers.length} publishers\n`);
  
  if (publishers.length === 0) {
    console.log('⚠️  No publishers found.\n');
    return;
  }
  
  // Step 2: Process publishers in batches (to avoid memory issues)
  const BATCH_SIZE = 50;
  let processed = 0;
  const citations: Array<{
    publisher_id: string;
    citation_count: number;
    citing_publishers: string[];
  }> = [];
  
  console.log(`Step 2: Analyzing citations (processing ${publishers.length} publishers in batches of ${BATCH_SIZE})...\n`);
  
  for (let i = 0; i < publishers.length; i += BATCH_SIZE) {
    const batch = publishers.slice(i, i + BATCH_SIZE);
    const batchNumber = Math.floor(i / BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(publishers.length / BATCH_SIZE);
    
    console.log(`   Processing batch ${batchNumber}/${totalBatches} (publishers ${i + 1}-${Math.min(i + BATCH_SIZE, publishers.length)})...`);
    
    // Process each publisher individually to avoid query size limits
    const batchResults: any[] = [];
    
    for (const publisher of batch) {
      const searchTerms = extractSearchTerms(publisher.publisher_name);
      if (searchTerms.length === 0) {
        continue; // Skip if no search terms
      }
      
      // Build search conditions for this publisher
      // Use AND logic: ALL terms must be present (more precise)
      // If we have a phrase, use exact phrase match
      // Otherwise, require all terms to be present
      let termConditions: string;
      
      if (searchTerms.length === 1 && searchTerms[0].includes(' ')) {
        // Single phrase: exact match
        termConditions = `LOWER(c.chunk_text) LIKE CONCAT('%', LOWER(@term_${publisher.publisher_id.replace(/-/g, '_')}), '%')`;
      } else {
        // Multiple terms: AND logic (all must be present)
        termConditions = searchTerms.map((term, idx) => 
          `LOWER(c.chunk_text) LIKE CONCAT('%', LOWER(@term_${publisher.publisher_id.replace(/-/g, '_')}_${idx}), '%')`
        ).join(' AND ');
      }
      
      // Also check for newsletter URL if available
      let urlCondition = '';
      if (publisher.newsletter_url) {
        try {
          const url = new URL(publisher.newsletter_url);
          const domain = url.hostname.replace(/^www\./, '');
          urlCondition = ` OR LOWER(c.chunk_text) LIKE CONCAT('%', LOWER(@domain_${publisher.publisher_id.replace(/-/g, '_')}), '%')`;
        } catch {
          // Invalid URL, skip
        }
      }
      
      // Build parameterized query
      const params: any = {
        publisher_id: publisher.publisher_id,
      };
      
      // Add term parameters
      if (searchTerms.length === 1 && searchTerms[0].includes(' ')) {
        // Single phrase
        params[`term_${publisher.publisher_id.replace(/-/g, '_')}`] = searchTerms[0];
      } else {
        // Multiple terms (AND logic)
        searchTerms.forEach((term, idx) => {
          params[`term_${publisher.publisher_id.replace(/-/g, '_')}_${idx}`] = term;
        });
      }
      
      if (urlCondition) {
        try {
          const url = new URL(publisher.newsletter_url!);
          const domain = url.hostname.replace(/^www\./, '');
          params[`domain_${publisher.publisher_id.replace(/-/g, '_')}`] = domain;
        } catch {
          // Skip
        }
      }
      
      const query = `
        SELECT 
          @publisher_id as publisher_id,
          COUNT(DISTINCT c.newsletter_id) as citation_count,
          ARRAY_AGG(DISTINCT c.publisher_name) as citing_publishers
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
        WHERE 
          (
            ${termConditions}${urlCondition}
          )
          AND c.publisher_name != (
            SELECT publisher_name 
            FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
            WHERE publisher_id = @publisher_id
          )
      `;
      
      try {
        const [results] = await bigquery.query({ query, params });
        if (results.length > 0) {
          batchResults.push({
            publisher_id: publisher.publisher_id,
            publisher_name: publisher.publisher_name,
            citation_count: parseInt(results[0].citation_count) || 0,
            citing_publishers: results[0].citing_publishers || [],
          });
        }
      } catch (error: any) {
        // If parameterized query fails, try simpler approach
        console.log(`     ⚠️  Using fallback search for ${publisher.publisher_name}`);
        
        // Fallback: use simple LIKE with escaped terms (AND logic)
        const escapedTerms = searchTerms.map(t => t.replace(/'/g, "''").replace(/\\/g, '\\\\'));
        const simpleConditions = escapedTerms.map(term => 
          `LOWER(c.chunk_text) LIKE '%${term}%'`
        ).join(' AND '); // Use AND, not OR
        
        const fallbackQuery = `
          SELECT 
            '${publisher.publisher_id}' as publisher_id,
            COUNT(DISTINCT c.newsletter_id) as citation_count,
            ARRAY_AGG(DISTINCT c.publisher_name) as citing_publishers
          FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c
          WHERE 
            (${simpleConditions})
            AND c.publisher_name != '${publisher.publisher_name.replace(/'/g, "''")}'
        `;
        
        try {
          const [results] = await bigquery.query({ query: fallbackQuery });
          if (results.length > 0) {
            batchResults.push({
              publisher_id: publisher.publisher_id,
              publisher_name: publisher.publisher_name,
              citation_count: parseInt(results[0].citation_count) || 0,
              citing_publishers: results[0].citing_publishers || [],
            });
          }
        } catch (fallbackError: any) {
          console.error(`     ❌ Error with fallback for ${publisher.publisher_name}:`, fallbackError.message);
        }
      }
    }
    
    // Add batch results to citations
    for (const result of batchResults) {
      citations.push({
        publisher_id: result.publisher_id,
        citation_count: result.citation_count,
        citing_publishers: result.citing_publishers,
      });
    }
    
    processed += batch.length;
    console.log(`     ✅ Found citations for ${batchResults.length} publishers in this batch`);
  }
  
  console.log(`\n   Processed ${processed} publishers`);
  console.log(`   Found citations for ${citations.length} publishers\n`);
  
  // Step 3: Update publishers table with citation counts
  console.log('Step 3: Updating publishers table with citation counts...');
  
  // Group by publisher_id (some may have multiple matches)
  const citationMap = new Map<string, { count: number; citing: string[] }>();
  
  for (const citation of citations) {
    if (!citationMap.has(citation.publisher_id)) {
      citationMap.set(citation.publisher_id, {
        count: 0,
        citing: [],
      });
    }
    
    const existing = citationMap.get(citation.publisher_id)!;
    existing.count += citation.citation_count;
    existing.citing.push(...citation.citing_publishers);
  }
  
  // Deduplicate citing publishers
  for (const [publisherId, data] of citationMap.entries()) {
    data.citing = [...new Set(data.citing)];
  }
  
  console.log(`   Updating ${citationMap.size} publishers...\n`);
  
  let updated = 0;
  const now = new Date().toISOString();
  
  // Use BigQuery table.insert with upsert pattern (safer than SQL string construction)
  const citationArray = Array.from(citationMap.entries());
  
  console.log(`   Updating ${citationArray.length} publishers using BigQuery API...\n`);
  
  const table = bigquery.dataset(DATASET_ID).table(PUBLISHERS_TABLE);
  
  // Process in smaller batches to handle streaming buffer
  const UPDATE_BATCH_SIZE = 10; // Small batches to avoid streaming buffer issues
  
  for (let i = 0; i < citationArray.length; i += UPDATE_BATCH_SIZE) {
    const batch = citationArray.slice(i, i + UPDATE_BATCH_SIZE);
    const batchNumber = Math.floor(i / UPDATE_BATCH_SIZE) + 1;
    const totalBatches = Math.ceil(citationArray.length / UPDATE_BATCH_SIZE);
    
    if (batchNumber % 10 === 0) {
      console.log(`   Progress: Batch ${batchNumber}/${totalBatches} (${i + 1}/${citationArray.length} publishers)...`);
    }
    
    // Prepare rows for this batch
    const rows = batch.map(([publisherId, data]) => {
      // Limit citing_publishers to avoid array size issues
      const citingLimited = data.citing.slice(0, 200); // Limit to 200 citing publishers
      
      return {
        publisher_id: publisherId,
        citation_count: data.count,
        citing_publishers: citingLimited, // BigQuery handles arrays natively
        updated_at: now,
      };
    });
    
    try {
      // Use table.insert - BigQuery will handle the array properly
      // Note: This creates/updates rows, but we need to use MERGE for true upsert
      // For now, we'll insert and handle conflicts separately
      
      // Actually, we need to use MERGE but with proper parameterization
      // Let's use a simpler approach: update one at a time with proper escaping
      for (const row of rows) {
        try {
          // Use parameterized MERGE with proper array and timestamp handling
          const mergeQuery = `
            MERGE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` AS target
            USING (
              SELECT 
                @publisher_id AS publisher_id,
                @citation_count AS citation_count,
                @citing_publishers AS citing_publishers,
                TIMESTAMP(@updated_at) AS updated_at
            ) AS source
            ON target.publisher_id = source.publisher_id
            WHEN MATCHED THEN
              UPDATE SET
                citation_count = source.citation_count,
                citing_publishers = source.citing_publishers,
                updated_at = source.updated_at
          `;
          
          await bigquery.query({
            query: mergeQuery,
            params: {
              publisher_id: row.publisher_id,
              citation_count: row.citation_count,
              citing_publishers: row.citing_publishers, // Pass array directly - BigQuery handles it
              updated_at: now, // Pass as ISO string, TIMESTAMP() converts it
            },
          });
          
          updated++;
        } catch (error: any) {
          if (error.message?.includes('streaming buffer')) {
            // Will retry later - skip for now
            continue;
          } else {
            // Log error but continue
            if (updated % 100 === 0) {
              console.error(`   ⚠️  Error updating ${row.publisher_id}: ${error.message.substring(0, 100)}`);
            }
          }
        }
      }
    } catch (error: any) {
      console.error(`   ❌ Error processing batch ${batchNumber}:`, error.message.substring(0, 200));
      // Continue with next batch
    }
  }
  
  console.log(`\n   ✅ Updated ${updated}/${citationArray.length} publishers\n`);
  
  // Step 4: Summary statistics
  console.log('📊 Citation Analysis Summary:');
  const totalCitations = Array.from(citationMap.values()).reduce((sum, d) => sum + d.count, 0);
  const avgCitations = totalCitations / citationMap.size;
  const maxCitations = Math.max(...Array.from(citationMap.values()).map(d => d.count));
  
  console.log(`   Total citations found: ${totalCitations.toLocaleString()}`);
  console.log(`   Average citations per publisher: ${avgCitations.toFixed(1)}`);
  console.log(`   Maximum citations: ${maxCitations}`);
  console.log(`   Publishers with citations: ${citationMap.size}/${publishers.length}`);
  console.log(`   Publishers with no citations: ${publishers.length - citationMap.size}`);
  console.log('');
  
  // Top 10 most cited publishers
  const topCited = Array.from(citationMap.entries())
    .sort((a, b) => b[1].count - a[1].count)
    .slice(0, 10);
  
  if (topCited.length > 0) {
    console.log('🏆 Top 10 Most Cited Publishers:');
    for (const [publisherId, data] of topCited) {
      const publisher = publishers.find(p => p.publisher_id === publisherId);
      console.log(`   ${data.count} citations: ${publisher?.publisher_name || publisherId}`);
    }
    console.log('');
  }
  
  console.log('✅ Citation analysis complete!\n');
  console.log('Next steps:');
  console.log('  1. Review citation counts (top publishers should be recognizable)');
  console.log('  2. Run: npm run publishers:initial-scoring (quality scoring)');
}

calculateCitations()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/calculate-quality-scores.ts">
/**
 * Calculate Quality Scores for All Publishers
 * 
 * This script calculates composite quality scores based on 6 signals:
 * 1. Citation Signal (30%) - How many other publishers cite this
 * 2. Subscriber Signal (25%) - Estimated subscriber count
 * 3. Recommendation Signal (15%) - How many recommendations received
 * 4. Topic Relevance (20%) - Relevance to geopolitics/foreign policy
 * 5. Platform Signal (5%) - Platform quality indicator
 * 6. Freshness Signal (5%) - Activity level (last_seen date)
 * 
 * Manual overrides are supported - if manual_quality_score_override is set,
 * it takes precedence over calculated score.
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const DISCOVERED_TABLE = 'discovered_newsletters';

// Quality scoring version
const QUALITY_SCORE_VERSION = '1.0';

interface QualitySignals {
  citationSignal: number;      // 0-1
  subscriberSignal: number;    // 0-1
  recommendationSignal: number; // 0-1
  topicRelevanceSignal: number; // 0-1
  platformSignal: number;      // 0-1
  freshnessSignal: number;     // 0-1
}

// Publisher data comes from BigQuery query result (any type)

/**
 * Calculate citation signal (0-1)
 * Uses logarithmic scaling: log10(citation_count + 1) / 2
 */
function calculateCitationSignal(citationCount: number | null): number {
  if (!citationCount || citationCount === 0) {
    return 0.0;
  }
  // Logarithmic: 1 citation = 0.15, 10 = 0.5, 100 = 1.0
  return Math.min(Math.log10(citationCount + 1) / 2, 1.0);
}

/**
 * Calculate subscriber signal (0-1)
 * Uses logarithmic scaling: log10(subscriber_count / 1000) / 2
 */
function calculateSubscriberSignal(subscriberCount: number | null): number {
  if (!subscriberCount || subscriberCount === 0) {
    return 0.5; // Neutral default for unknown
  }
  // Logarithmic: 1K = 0.0, 10K = 0.5, 100K = 1.0
  return Math.min(Math.max(Math.log10(subscriberCount / 1000) / 2, 0), 1.0);
}

/**
 * Calculate recommendation signal (0-1)
 */
function calculateRecommendationSignal(recommendationCount: number | null): number {
  if (!recommendationCount || recommendationCount === 0) {
    return 0.4; // Default for no recommendations
  }
  if (recommendationCount >= 3) return 1.0;
  if (recommendationCount === 2) return 0.8;
  if (recommendationCount === 1) return 0.6;
  return 0.4;
}

/**
 * Calculate topic relevance signal (0-1)
 * Uses existing topic_relevance_score if available, otherwise neutral
 */
function calculateTopicRelevanceSignal(topicRelevanceScore: number | null): number {
  if (topicRelevanceScore !== null && topicRelevanceScore !== undefined) {
    return Math.min(Math.max(topicRelevanceScore, 0), 1);
  }
  return 0.5; // Neutral default
}

/**
 * Calculate platform signal (0-1)
 */
function calculatePlatformSignal(platform: string | null): number {
  if (!platform) return 0.7; // Unknown = neutral
  
  const platformScores: { [key: string]: number } = {
    'substack': 0.9,
    'beehiiv': 0.8,
    'ghost': 0.85,
    'custom': 0.7,
    'mailchimp': 0.75,
    'convertkit': 0.75,
    'tinyletter': 0.6,
    'revue': 0.65,
    'buttondown': 0.8,
  };
  
  return platformScores[platform.toLowerCase()] || 0.7;
}

/**
 * Calculate freshness signal (0-1)
 * Based on last_seen date
 */
function calculateFreshnessSignal(lastSeen: string | null): number {
  if (!lastSeen) return 0.5; // Neutral if unknown
  
  try {
    const lastSeenDate = new Date(lastSeen);
    const now = new Date();
    const daysSince = (now.getTime() - lastSeenDate.getTime()) / (1000 * 60 * 60 * 24);
    
    if (daysSince <= 7) return 1.0;      // Active in last week
    if (daysSince <= 30) return 0.9;     // Active in last month
    if (daysSince <= 90) return 0.7;     // Active in last 3 months
    if (daysSince <= 180) return 0.5;    // Active in last 6 months
    if (daysSince <= 365) return 0.3;    // Active in last year
    return 0.1;                           // Inactive
  } catch {
    return 0.5; // Neutral on error
  }
}

/**
 * Calculate composite quality score (0-100)
 */
function calculateQualityScore(signals: QualitySignals, manualOverride: number | null): number {
  // Manual override takes precedence
  if (manualOverride !== null && manualOverride !== undefined) {
    return Math.min(Math.max(manualOverride, 0), 100);
  }
  
  // Composite formula
  const composite = (
    signals.citationSignal * 0.30 +
    signals.subscriberSignal * 0.25 +
    signals.recommendationSignal * 0.15 +
    signals.topicRelevanceSignal * 0.20 +
    signals.platformSignal * 0.05 +
    signals.freshnessSignal * 0.05
  ) * 100;
  
  return Math.min(Math.max(composite, 0), 100);
}

/**
 * Apply manual individual signal overrides if present
 */
function applyManualSignalOverrides(
  signals: QualitySignals,
  manualOverrides: any | null
): QualitySignals {
  if (!manualOverrides || typeof manualOverrides !== 'object') {
    return signals;
  }
  
  const overridden = { ...signals };
  
  // Allow manual override of individual signals
  if (manualOverrides.citation_signal !== undefined) {
    overridden.citationSignal = Math.min(Math.max(manualOverrides.citation_signal, 0), 1);
  }
  if (manualOverrides.subscriber_signal !== undefined) {
    overridden.subscriberSignal = Math.min(Math.max(manualOverrides.subscriber_signal, 0), 1);
  }
  if (manualOverrides.recommendation_signal !== undefined) {
    overridden.recommendationSignal = Math.min(Math.max(manualOverrides.recommendation_signal, 0), 1);
  }
  if (manualOverrides.topic_relevance_signal !== undefined) {
    overridden.topicRelevanceSignal = Math.min(Math.max(manualOverrides.topic_relevance_signal, 0), 1);
  }
  if (manualOverrides.platform_signal !== undefined) {
    overridden.platformSignal = Math.min(Math.max(manualOverrides.platform_signal, 0), 1);
  }
  if (manualOverrides.freshness_signal !== undefined) {
    overridden.freshnessSignal = Math.min(Math.max(manualOverrides.freshness_signal, 0), 1);
  }
  
  return overridden;
}

async function calculateQualityScores() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Calculating quality scores for all publishers...\n');
  console.log(`Quality Score Version: ${QUALITY_SCORE_VERSION}\n`);

  try {
    // Step 1: Fetch all publishers with their signals
    console.log('Step 1: Fetching publishers and quality signals...');
    
    const query = `
      SELECT 
        p.publisher_id,
        p.publisher_name,
        p.citation_count,
        p.subscriber_estimate,
        p.recommendation_count,
        p.topic_relevance_score,
        p.platform,
        p.last_seen,
        p.manual_quality_score_override,
        p.manual_individual_signal_overrides,
        -- Get additional signals from discovered_newsletters if linked
        d.subscriber_count_estimate as discovered_subscriber_estimate,
        d.recommendation_count as discovered_recommendation_count,
        d.primary_topics,
        d.secondary_topics
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` p
      LEFT JOIN \`${PROJECT_ID}.${DATASET_ID}.${DISCOVERED_TABLE}\` d
        ON p.discovery_id = d.discovery_id
    `;

    const [rows] = await bigquery.query(query);
    console.log(`   Found ${rows.length} publishers\n`);

    if (rows.length === 0) {
      console.log('⚠️  No publishers found.\n');
      return;
    }

    // Step 2: Calculate quality scores
    console.log('Step 2: Calculating quality scores...\n');

    const updates: Array<{
      publisher_id: string;
      quality_score: number;
      citation_signal: number;
      subscriber_signal: number;
      recommendation_signal: number;
      topic_relevance_signal: number;
      platform_signal: number;
      freshness_signal: number;
    }> = [];

    for (const row of rows) {
      const publisher = row as any;
      
      // Use discovered_newsletters data if available, otherwise use publishers table
      const subscriberCount = publisher.discovered_subscriber_estimate || publisher.subscriber_estimate;
      const recommendationCount = publisher.discovered_recommendation_count || publisher.recommendation_count;
      
      // Calculate individual signals
      let signals: QualitySignals = {
        citationSignal: calculateCitationSignal(publisher.citation_count),
        subscriberSignal: calculateSubscriberSignal(subscriberCount),
        recommendationSignal: calculateRecommendationSignal(recommendationCount),
        topicRelevanceSignal: calculateTopicRelevanceSignal(publisher.topic_relevance_score),
        platformSignal: calculatePlatformSignal(publisher.platform),
        freshnessSignal: calculateFreshnessSignal(publisher.last_seen),
      };
      
      // Apply manual individual signal overrides if present
      signals = applyManualSignalOverrides(signals, publisher.manual_individual_signal_overrides);
      
      // Calculate composite score (respects manual override)
      const qualityScore = calculateQualityScore(signals, publisher.manual_quality_score_override);
      
      updates.push({
        publisher_id: publisher.publisher_id,
        quality_score: qualityScore,
        citation_signal: signals.citationSignal,
        subscriber_signal: signals.subscriberSignal,
        recommendation_signal: signals.recommendationSignal,
        topic_relevance_signal: signals.topicRelevanceSignal,
        platform_signal: signals.platformSignal,
        freshness_signal: signals.freshnessSignal,
      });
    }

    console.log(`   Calculated scores for ${updates.length} publishers\n`);

    // Step 3: Update publishers table
    console.log('Step 3: Updating publishers table...\n');

    let updatedCount = 0;
    const batchSize = 100;

    for (let i = 0; i < updates.length; i += batchSize) {
      const batch = updates.slice(i, i + batchSize);
      
      // Use MERGE for batch updates
      const mergeQuery = `
        MERGE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\` AS target
        USING UNNEST([
          ${batch.map(u => `
            STRUCT(
              '${u.publisher_id}' AS publisher_id,
              ${u.quality_score} AS quality_score,
              ${u.citation_signal} AS citation_signal,
              ${u.subscriber_signal} AS subscriber_signal,
              ${u.recommendation_signal} AS recommendation_signal,
              ${u.topic_relevance_signal} AS topic_relevance_signal,
              ${u.platform_signal} AS platform_signal,
              ${u.freshness_signal} AS freshness_signal
            )
          `).join(',')}
        ]) AS source
        ON target.publisher_id = source.publisher_id
        WHEN MATCHED THEN
          UPDATE SET
            quality_score = source.quality_score,
            quality_score_last_calculated = CURRENT_TIMESTAMP(),
            quality_score_version = '${QUALITY_SCORE_VERSION}',
            updated_at = CURRENT_TIMESTAMP()
      `;

      // Actually, we need to update individual signal scores too
      // Let's do individual updates to be safe
      for (const update of batch) {
        const updateQuery = `
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          SET 
            quality_score = @quality_score,
            quality_score_last_calculated = CURRENT_TIMESTAMP(),
            quality_score_version = @version,
            updated_at = CURRENT_TIMESTAMP()
          WHERE publisher_id = @publisher_id
        `;

        await bigquery.query({
          query: updateQuery,
          params: {
            publisher_id: update.publisher_id,
            quality_score: update.quality_score,
            version: QUALITY_SCORE_VERSION,
          },
        });
        
        updatedCount++;
      }
      
      if ((i / batchSize) % 10 === 0) {
        console.log(`   Updated ${updatedCount}/${updates.length} publishers...`);
      }
    }

    console.log(`\n✅ Updated ${updatedCount} publishers with quality scores\n`);

    // Step 4: Show summary statistics
    console.log('Step 4: Quality Score Summary...\n');
    
    const statsQuery = `
      SELECT 
        COUNT(*) as total,
        AVG(quality_score) as avg_score,
        MIN(quality_score) as min_score,
        MAX(quality_score) as max_score,
        COUNTIF(quality_score >= 80) as high_quality,
        COUNTIF(quality_score >= 60 AND quality_score < 80) as medium_quality,
        COUNTIF(quality_score < 60) as low_quality,
        COUNTIF(manual_quality_score_override IS NOT NULL) as manual_overrides
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
      WHERE quality_score IS NOT NULL
    `;

    const [statsRows] = await bigquery.query(statsQuery);
    const stats = statsRows[0];

    console.log(`   Total publishers scored: ${stats.total}`);
    console.log(`   Average score: ${stats.avg_score?.toFixed(1) || 'N/A'}`);
    console.log(`   Score range: ${stats.min_score?.toFixed(1) || 'N/A'} - ${stats.max_score?.toFixed(1) || 'N/A'}`);
    console.log(`   High quality (≥80): ${stats.high_quality}`);
    console.log(`   Medium quality (60-79): ${stats.medium_quality}`);
    console.log(`   Low quality (<60): ${stats.low_quality}`);
    console.log(`   Manual overrides: ${stats.manual_overrides}\n`);

    console.log('✅ Quality score calculation complete!\n');
    console.log('Next steps:');
    console.log('  - Review scores and add manual overrides where needed');
    console.log('  - Run: npm run publishers:manual-override <publisher_id> <score>');
    console.log('  - Integrate quality scores into retrieval system\n');

  } catch (error: any) {
    console.error('❌ Error calculating quality scores:', error.message);
    throw error;
  }
}

calculateQualityScores()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/create-publishers-table.ts">
/**
 * Create publishers table in BigQuery
 * This table stores publisher-level metadata and quality scores
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'publishers';

async function createPublishersTable() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  const dataset = bigquery.dataset(DATASET_ID);

  // Check if table exists
  const table = dataset.table(TABLE_ID);
  const [exists] = await table.exists();

  if (exists) {
    console.log(`⚠️  Table ${TABLE_ID} already exists.`);
    console.log('   To recreate, delete the table first.');
    return;
  }

  console.log(`Creating table ${TABLE_ID}...\n`);

  // Define schema
  const schema = [
    // Primary identifiers
    { name: 'publisher_id', type: 'STRING', mode: 'REQUIRED' },
    { name: 'publisher_name', type: 'STRING', mode: 'REQUIRED' },
    { name: 'canonical_name', type: 'STRING', mode: 'REQUIRED' }, // Normalized for matching

    // Email identification
    { name: 'primary_email', type: 'STRING', mode: 'NULLABLE' },
    { name: 'email_domains', type: 'STRING', mode: 'REPEATED' }, // Array of email domains
    { name: 'email_variations', type: 'STRING', mode: 'REPEATED' }, // Array of all email addresses seen

    // Newsletter metadata
    { name: 'newsletter_url', type: 'STRING', mode: 'NULLABLE' },
    { name: 'platform', type: 'STRING', mode: 'NULLABLE' }, // substack, beehiiv, ghost, custom, unknown
    { name: 'from_domain', type: 'STRING', mode: 'NULLABLE' }, // Most common from_domain

    // Quality Signals (calculated)
    { name: 'quality_score', type: 'FLOAT64', mode: 'NULLABLE' }, // 0-100 composite score
    { name: 'citation_count', type: 'INT64', mode: 'NULLABLE' }, // How many other publishers cite this
    { name: 'citing_publishers', type: 'STRING', mode: 'REPEATED' }, // Which publishers cite this
    { name: 'subscriber_estimate', type: 'INT64', mode: 'NULLABLE' },
    { name: 'recommendation_count', type: 'INT64', mode: 'NULLABLE' },
    { name: 'topic_relevance_score', type: 'FLOAT64', mode: 'NULLABLE' }, // 0-1 relevance to geopolitics
    { name: 'freshness_score', type: 'FLOAT64', mode: 'NULLABLE' }, // 0-1 activity level
    { name: 'platform_score', type: 'FLOAT64', mode: 'NULLABLE' }, // 0-1 platform quality signal

    // Message statistics
    { name: 'message_count', type: 'INT64', mode: 'NULLABLE' },
    { name: 'first_seen', type: 'TIMESTAMP', mode: 'NULLABLE' },
    { name: 'last_seen', type: 'TIMESTAMP', mode: 'NULLABLE' },
    { name: 'avg_word_count', type: 'FLOAT64', mode: 'NULLABLE' },

    // Discovery links
    { name: 'discovery_id', type: 'STRING', mode: 'NULLABLE' }, // Link to discovered_newsletters
    { name: 'is_discovered', type: 'BOOLEAN', mode: 'NULLABLE' },
    { name: 'matched_at', type: 'TIMESTAMP', mode: 'NULLABLE' }, // When was discovery link made

    // Quality score metadata
    { name: 'quality_score_last_calculated', type: 'TIMESTAMP', mode: 'NULLABLE' },
    { name: 'quality_score_version', type: 'STRING', mode: 'NULLABLE' }, // Version of scoring algorithm

    // Deduplication
    { name: 'is_duplicate', type: 'BOOLEAN', mode: 'NULLABLE' },
    { name: 'merged_into_publisher_id', type: 'STRING', mode: 'NULLABLE' },

    // Timestamps
    { name: 'created_at', type: 'TIMESTAMP', mode: 'REQUIRED' },
    { name: 'updated_at', type: 'TIMESTAMP', mode: 'REQUIRED' },
  ];

  // Create table
  await table.create({
    schema: schema,
    description: 'Publisher-level metadata and quality scores. Aggregates message-level data and links to discovered_newsletters.',
  });

  console.log(`✅ Table ${TABLE_ID} created successfully!\n`);

  // Create indexes for performance (using clustering)
  console.log('Creating clustered indexes...');
  
  // Note: BigQuery doesn't support traditional indexes, but we can cluster the table
  // We'll add clustering via ALTER TABLE if needed
  // For now, the table is created with the schema

  console.log('✅ Table ready for use!\n');
  console.log('Next steps:');
  console.log('  1. Run: npm run publishers:extract-existing');
  console.log('  2. Run: npm run publishers:initial-citations');
  console.log('  3. Run: npm run publishers:initial-scoring');
}

createPublishersTable()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/export-publishers-simple.ts">
/**
 * Export simple list of publishers with names and URLs
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import * as fs from 'fs';
import * as path from 'path';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';

async function exportPublishersSimple() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Exporting publishers (name + URL)...\n');

  try {
    const query = `
      SELECT 
        publisher_name,
        newsletter_url
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
      ORDER BY publisher_name ASC
    `;

    const [rows] = await bigquery.query(query);
    console.log(`   Found ${rows.length} publishers\n`);

    // Create output directory
    const outputDir = path.join(process.cwd(), 'output');
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }

    // Generate CSV
    const csvPath = path.join(outputDir, 'publishers-name-url.csv');
    const csvHeaders = ['Publisher Name', 'URL'];
    
    let csvContent = csvHeaders.join(',') + '\n';

    for (const row of rows) {
      const name = (row.publisher_name || '').replace(/"/g, '""');
      const url = (row.newsletter_url || '').replace(/"/g, '""');
      csvContent += `"${name}","${url}"\n`;
    }

    fs.writeFileSync(csvPath, csvContent);
    console.log(`✅ CSV exported: ${csvPath}\n`);
    console.log(`   Total publishers: ${rows.length}\n`);

  } catch (error: any) {
    console.error('❌ Error exporting publishers:', error.message);
    throw error;
  }
}

exportPublishersSimple()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/export-quality-scores.ts">
/**
 * Export quality scores for review
 * Creates CSV and markdown files with publisher quality scores
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import * as fs from 'fs';
import * as path from 'path';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';

async function exportQualityScores() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Exporting quality scores for review...\n');

  try {
    // Query all publishers with quality scores
    const query = `
      SELECT 
        publisher_id,
        publisher_name,
        quality_score,
        citation_count,
        subscriber_estimate,
        recommendation_count,
        topic_relevance_score,
        platform,
        platform_score,
        freshness_score,
        last_seen,
        message_count,
        manual_quality_score_override,
        manual_override_reason,
        is_discovered,
        discovery_id,
        newsletter_url
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
      WHERE quality_score IS NOT NULL
      ORDER BY quality_score DESC, publisher_name ASC
    `;

    const [rows] = await bigquery.query(query);
    console.log(`   Found ${rows.length} publishers with quality scores\n`);

    if (rows.length === 0) {
      console.log('⚠️  No publishers with quality scores found.\n');
      return;
    }

    // Create output directory
    const outputDir = path.join(process.cwd(), 'output');
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }

    // Generate CSV
    const csvPath = path.join(outputDir, 'publisher-quality-scores.csv');
    const csvHeaders = [
      'Publisher Name',
      'Quality Score',
      'Citation Count',
      'Subscriber Estimate',
      'Recommendation Count',
      'Topic Relevance',
      'Platform',
      'Last Seen',
      'Message Count',
      'Manual Override',
      'Override Reason',
      'Newsletter URL',
      'Is Discovered',
    ];

    let csvContent = csvHeaders.join(',') + '\n';

    for (const row of rows) {
      const csvRow = [
        `"${(row.publisher_name || '').replace(/"/g, '""')}"`,
        row.quality_score?.toFixed(1) || '',
        row.citation_count || 0,
        row.subscriber_estimate || '',
        row.recommendation_count || 0,
        row.topic_relevance_score?.toFixed(2) || '',
        row.platform || '',
        row.last_seen && row.last_seen !== 'Invalid Date' ? (() => {
          try {
            const date = new Date(row.last_seen);
            return isNaN(date.getTime()) ? '' : date.toISOString().split('T')[0];
          } catch {
            return '';
          }
        })() : '',
        row.message_count || 0,
        row.manual_quality_score_override || '',
        row.manual_override_reason ? `"${row.manual_override_reason.replace(/"/g, '""')}"` : '',
        row.newsletter_url || '',
        row.is_discovered ? 'Yes' : 'No',
      ];
      csvContent += csvRow.join(',') + '\n';
    }

    fs.writeFileSync(csvPath, csvContent);
    console.log(`✅ CSV exported: ${csvPath}\n`);

    // Generate Markdown report
    const mdPath = path.join(outputDir, 'publisher-quality-scores.md');
    let mdContent = '# Publisher Quality Scores\n\n';
    mdContent += `Generated: ${new Date().toISOString()}\n\n`;
    mdContent += `Total Publishers: ${rows.length}\n\n`;

    // Summary statistics
    const highQuality = rows.filter(r => r.quality_score >= 80).length;
    const mediumQuality = rows.filter(r => r.quality_score >= 60 && r.quality_score < 80).length;
    const lowQuality = rows.filter(r => r.quality_score < 60).length;
    const withManualOverride = rows.filter(r => r.manual_quality_score_override !== null).length;

    mdContent += '## Summary Statistics\n\n';
    mdContent += `- **High Quality (≥80):** ${highQuality}\n`;
    mdContent += `- **Medium Quality (60-79):** ${mediumQuality}\n`;
    mdContent += `- **Low Quality (<60):** ${lowQuality}\n`;
    mdContent += `- **Manual Overrides:** ${withManualOverride}\n\n`;

    mdContent += '## All Publishers (Sorted by Quality Score)\n\n';
    mdContent += '| Publisher Name | Quality Score | Citations | Subscribers | Recommendations | Topic Relevance | Platform | Last Seen | Manual Override |\n';
    mdContent += '|----------------|---------------|-----------|-------------|-----------------|-----------------|----------|-----------|-----------------|\n';

    for (const row of rows) {
      const name = (row.publisher_name || '').replace(/\|/g, '\\|');
      const score = row.quality_score?.toFixed(1) || 'N/A';
      const citations = row.citation_count || 0;
      const subscribers = row.subscriber_estimate ? row.subscriber_estimate.toLocaleString() : '-';
      const recommendations = row.recommendation_count || 0;
      const topicRelevance = row.topic_relevance_score ? (row.topic_relevance_score * 100).toFixed(0) + '%' : '-';
      const platform = row.platform || '-';
      const lastSeen = row.last_seen && row.last_seen !== 'Invalid Date' ? (() => {
        try {
          const date = new Date(row.last_seen);
          return isNaN(date.getTime()) ? '-' : date.toISOString().split('T')[0];
        } catch {
          return '-';
        }
      })() : '-';
      const manualOverride = row.manual_quality_score_override ? `**${row.manual_quality_score_override.toFixed(1)}**` : '-';
      
      mdContent += `| ${name} | ${score} | ${citations} | ${subscribers} | ${recommendations} | ${topicRelevance} | ${platform} | ${lastSeen} | ${manualOverride} |\n`;
    }

    fs.writeFileSync(mdPath, mdContent);
    console.log(`✅ Markdown report exported: ${mdPath}\n`);

    // Generate top publishers list
    const topPublishers = rows.slice(0, 50);
    const topPath = path.join(outputDir, 'top-publishers.md');
    let topContent = '# Top 50 Publishers by Quality Score\n\n';
    topContent += `Generated: ${new Date().toISOString()}\n\n`;

    topContent += '| Rank | Publisher Name | Quality Score | Citations | Topic Relevance | Platform |\n';
    topContent += '|------|----------------|---------------|-----------|-----------------|----------|\n';

    topPublishers.forEach((row, idx) => {
      const name = (row.publisher_name || '').replace(/\|/g, '\\|');
      const score = row.quality_score?.toFixed(1) || 'N/A';
      const citations = row.citation_count || 0;
      const topicRelevance = row.topic_relevance_score ? (row.topic_relevance_score * 100).toFixed(0) + '%' : '-';
      const platform = row.platform || '-';
      
      topContent += `| ${idx + 1} | ${name} | ${score} | ${citations} | ${topicRelevance} | ${platform} |\n`;
    });

    fs.writeFileSync(topPath, topContent);
    console.log(`✅ Top 50 publishers exported: ${topPath}\n`);

    // Show summary
    console.log('📊 Export Summary:');
    console.log(`   Total publishers: ${rows.length}`);
    console.log(`   High quality (≥80): ${highQuality}`);
    console.log(`   Medium quality (60-79): ${mediumQuality}`);
    console.log(`   Low quality (<60): ${lowQuality}`);
    console.log(`   With manual overrides: ${withManualOverride}\n`);

    console.log('📁 Files created:');
    console.log(`   - ${csvPath}`);
    console.log(`   - ${mdPath}`);
    console.log(`   - ${topPath}\n`);

    console.log('✅ Export complete!\n');

  } catch (error: any) {
    console.error('❌ Error exporting quality scores:', error.message);
    throw error;
  }
}

exportQualityScores()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/extract-existing-publishers.ts">
/**
 * Extract existing publishers from messages table and populate publishers table
 * This is a one-time initial population
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { v4 as uuidv4 } from 'uuid';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const MESSAGES_TABLE = 'messages';
const PUBLISHERS_TABLE = 'publishers';

/**
 * Normalize publisher name to canonical form
 */
function normalizePublisherName(name: string): string {
  return name
    .toLowerCase()
    .trim()
    .replace(/[^a-z0-9\s]/g, '') // Remove special chars
    .replace(/\s+/g, ' ') // Normalize whitespace
    .trim();
}

/**
 * Extract domain from email
 */
function extractDomain(email: string): string | null {
  const match = email.match(/@([^@]+)$/);
  return match ? match[1] : null;
}

async function extractExistingPublishers() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  console.log('📊 Extracting existing publishers from messages table...\n');
  
  // Step 1: Query unique publishers from messages table
  console.log('Step 1: Querying unique publishers...');
  const [publisherRows] = await bigquery.query({
    query: `
      SELECT 
        publisher_name,
        sender,
        from_domain,
        COUNT(*) as message_count,
        MIN(sent_date) as first_seen,
        MAX(sent_date) as last_seen,
        AVG(word_count) as avg_word_count,
        ARRAY_AGG(DISTINCT sender) as all_senders
      FROM \`${PROJECT_ID}.${DATASET_ID}.${MESSAGES_TABLE}\`
      WHERE publisher_name IS NOT NULL
        AND publisher_name != ''
      GROUP BY publisher_name, sender, from_domain
      ORDER BY message_count DESC
    `,
  });
  
  console.log(`   Found ${publisherRows.length} unique publisher entries\n`);
  
  if (publisherRows.length === 0) {
    console.log('⚠️  No publishers found in messages table.\n');
    return;
  }
  
  // Step 2: Group by publisher_name (some publishers may have multiple emails)
  console.log('Step 2: Grouping by publisher name...');
  const publisherMap = new Map<string, any>();
  
  for (const row of publisherRows) {
    const publisherName = row.publisher_name;
    const canonicalName = normalizePublisherName(publisherName);
    
    if (!publisherMap.has(canonicalName)) {
      publisherMap.set(canonicalName, {
        publisher_name: publisherName,
        canonical_name: canonicalName,
        primary_email: row.sender,
        email_domains: new Set<string>(),
        email_variations: new Set<string>(),
        from_domain: row.from_domain,
        message_count: 0,
        first_seen: null,
        last_seen: null,
        avg_word_count: 0,
        all_senders: new Set<string>(),
      });
    }
    
    const publisher = publisherMap.get(canonicalName)!;
    
    // Aggregate data
    publisher.message_count += row.message_count;
    publisher.email_variations.add(row.sender);
    publisher.all_senders.add(row.sender);
    
    if (row.from_domain) {
      publisher.email_domains.add(row.from_domain);
    }
    
    // Extract domain from sender email
    const senderDomain = extractDomain(row.sender);
    if (senderDomain) {
      publisher.email_domains.add(senderDomain);
    }
    
    // Update first_seen and last_seen
    if (!publisher.first_seen || (row.first_seen && new Date(row.first_seen) < new Date(publisher.first_seen))) {
      publisher.first_seen = row.first_seen;
    }
    if (!publisher.last_seen || (row.last_seen && new Date(row.last_seen) > new Date(publisher.last_seen))) {
      publisher.last_seen = row.last_seen;
    }
    
    // Update avg_word_count (weighted average)
    const totalWords = publisher.avg_word_count * (publisher.message_count - row.message_count) + 
                       (row.avg_word_count || 0) * row.message_count;
    publisher.avg_word_count = totalWords / publisher.message_count;
  }
  
  console.log(`   Grouped into ${publisherMap.size} unique publishers\n`);
  
  // Step 3: Prepare rows for insertion
  console.log('Step 3: Preparing publisher entries...');
  const now = new Date().toISOString();
  const rows = Array.from(publisherMap.values()).map((publisher) => {
    const publisherId = uuidv4();
    
    return {
      publisher_id: publisherId,
      publisher_name: publisher.publisher_name,
      canonical_name: publisher.canonical_name,
      primary_email: publisher.primary_email,
      email_domains: Array.from(publisher.email_domains),
      email_variations: Array.from(publisher.email_variations),
      newsletter_url: null, // Will be populated later if linked to discovered_newsletters
      platform: null, // Will be inferred later
      from_domain: publisher.from_domain,
      quality_score: null, // Will be calculated later
      citation_count: 0, // Will be calculated later
      citing_publishers: [], // Will be calculated later
      subscriber_estimate: null,
      recommendation_count: 0,
      topic_relevance_score: null,
      freshness_score: null,
      platform_score: null,
      message_count: publisher.message_count,
      first_seen: publisher.first_seen,
      last_seen: publisher.last_seen,
      avg_word_count: publisher.avg_word_count,
      discovery_id: null, // Will be linked later
      is_discovered: false,
      matched_at: null,
      quality_score_last_calculated: null,
      quality_score_version: null,
      is_duplicate: false,
      merged_into_publisher_id: null,
      created_at: now,
      updated_at: now,
    };
  });
  
  console.log(`   Prepared ${rows.length} publisher entries\n`);
  
  // Step 4: Insert into publishers table
  console.log('Step 4: Inserting into publishers table...');
  const table = bigquery.dataset(DATASET_ID).table(PUBLISHERS_TABLE);
  
  // Insert in chunks to avoid BigQuery limits
  const CHUNK_SIZE = 500;
  let inserted = 0;
  
  for (let i = 0; i < rows.length; i += CHUNK_SIZE) {
    const chunk = rows.slice(i, i + CHUNK_SIZE);
    const chunkNumber = Math.floor(i / CHUNK_SIZE) + 1;
    const totalChunks = Math.ceil(rows.length / CHUNK_SIZE);
    
    try {
      await table.insert(chunk);
      inserted += chunk.length;
      console.log(`   ✅ Inserted chunk ${chunkNumber}/${totalChunks} (${chunk.length} publishers)`);
    } catch (error: any) {
      console.error(`   ❌ Failed to insert chunk ${chunkNumber}:`, error.message);
      // Continue with next chunk
    }
  }
  
  console.log(`\n✅ Successfully inserted ${inserted}/${rows.length} publishers\n`);
  
  // Step 5: Summary statistics
  console.log('📊 Summary Statistics:');
  console.log(`   Total unique publishers: ${publisherMap.size}`);
  console.log(`   Total messages: ${rows.reduce((sum, r) => sum + r.message_count, 0).toLocaleString()}`);
  console.log(`   Average messages per publisher: ${Math.round(rows.reduce((sum, r) => sum + r.message_count, 0) / rows.length)}`);
  console.log(`   Publishers with multiple emails: ${rows.filter(r => r.email_variations.length > 1).length}`);
  console.log('');
  
  console.log('✅ Initial publisher extraction complete!\n');
  console.log('Next steps:');
  console.log('  1. Run: npm run publishers:link-discoveries (optional - links to discovered_newsletters)');
  console.log('  2. Run: npm run publishers:initial-citations (citation analysis)');
  console.log('  3. Run: npm run publishers:initial-scoring (quality scoring)');
}

extractExistingPublishers()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/link-discovered-newsletters.ts">
/**
 * Auto-link publishers to discovered_newsletters
 * Matches by URL domain, email domain, or publisher name
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const DISCOVERED_TABLE = 'discovered_newsletters';

/**
 * Extract domain from URL
 */
function extractUrlDomain(url: string): string | null {
  try {
    const urlObj = new URL(url);
    return urlObj.hostname.replace(/^www\./, '');
  } catch {
    return null;
  }
}

/**
 * Extract domain from email
 */
function extractEmailDomain(email: string): string | null {
  const match = email.match(/@([^@]+)$/);
  return match ? match[1].toLowerCase() : null;
}

/**
 * Normalize for matching
 */
function normalizeForMatch(str: string): string {
  return str.toLowerCase().trim().replace(/[^a-z0-9]/g, '');
}

/**
 * Calculate match confidence
 */
function calculateMatchConfidence(
  publisher: any,
  discovered: any
): { confidence: number; reason: string } {
  // High confidence: Exact URL domain match
  if (discovered.newsletter_url) {
    const discoveredDomain = extractUrlDomain(discovered.newsletter_url);
    if (discoveredDomain) {
      // Check if publisher email domain matches
      for (const emailDomain of publisher.email_domains || []) {
        if (emailDomain.toLowerCase() === discoveredDomain.toLowerCase()) {
          return { confidence: 0.95, reason: 'URL domain match' };
        }
        // Check if email domain is subdomain (e.g., zeihan.substack.com matches substack.com)
        if (emailDomain.toLowerCase().endsWith('.' + discoveredDomain.toLowerCase())) {
          return { confidence: 0.90, reason: 'URL subdomain match' };
        }
      }
    }
  }

  // Medium confidence: Publisher name match
  const publisherNameNorm = normalizeForMatch(publisher.publisher_name);
  const discoveredNameNorm = normalizeForMatch(discovered.newsletter_name);
  
  if (publisherNameNorm === discoveredNameNorm) {
    return { confidence: 0.85, reason: 'Exact name match' };
  }
  
  // Check if one contains the other (e.g., "Zeihan" vs "Zeihan on Geopolitics")
  if (publisherNameNorm.includes(discoveredNameNorm) || discoveredNameNorm.includes(publisherNameNorm)) {
    return { confidence: 0.70, reason: 'Partial name match' };
  }

  // Low confidence: Email domain match only
  if (discovered.newsletter_url) {
    const discoveredDomain = extractUrlDomain(discovered.newsletter_url);
    if (discoveredDomain) {
      for (const emailDomain of publisher.email_domains || []) {
        const emailDomainNorm = emailDomain.toLowerCase();
        const discoveredDomainNorm = discoveredDomain.toLowerCase();
        
        // Check if domains share common parts (e.g., both contain "substack")
        if (emailDomainNorm.includes('substack') && discoveredDomainNorm.includes('substack')) {
          return { confidence: 0.60, reason: 'Platform domain match' };
        }
      }
    }
  }

  return { confidence: 0, reason: 'No match' };
}

async function linkDiscoveredNewsletters() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  console.log('🔗 Linking publishers to discovered newsletters...\n');
  
  // Step 1: Get all publishers
  console.log('Step 1: Fetching publishers...');
  const [publishers] = await bigquery.query({
    query: `
      SELECT 
        publisher_id,
        publisher_name,
        canonical_name,
        primary_email,
        email_domains,
        newsletter_url,
        discovery_id,
        is_discovered
      FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
      WHERE is_discovered = false OR is_discovered IS NULL
      ORDER BY publisher_name
    `,
  });
  
  console.log(`   Found ${publishers.length} publishers to check\n`);
  
  // Step 2: Get all discovered newsletters
  console.log('Step 2: Fetching discovered newsletters...');
  const [discovered] = await bigquery.query({
    query: `
      SELECT 
        discovery_id,
        newsletter_name,
        newsletter_url,
        canonical_url,
        platform,
        is_relevant
      FROM \`${PROJECT_ID}.${DATASET_ID}.${DISCOVERED_TABLE}\`
      WHERE is_relevant = true
        AND needs_review = false
      ORDER BY newsletter_name
    `,
  });
  
  console.log(`   Found ${discovered.length} discovered newsletters\n`);
  
  // Step 3: Match publishers to discovered newsletters
  console.log('Step 3: Matching publishers to discovered newsletters...');
  const matches: Array<{
    publisher_id: string;
    discovery_id: string;
    confidence: number;
    reason: string;
  }> = [];
  
  for (const publisher of publishers) {
    let bestMatch: any = null;
    let bestConfidence = 0;
    let bestReason = '';
    
    for (const disc of discovered) {
      const match = calculateMatchConfidence(publisher, disc);
      
      if (match.confidence > bestConfidence) {
        bestConfidence = match.confidence;
        bestReason = match.reason;
        bestMatch = disc;
      }
    }
    
    // Only match if confidence is high (>= 0.70)
    if (bestMatch && bestConfidence >= 0.70) {
      matches.push({
        publisher_id: publisher.publisher_id,
        discovery_id: bestMatch.discovery_id,
        confidence: bestConfidence,
        reason: bestReason,
      });
    }
  }
  
  console.log(`   Found ${matches.length} high-confidence matches\n`);
  
  // Step 4: Update publishers table with matches
  console.log('Step 4: Updating publishers table...');
  const now = new Date().toISOString();
  let updated = 0;
  
  for (const match of matches) {
    try {
      // Get discovery metadata to enrich publisher
      const discovery = discovered.find(d => d.discovery_id === match.discovery_id);
      
      await bigquery.query({
        query: `
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          SET 
            discovery_id = @discovery_id,
            is_discovered = true,
            matched_at = @matched_at,
            newsletter_url = COALESCE(newsletter_url, @newsletter_url),
            platform = COALESCE(platform, @platform),
            updated_at = @updated_at
          WHERE publisher_id = @publisher_id
        `,
        params: {
          discovery_id: match.discovery_id,
          matched_at: now,
          newsletter_url: discovery?.newsletter_url || null,
          platform: discovery?.platform || null,
          publisher_id: match.publisher_id,
          updated_at: now,
        },
      });
      
      updated++;
    } catch (error: any) {
      console.error(`   ❌ Failed to update publisher ${match.publisher_id}:`, error.message);
    }
  }
  
  console.log(`   ✅ Updated ${updated}/${matches.length} publishers\n`);
  
  // Step 5: Summary
  console.log('📊 Summary:');
  console.log(`   Publishers checked: ${publishers.length}`);
  console.log(`   Discovered newsletters: ${discovered.length}`);
  console.log(`   High-confidence matches: ${matches.length}`);
  console.log(`   Publishers linked: ${updated}`);
  
  // Breakdown by confidence
  const highConf = matches.filter(m => m.confidence >= 0.85).length;
  const mediumConf = matches.filter(m => m.confidence >= 0.70 && m.confidence < 0.85).length;
  
  console.log(`   High confidence (>=0.85): ${highConf}`);
  console.log(`   Medium confidence (0.70-0.84): ${mediumConf}`);
  console.log('');
  
  console.log('✅ Auto-linking complete!\n');
  console.log('Next steps:');
  console.log('  1. Run: npm run publishers:initial-citations (citation analysis)');
  console.log('  2. Run: npm run publishers:initial-scoring (quality scoring)');
}

linkDiscoveredNewsletters()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/manual-override-quality-score.ts">
/**
 * Manual Quality Score Override
 * 
 * Allows manual adjustment of quality scores for specific publishers.
 * Supports:
 * - Full quality score override
 * - Individual signal overrides
 * - Reason tracking
 * 
 * Usage:
 *   npm run publishers:override <publisher_id> <score> [reason]
 *   npm run publishers:override-signal <publisher_id> <signal_name> <value> [reason]
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'publishers';

interface OverrideOptions {
  publisherId: string;
  score?: number;
  signalName?: string;
  signalValue?: number;
  reason?: string;
  updatedBy?: string;
}

async function setManualOverride(options: OverrideOptions) {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  const { publisherId, score, signalName, signalValue, reason, updatedBy } = options;

  // Validate
  if (score !== undefined && (score < 0 || score > 100)) {
    throw new Error('Quality score must be between 0 and 100');
  }
  if (signalValue !== undefined && (signalValue < 0 || signalValue > 1)) {
    throw new Error('Signal value must be between 0 and 1');
  }

  const validSignals = ['citation_signal', 'subscriber_signal', 'recommendation_signal', 
                        'topic_relevance_signal', 'platform_signal', 'freshness_signal'];
  if (signalName && !validSignals.includes(signalName)) {
    throw new Error(`Invalid signal name. Must be one of: ${validSignals.join(', ')}`);
  }

  try {
    // First, check if publisher exists and get current manual overrides
    const checkQuery = `
      SELECT 
        publisher_id,
        publisher_name,
        quality_score,
        manual_quality_score_override,
        manual_individual_signal_overrides
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      WHERE publisher_id = @publisher_id
    `;

    const [rows] = await bigquery.query({
      query: checkQuery,
      params: { publisher_id: publisherId },
    });

    if (!rows || rows.length === 0) {
      throw new Error(`Publisher not found: ${publisherId}`);
    }

    const publisher = rows[0];
    console.log(`\n📝 Setting manual override for: ${publisher.publisher_name}`);
    console.log(`   Current quality score: ${publisher.quality_score?.toFixed(1) || 'N/A'}`);
    console.log(`   Current manual override: ${publisher.manual_quality_score_override || 'None'}\n`);

    // Build update query
    let updateFields: string[] = [];
    const params: any = { publisher_id: publisherId };

    if (score !== undefined) {
      // Full quality score override
      updateFields.push('manual_quality_score_override = @score');
      params.score = score;
      console.log(`   Setting quality score override: ${score}`);
    }

    if (signalName && signalValue !== undefined) {
      // Individual signal override
      let signalOverrides = publisher.manual_individual_signal_overrides 
        ? JSON.parse(JSON.stringify(publisher.manual_individual_signal_overrides))
        : {};
      
      signalOverrides[signalName] = signalValue;
      
      updateFields.push('manual_individual_signal_overrides = @signal_overrides');
      params.signal_overrides = JSON.stringify(signalOverrides);
      
      console.log(`   Setting ${signalName} override: ${signalValue}`);
      console.log(`   Note: You'll need to re-run quality scoring for this to take effect`);
    }

    if (reason) {
      updateFields.push('manual_override_reason = @reason');
      params.reason = reason;
      console.log(`   Reason: ${reason}`);
    }

    updateFields.push('manual_override_updated_at = CURRENT_TIMESTAMP()');
    updateFields.push('updated_at = CURRENT_TIMESTAMP()');

    if (updatedBy) {
      updateFields.push('manual_override_updated_by = @updated_by');
      params.updated_by = updatedBy;
    }

    const updateQuery = `
      UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      SET ${updateFields.join(', ')}
      WHERE publisher_id = @publisher_id
    `;

    await bigquery.query({
      query: updateQuery,
      params,
    });

    console.log('\n✅ Manual override set successfully!\n');

    if (score !== undefined) {
      console.log('⚠️  Note: Manual quality score override will take precedence over calculated score.');
      console.log('   The calculated score will be ignored until you remove the override.\n');
    }

    if (signalName && signalValue !== undefined) {
      console.log('⚠️  Note: Individual signal override requires re-running quality scoring.');
      console.log('   Run: npm run publishers:calculate-scores\n');
    }

  } catch (error: any) {
    console.error('❌ Error setting manual override:', error.message);
    throw error;
  }
}

// CLI interface
if (require.main === module) {
  const args = process.argv.slice(2);
  
  if (args.length < 2) {
    console.error('Usage:');
    console.error('  Set full quality score override:');
    console.error('    npm run publishers:override <publisher_id> <score> [reason]');
    console.error('');
    console.error('  Set individual signal override:');
    console.error('    npm run publishers:override-signal <publisher_id> <signal_name> <value> [reason]');
    console.error('');
    console.error('  Valid signals: citation_signal, subscriber_signal, recommendation_signal,');
    console.error('                 topic_relevance_signal, platform_signal, freshness_signal');
    console.error('');
    console.error('Examples:');
    console.error('  npm run publishers:override pub_123 85 "High-value source"');
    console.error('  npm run publishers:override-signal pub_123 citation_signal 0.9 "Well-cited"');
    process.exit(1);
  }

  const mode = process.env.OVERRIDE_MODE || 'score';
  const publisherId = args[0];
  
  if (mode === 'signal') {
    // Signal override mode
    const signalName = args[1];
    const signalValue = parseFloat(args[2]);
    const reason = args[3] || undefined;
    
    if (isNaN(signalValue)) {
      console.error('Error: Signal value must be a number between 0 and 1');
      process.exit(1);
    }
    
    setManualOverride({
      publisherId,
      signalName,
      signalValue,
      reason,
      updatedBy: process.env.USER || 'manual',
    })
      .then(() => process.exit(0))
      .catch((error) => {
        console.error('Error:', error);
        process.exit(1);
      });
  } else {
    // Full score override mode
    const score = parseFloat(args[1]);
    const reason = args[2] || undefined;
    
    if (isNaN(score)) {
      console.error('Error: Score must be a number between 0 and 100');
      process.exit(1);
    }
    
    setManualOverride({
      publisherId,
      score,
      reason,
      updatedBy: process.env.USER || 'manual',
    })
      .then(() => process.exit(0))
      .catch((error) => {
        console.error('Error:', error);
        process.exit(1);
      });
  }
}

export { setManualOverride };
</file>

<file path="scripts/publishers/populate-quality-signals.ts">
/**
 * Populate quality signals from discovered_newsletters to publishers
 * This enriches publishers with subscriber estimates, recommendation counts, and topic relevance
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const DISCOVERED_TABLE = 'discovered_newsletters';

/**
 * Calculate topic relevance score from topics
 */
function calculateTopicRelevance(primaryTopics: string[] | null, secondaryTopics: string[] | null): number {
  if (!primaryTopics || primaryTopics.length === 0) {
    return 0.5; // Neutral if no topics
  }
  
  // High-relevance topics
  const highRelevanceTerms = [
    'geopolitics', 'foreign policy', 'international relations', 'national security',
    'defense', 'diplomacy', 'trade', 'economics', 'macro', 'china', 'taiwan',
    'russia', 'ukraine', 'middle east', 'asia', 'europe', 'nato'
  ];
  
  // Medium-relevance topics
  const mediumRelevanceTerms = [
    'policy', 'politics', 'security', 'strategy', 'global', 'world'
  ];
  
  const allTopics = [...(primaryTopics || []), ...(secondaryTopics || [])];
  const topicsLower = allTopics.map(t => t.toLowerCase());
  
  // Check for high-relevance matches
  for (const term of highRelevanceTerms) {
    if (topicsLower.some(t => t.includes(term))) {
      return 0.9; // High relevance
    }
  }
  
  // Check for medium-relevance matches
  for (const term of mediumRelevanceTerms) {
    if (topicsLower.some(t => t.includes(term))) {
      return 0.7; // Medium relevance
    }
  }
  
  // Has topics but no clear match
  return 0.6;
}

async function populateQualitySignals() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Populating quality signals from discovered_newsletters...\n');

  try {
    // Step 1: Get all discovered newsletters with quality signals
    console.log('Step 1: Fetching discovered newsletters with quality signals...');
    
    const discoveredQuery = `
      SELECT 
        discovery_id,
        newsletter_name,
        newsletter_url,
        subscriber_count_estimate,
        recommendation_count,
        primary_topics,
        secondary_topics,
        platform
      FROM \`${PROJECT_ID}.${DATASET_ID}.${DISCOVERED_TABLE}\`
      WHERE is_relevant = true
        AND needs_review = false
        AND (
          subscriber_count_estimate IS NOT NULL
          OR recommendation_count IS NOT NULL
          OR primary_topics IS NOT NULL
        )
    `;

    const [discoveredRows] = await bigquery.query(discoveredQuery);
    console.log(`   Found ${discoveredRows.length} discovered newsletters with quality signals\n`);

    if (discoveredRows.length === 0) {
      console.log('⚠️  No discovered newsletters with quality signals found.\n');
      return;
    }

    // Step 2: Try to match to publishers by name or email domain
    console.log('Step 2: Matching to publishers...\n');

    let updatedCount = 0;
    let matchedCount = 0;

    for (const discovered of discoveredRows) {
      try {
        // Try to find publisher by name match (fuzzy)
        const matchQuery = `
          SELECT publisher_id, publisher_name, primary_email, email_domains
          FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          WHERE LOWER(publisher_name) = LOWER(@newsletter_name)
             OR LOWER(TRIM(publisher_name)) = LOWER(TRIM(@newsletter_name))
          LIMIT 1
        `;

        const [matchRows] = await bigquery.query({
          query: matchQuery,
          params: { newsletter_name: discovered.newsletter_name },
        });

        if (!matchRows || matchRows.length === 0) {
          // Try partial match
          const partialMatchQuery = `
            SELECT publisher_id, publisher_name, primary_email
            FROM \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
            WHERE LOWER(publisher_name) LIKE LOWER(CONCAT('%', @newsletter_name, '%'))
               OR LOWER(@newsletter_name) LIKE LOWER(CONCAT('%', publisher_name, '%'))
            LIMIT 1
          `;

          const [partialRows] = await bigquery.query({
            query: partialMatchQuery,
            params: { newsletter_name: discovered.newsletter_name },
          });

          if (partialRows && partialRows.length > 0) {
            const publisher = partialRows[0];
            
            // Update publisher with quality signals
            const topicRelevance = calculateTopicRelevance(
              discovered.primary_topics,
              discovered.secondary_topics
            );

            // Build update query dynamically to handle nulls
            const updateFields: string[] = [
              'discovery_id = @discovery_id',
              'topic_relevance_score = @topic_relevance',
              'is_discovered = TRUE',
              'matched_at = CURRENT_TIMESTAMP()',
              'updated_at = CURRENT_TIMESTAMP()',
            ];
            
            const params: any = {
              publisher_id: publisher.publisher_id,
              discovery_id: discovered.discovery_id,
              topic_relevance: topicRelevance,
            };

            if (discovered.subscriber_count_estimate !== null) {
              updateFields.push('subscriber_estimate = COALESCE(@subscriber_estimate, subscriber_estimate)');
              params.subscriber_estimate = discovered.subscriber_count_estimate;
            }
            
            if (discovered.recommendation_count !== null) {
              updateFields.push('recommendation_count = COALESCE(@recommendation_count, recommendation_count)');
              params.recommendation_count = discovered.recommendation_count;
            }
            
            if (discovered.platform) {
              updateFields.push('platform = COALESCE(@platform, platform)');
              params.platform = discovered.platform;
            }
            
            if (discovered.newsletter_url) {
              updateFields.push('newsletter_url = COALESCE(@newsletter_url, newsletter_url)');
              params.newsletter_url = discovered.newsletter_url;
            }

            const updateQuery = `
              UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
              SET ${updateFields.join(', ')}
              WHERE publisher_id = @publisher_id
            `;

            await bigquery.query({
              query: updateQuery,
              params,
            });

            updatedCount++;
            matchedCount++;
          }
        } else {
          const publisher = matchRows[0];
          
          // Update publisher with quality signals
          const topicRelevance = calculateTopicRelevance(
            discovered.primary_topics,
            discovered.secondary_topics
          );

          // Build update query dynamically to handle nulls
          const updateFields: string[] = [
            'discovery_id = @discovery_id',
            'topic_relevance_score = @topic_relevance',
            'is_discovered = TRUE',
            'matched_at = CURRENT_TIMESTAMP()',
            'updated_at = CURRENT_TIMESTAMP()',
          ];
          
          const params: any = {
            publisher_id: publisher.publisher_id,
            discovery_id: discovered.discovery_id,
            topic_relevance: topicRelevance,
          };

          if (discovered.subscriber_count_estimate !== null) {
            updateFields.push('subscriber_estimate = COALESCE(@subscriber_estimate, subscriber_estimate)');
            params.subscriber_estimate = discovered.subscriber_count_estimate;
          }
          
          if (discovered.recommendation_count !== null) {
            updateFields.push('recommendation_count = COALESCE(@recommendation_count, recommendation_count)');
            params.recommendation_count = discovered.recommendation_count;
          }
          
          if (discovered.platform) {
            updateFields.push('platform = COALESCE(@platform, platform)');
            params.platform = discovered.platform;
          }
          
          if (discovered.newsletter_url) {
            updateFields.push('newsletter_url = COALESCE(@newsletter_url, newsletter_url)');
            params.newsletter_url = discovered.newsletter_url;
          }

          const updateQuery = `
            UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
            SET ${updateFields.join(', ')}
            WHERE publisher_id = @publisher_id
          `;

          await bigquery.query({
            query: updateQuery,
            params,
          });

          updatedCount++;
          matchedCount++;
        }
      } catch (error: any) {
        console.error(`   ⚠️  Error matching ${discovered.newsletter_name}:`, error.message);
      }
    }

    console.log(`\n✅ Updated ${updatedCount} publishers with quality signals`);
    console.log(`   Matched ${matchedCount} discovered newsletters to publishers\n`);

    // Step 3: Re-run citation analysis to catch newly linked publishers
    console.log('Step 3: Re-running citation analysis to update linked publishers...\n');
    console.log('   (Run: npm run publishers:calculate-citations)\n');

    // Step 4: Re-calculate quality scores
    console.log('Step 4: Re-calculate quality scores with new data...\n');
    console.log('   (Run: npm run publishers:calculate-scores)\n');

  } catch (error: any) {
    console.error('❌ Error populating quality signals:', error.message);
    throw error;
  }
}

populateQualitySignals()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/publishers/update-citation-counts.ts">
/**
 * Update publishers table with citation counts from pattern-based analysis
 * This script reads citation results and updates the publishers table
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';
const DISCOVERED_TABLE = 'discovered_newsletters';

interface CitationResult {
  discovery_id?: string;
  publisher_id?: string;
  newsletter_name: string;
  citation_count: number;
  citing_publishers: string[];
}

async function updateCitationCounts() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('📊 Updating citation counts in publishers table...\n');

  try {
    // Step 1: Get citation results from the last run
    // We'll need to re-run the citation analysis or read from a file
    // For now, let's query the discovered_newsletters to get what we have
    console.log('Step 1: Fetching citation results...\n');
    
    // We need to get the citation data from the citation analysis script
    // Since we just ran it, we'll query discovered_newsletters for any existing citation data
    // OR we can re-run the citation analysis and store results
    
    // For now, let's create a script that can be called with citation data
    // We'll use a parameterized approach
    
    console.log('⚠️  This script needs citation data from calculate-citations-pattern-based-robust.ts');
    console.log('   Two options:');
    console.log('   1. Run citation analysis and pipe results to this script');
    console.log('   2. Re-run citation analysis and update in same script');
    console.log('\n   Proceeding with option 2: Re-running citation analysis...\n');
    
    // Import and run the citation analysis
    // Actually, we should extract the citation results from the citation script
    // Let's create a helper function that can be called
    
    console.log('✅ Citation counts will be updated after running citation analysis');
    console.log('   See: calculate-citations-pattern-based-robust.ts for citation data\n');
    
  } catch (error: any) {
    console.error('❌ Error updating citation counts:', error.message);
    throw error;
  }
}

// For now, create a version that accepts citation data as input
export async function updatePublishersWithCitations(citations: CitationResult[]) {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log(`\n📊 Updating ${citations.length} publishers with citation counts...\n`);

  let updatedCount = 0;
  let notFoundCount = 0;

  for (const citation of citations) {
    try {
      // Try to find publisher by discovery_id first
      if (citation.discovery_id) {
        const updateQuery = `
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          SET 
            citation_count = @citation_count,
            citing_publishers = @citing_publishers,
            updated_at = CURRENT_TIMESTAMP()
          WHERE discovery_id = @discovery_id
        `;

        const options = {
          query: updateQuery,
          params: {
            discovery_id: citation.discovery_id,
            citation_count: citation.citation_count,
            citing_publishers: citation.citing_publishers,
          },
        };

        const [job] = await bigquery.createQueryJob(options);
        const [rows] = await job.getQueryResults();
        
        if (rows && rows.length > 0) {
          updatedCount++;
        } else {
          // Try to find by publisher_name
          notFoundCount++;
        }
      } else if (citation.publisher_id) {
        // Update by publisher_id
        const updateQuery = `
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${PUBLISHERS_TABLE}\`
          SET 
            citation_count = @citation_count,
            citing_publishers = @citing_publishers,
            updated_at = CURRENT_TIMESTAMP()
          WHERE publisher_id = @publisher_id
        `;

        const options = {
          query: updateQuery,
          params: {
            publisher_id: citation.publisher_id,
            citation_count: citation.citation_count,
            citing_publishers: citation.citing_publishers,
          },
        };

        const [job] = await bigquery.createQueryJob(options);
        const [rows] = await job.getQueryResults();
        
        if (rows && rows.length > 0) {
          updatedCount++;
        } else {
          notFoundCount++;
        }
      }
    } catch (error: any) {
      console.error(`   ⚠️  Error updating ${citation.newsletter_name}:`, error.message);
    }
  }

  console.log(`\n✅ Updated ${updatedCount} publishers`);
  if (notFoundCount > 0) {
    console.log(`   ⚠️  ${notFoundCount} citations not matched to publishers`);
  }
}

if (require.main === module) {
  updateCitationCounts()
    .then(() => process.exit(0))
    .catch((error) => {
      console.error('Error:', error);
      process.exit(1);
    });
}
</file>

<file path="scripts/publishers/update-citations-only.ts">
/**
 * Update citation counts only (skip analysis, just update)
 * Use this if citation analysis already ran but updates failed
 */

import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const PUBLISHERS_TABLE = 'publishers';

async function updateCitationsOnly() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  console.log('📊 Re-running citation analysis to update counts...\n');
  
  // Re-run the full citation analysis
  // This will take 1-2 hours but will properly update all citation counts
  const { exec } = require('child_process');
  const { promisify } = require('util');
  const execAsync = promisify(exec);
  
  console.log('Running full citation analysis...\n');
  const { stdout, stderr } = await execAsync('npm run publishers:initial-citations');
  
  console.log(stdout);
  if (stderr) {
    console.error(stderr);
  }
}

updateCitationsOnly()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/add-doc-ids-provenance.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { createHash } from 'crypto';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

/**
 * Generate stable doc_id from message metadata
 */
function generateDocId(sender: string, subject: string, sentDate: string | null): string {
  const data = `${sender}|${subject}|${sentDate || ''}`;
  return createHash('sha256').update(data).digest('hex').substring(0, 32);
}

async function addDocIdsAndProvenance() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  try {
    console.log('🔄 Adding document IDs and provenance fields to messages table...\n');
    
    // Check current schema
    console.log('📋 Checking current schema...');
    const [metadata] = await bigquery
      .dataset(DATASET_ID)
      .table(TABLE_ID)
      .getMetadata();
    
    const schema = metadata.schema?.fields || [];
    const fieldNames = schema.map(f => f.name);
    
    // Columns to add
    const fieldsToAdd: any[] = [];
    
    if (!fieldNames.includes('doc_id')) {
      fieldsToAdd.push({
        name: 'doc_id',
        type: 'STRING',
        mode: 'NULLABLE',
        description: 'Stable canonical ID (hash of sender + subject + sent_date)'
      });
    }
    
    if (!fieldNames.includes('doc_version')) {
      fieldsToAdd.push({
        name: 'doc_version',
        type: 'INTEGER',
        mode: 'NULLABLE',
        description: 'Document version (incremented on reprocessing)'
      });
    }
    
    if (!fieldNames.includes('list_id')) {
      fieldsToAdd.push({
        name: 'list_id',
        type: 'STRING',
        mode: 'NULLABLE',
        description: 'List-Id header (newsletter identifier)'
      });
    }
    
    if (!fieldNames.includes('from_domain')) {
      fieldsToAdd.push({
        name: 'from_domain',
        type: 'STRING',
        mode: 'NULLABLE',
        description: 'Domain from sender email'
      });
    }
    
    if (!fieldNames.includes('was_forwarded')) {
      fieldsToAdd.push({
        name: 'was_forwarded',
        type: 'BOOLEAN',
        mode: 'NULLABLE',
        description: 'True if email was forwarded'
      });
    }
    
    if (fieldsToAdd.length === 0) {
      console.log('✅ All fields already exist!\n');
    } else {
      // Add columns via ALTER TABLE
      console.log(`📝 Adding ${fieldsToAdd.length} new fields...`);
      for (const field of fieldsToAdd) {
        let query: string;
        if (field.mode === 'NULLABLE') {
          query = `
            ALTER TABLE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
            ADD COLUMN IF NOT EXISTS ${field.name} ${field.type} OPTIONS(description="${field.description}")
          `;
        } else {
          query = `
            ALTER TABLE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
            ADD COLUMN IF NOT EXISTS ${field.name} ${field.type}
          `;
        }
        await bigquery.query(query);
        console.log(`   ✅ Added ${field.name}`);
      }
      console.log('');
    }
    
    // Backfill doc_id and other fields for existing rows
    console.log('📊 Checking if backfill needed...');
    const [countResult] = await bigquery.query(`
      SELECT 
        COUNT(*) as total,
        COUNT(doc_id) as with_doc_id,
        COUNT(from_domain) as with_from_domain,
        COUNT(list_id) as with_list_id
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
    `);
    const row = countResult[0];
    
    if (row.with_doc_id < row.total) {
      console.log(`📝 Backfilling ${row.total - row.with_doc_id} rows with doc_id and provenance...`);
      console.log('   This may take a few minutes...\n');
      
      // Process in batches to avoid memory issues
      const BATCH_SIZE = 1000;
      let processed = 0;
      
      while (processed < row.total) {
        // Fetch batch
        const [batchRows] = await bigquery.query(`
          SELECT 
            id,
            sender,
            subject,
            sent_date
          FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
          WHERE doc_id IS NULL
          LIMIT ${BATCH_SIZE}
        `);
        
        if (batchRows.length === 0) {
          console.log('✅ No more rows to process');
          break;
        }
        
        // Generate doc_ids and updates
        const updates = batchRows.map((msg: any) => {
          const docId = generateDocId(msg.sender, msg.subject, msg.sent_date);
          const fromDomain = msg.sender?.split('@')[1] || null;
          
          return {
            id: msg.id,
            doc_id: docId,
            doc_version: 1,
            from_domain: fromDomain
          };
        });
        
        // Update batch using temporary table + MERGE
        // Create temp table
        const tempTable = `\`${PROJECT_ID}.${DATASET_ID}.temp_backfill_${Date.now()}\``;
        await bigquery.query(`
          CREATE TABLE ${tempTable} AS
          SELECT * FROM UNNEST([
            ${updates.map(u => 
              `STRUCT('${u.id}' AS id, '${u.doc_id}' AS doc_id, ${u.doc_version} AS doc_version, ${u.from_domain ? `'${u.from_domain}'` : 'NULL'} AS from_domain)`
            ).join(',\n            ')}
          ])
        `);
        
        // MERGE
        await bigquery.query(`
          UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` m
          SET 
            doc_id = t.doc_id,
            doc_version = t.doc_version,
            from_domain = t.from_domain
          FROM ${tempTable} t
          WHERE m.id = t.id
        `);
        
        // Drop temp table
        await bigquery.query(`DROP TABLE ${tempTable}`);
        
        processed += batchRows.length;
        console.log(`   ✅ Processed ${processed}/${row.total} rows`);
      }
      
      console.log('');
    } else {
      console.log('✅ All rows already have doc_id\n');
    }
    
    console.log('🎉 Schema migration complete!\n');
    console.log('📋 Summary:');
    console.log(`   - doc_id: Stable canonical ID`);
    console.log(`   - doc_version: Version tracking`);
    console.log(`   - from_domain: Sender domain`);
    console.log(`   - list_id: Newsletter identifier (will be populated during future ingestion)`);
    console.log(`   - was_forwarded: Forward detection (will be populated during future ingestion)`);
    console.log('');
    
  } catch (error) {
    console.error('❌ Migration failed:', error);
    throw error;
  }
}

addDocIdsAndProvenance();
</file>

<file path="scripts/check-labels-and-recent.ts">
/**
 * Check for labels and recent emails
 */

import * as dotenv from 'dotenv';
import { getGmail } from '../src/lib/gmail';

dotenv.config();

async function checkLabelsAndRecent() {
  const gmail = getGmail('legacy');
  
  console.log('🔍 Checking Gmail setup...\n');
  
  // Check labels
  const labels = await gmail.users.labels.list({ userId: 'me' });
  console.log('📋 Labels in this account:');
  const labelNames = labels.data.labels?.map(l => l.name) || [];
  labelNames.forEach(name => {
    if (name.toLowerCase().includes('nsm') || name.toLowerCase().includes('international')) {
      console.log(`   ✅ ${name}`);
    }
  });
  
  if (!labelNames.some(n => n.toLowerCase().includes('nsm'))) {
    console.log('   (No labels found with "nsm" or "international")\n');
  }
  
  // Check recent emails in inbox
  console.log('\n📧 Recent emails in inbox (last 10):');
  const recent = await gmail.users.messages.list({
    userId: 'me',
    q: 'in:inbox newer_than:1d',
    maxResults: 10
  });
  
  if (recent.data.messages && recent.data.messages.length > 0) {
    console.log(`   Found ${recent.data.messages.length} recent emails`);
    for (const msg of recent.data.messages.slice(0, 5)) {
      const fullMsg = await gmail.users.messages.get({
        userId: 'me',
        id: msg.id!,
        format: 'metadata',
        metadataHeaders: ['From', 'To', 'Delivered-To']
      });
      
      const from = fullMsg.data.payload?.headers?.find(h => h.name === 'From')?.value || 'unknown';
      const to = fullMsg.data.payload?.headers?.find(h => h.name === 'To')?.value || 'unknown';
      const deliveredTo = fullMsg.data.payload?.headers?.find(h => h.name === 'Delivered-To')?.value || to;
      
      console.log(`   - From: ${from}`);
      console.log(`     To: ${to}`);
      console.log(`     Delivered-To: ${deliveredTo}\n`);
    }
  } else {
    console.log('   No recent emails found\n');
  }
  
  console.log('\n💡 If nsm@internationalintrigue.io is a different Gmail account,');
  console.log('   you may need to authenticate with that account instead.\n');
}

checkLabelsAndRecent()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/check-processed-data.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const CHUNKS_TABLE = 'chunks';

async function checkProcessedData() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  console.log('\n📊 DIAGNOSING OVERNIGHT PROCESSING FAILURE');
  console.log('==========================================\n');

  try {
    // Check total chunks in database
    const totalQuery = `SELECT COUNT(*) as total FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\``;
    const [totalRows] = await bigquery.query(totalQuery);
    const totalChunks = totalRows[0].total;
    console.log(`📦 Total chunks in database: ${totalChunks}`);

    // Check unique newsletters
    const newsletterQuery = `
      SELECT COUNT(DISTINCT newsletter_id) as unique_newsletters
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
    `;
    const [newsletterRows] = await bigquery.query(newsletterQuery);
    const uniqueNewsletters = newsletterRows[0].unique_newsletters;
    console.log(`📰 Unique newsletters processed: ${uniqueNewsletters}`);

    // Get breakdown by publisher
    const publisherQuery = `
      SELECT 
        publisher_name,
        COUNT(DISTINCT newsletter_id) as newsletter_count,
        COUNT(*) as chunk_count
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      WHERE publisher_name IS NOT NULL
      GROUP BY publisher_name
      ORDER BY newsletter_count DESC
      LIMIT 10
    `;
    const [publisherRows] = await bigquery.query(publisherQuery);
    console.log('\n📊 Top 10 publishers by newsletters processed:');
    publisherRows.forEach((row: any, idx: number) => {
      console.log(`   ${idx + 1}. ${row.publisher_name}: ${row.newsletter_count} newsletters, ${row.chunk_count} chunks`);
    });

    // Check recent entries (to see if any succeeded late)
    const recentQuery = `
      SELECT 
        newsletter_id,
        subject,
        publisher_name,
        COUNT(*) as chunk_count,
        MAX(created_at) as last_processed
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      GROUP BY newsletter_id, subject, publisher_name
      ORDER BY last_processed DESC
      LIMIT 20
    `;
    const [recentRows] = await bigquery.query(recentQuery);
    console.log('\n🕐 Most recently processed newsletters:');
    recentRows.forEach((row: any, idx: number) => {
      console.log(`   ${idx + 1}. ${row.publisher_name}: "${row.subject.substring(0, 50)}..." (${row.chunk_count} chunks)`);
    });

    // Calculate success rate
    const expectedChunks = totalChunks; // Rough estimate
    console.log(`\n📈 Processing stats from BigQuery:`);
    console.log(`   ✅ Successfully processed: ~${uniqueNewsletters} newsletters`);
    console.log(`   📦 Total chunks created: ${totalChunks}`);
    console.log(`   🎯 Based on ~12 chunks per newsletter: ${Math.round(totalChunks / 12)} fully processed`);

    // Check if we can resume
    const offsetQuery = `
      SELECT newsletter_id
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      ORDER BY created_at DESC
      LIMIT 1
    `;
    const [offsetRows] = await bigquery.query(offsetQuery);
    console.log(`\n💾 Database shows ${uniqueNewsletters} newsletters already processed.`);
    console.log(`   You can resume by setting START_FROM=${uniqueNewsletters + 119} to skip already processed items.`);

  } catch (error) {
    console.error('❌ Error querying BigQuery:', error);
  }
}

checkProcessedData();
</file>

<file path="scripts/classify-recent.ts">
import * as dotenv from 'dotenv';
import { getGmail, extractEmailAddress } from '../src/lib/gmail';
import vipConfig from '../config/vip.json';

dotenv.config();

interface MessageData {
  id: string;
  from: string;
  fromEmail: string;
  subject: string;
  dateISO: string;
}

function isVip(fromEmail: string): boolean {
  // Check if email exactly matches any VIP sender
  if (vipConfig.senders.includes(fromEmail)) {
    return true;
  }
  
  // Check if domain matches any VIP domain
  const domain = fromEmail.split('@')[1]?.toLowerCase();
  if (domain && vipConfig.domains.includes(domain)) {
    return true;
  }
  
  return false;
}

(async () => {
  try {
    const gmail = getGmail();
    
    // Get recent messages
    const listRes = await gmail.users.messages.list({ 
      userId: 'me', 
      q: 'newer_than:1d', 
      maxResults: 100 
    });
    
    const messageIds = listRes.data.messages || [];
    console.log(`Processing ${messageIds.length} messages...`);
    
    const vip: MessageData[] = [];
    const nonVip: MessageData[] = [];
    
    // Process each message
    for (const msg of messageIds) {
      const metaRes = await gmail.users.messages.get({ 
        userId: 'me', 
        id: msg.id!, 
        format: 'metadata', 
        metadataHeaders: ['From', 'Subject', 'Date'] 
      });
      
      const headers = metaRes.data.payload?.headers || [];
      const fromHeader = headers.find(h => h.name === 'From')?.value || '';
      const subjectHeader = headers.find(h => h.name === 'Subject')?.value || '(no subject)';
      const dateHeader = headers.find(h => h.name === 'Date')?.value || '';
      
      const fromEmail = extractEmailAddress(fromHeader);
      let dateISO = '';
      try {
        dateISO = new Date(dateHeader).toISOString();
      } catch {
        dateISO = '';
      }
      
      const messageData: MessageData = {
        id: msg.id!,
        from: fromHeader,
        fromEmail,
        subject: subjectHeader,
        dateISO
      };
      
      if (isVip(fromEmail)) {
        vip.push(messageData);
      } else {
        nonVip.push(messageData);
      }
    }
    
    // Log summary
    console.log(`VIP: ${vip.length}  Non-VIP: ${nonVip.length}  Total: ${vip.length + nonVip.length}`);
    
    // Show VIP messages (up to 5)
    console.log('\nVIP messages:');
    vip.slice(0, 5).forEach(msg => {
      console.log(`VIP  | ${msg.fromEmail} | ${msg.subject}`);
    });
    
    // Show non-VIP messages (up to 5)
    console.log('\nNon-VIP messages:');
    nonVip.slice(0, 5).forEach(msg => {
      console.log(`REST | ${msg.fromEmail} | ${msg.subject}`);
    });
    
    process.exit(0);
  } catch (error) {
    console.error('Error classifying recent messages:', error);
    process.exit(1);
  }
})();
</file>

<file path="scripts/deduplicate-chunks.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const CHUNKS_TABLE = 'chunks';
const LOCATION = 'us-central1';

interface DeduplicationStats {
  totalDuplicates: number;
  newslettersAffected: number;
  chunksDeleted: number;
  dryRun: boolean;
}

async function analyzeDuplicates(bigquery: BigQuery): Promise<DeduplicationStats> {
  console.log('\n🔍 Analyzing duplicates...\n');
  
  const query = `
    WITH duplicates AS (
      SELECT 
        newsletter_id,
        chunk_index,
        COUNT(*) as duplicate_count,
        ARRAY_AGG(chunk_id ORDER BY created_at DESC LIMIT 1)[OFFSET(0)] as keep_chunk_id,
        ARRAY_AGG(chunk_id ORDER BY created_at DESC)[OFFSET(1)] as duplicate_chunk_id
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      GROUP BY newsletter_id, chunk_index
      HAVING COUNT(*) > 1
    )
    SELECT 
      COUNT(*) as total_duplicate_groups,
      COUNT(DISTINCT newsletter_id) as newsletters_affected,
      SUM(duplicate_count - 1) as total_duplicates_to_delete
    FROM duplicates
  `;

  const [rows] = await bigquery.query(query);
  const stats = rows[0] as any;

  return {
    totalDuplicates: parseInt(stats.total_duplicates_to_delete) || 0,
    newslettersAffected: parseInt(stats.newsletters_affected) || 0,
    chunksDeleted: 0,
    dryRun: true
  };
}

async function deduplicateChunks(bigquery: BigQuery, dryRun: boolean = true): Promise<DeduplicationStats> {
  console.log('\n═══════════════════════════════════════════════════════════════');
  console.log('🧹 CHUNK DEDUPLICATION');
  console.log('═══════════════════════════════════════════════════════════════\n');

  if (dryRun) {
    console.log('🔍 DRY RUN MODE - No changes will be made\n');
  } else {
    console.log('⚠️  LIVE MODE - Duplicates will be deleted!\n');
  }

  // Step 1: Analyze duplicates
  const stats = await analyzeDuplicates(bigquery);
  
  console.log('📊 Duplicate Analysis:');
  console.log(`   Newsletters affected: ${stats.newslettersAffected.toLocaleString()}`);
  console.log(`   Duplicate chunks to delete: ${stats.totalDuplicates.toLocaleString()}\n`);

  if (stats.totalDuplicates === 0) {
    console.log('✅ No duplicates found!');
    return stats;
  }

  // CRITICAL SAFETY CHECK: Verify unique chunks won't be deleted
  console.log('🔒 Performing critical safety check...');
  const safetyCheckQuery = `
    WITH unique_chunks AS (
      SELECT newsletter_id, chunk_index
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      GROUP BY newsletter_id, chunk_index
      HAVING COUNT(*) = 1
    ),
    delete_candidates AS (
      SELECT 
        newsletter_id,
        chunk_index
      FROM (
        SELECT 
          newsletter_id,
          chunk_index,
          ROW_NUMBER() OVER (PARTITION BY newsletter_id, chunk_index ORDER BY created_at DESC) as rn
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      )
      WHERE rn > 1
    )
    SELECT COUNT(*) as unique_chunks_in_delete_set
    FROM delete_candidates dc
    JOIN unique_chunks uc ON dc.newsletter_id = uc.newsletter_id AND dc.chunk_index = uc.chunk_index
  `;
  
  const [safetyRows] = await bigquery.query(safetyCheckQuery);
  const uniqueInDeleteSet = parseInt((safetyRows[0] as any).unique_chunks_in_delete_set);
  
  if (uniqueInDeleteSet > 0) {
    console.error(`\n❌ ❌ ❌ CRITICAL SAFETY CHECK FAILED ❌ ❌ ❌`);
    console.error(`   ${uniqueInDeleteSet} unique chunks would be deleted!`);
    console.error(`   Aborting to prevent data loss.`);
    console.error(`   The deduplication logic needs to be fixed.\n`);
    throw new Error('Safety check failed: unique chunks would be deleted');
  }
  
  console.log(`✅ Safety check passed: No unique chunks in delete set\n`);

  if (dryRun) {
    console.log('🔍 DRY RUN: Would delete the above chunks');
    console.log('   Run with DRY_RUN=false to actually delete duplicates\n');
    return stats;
  }

  // Step 2: Create backup of ALL duplicates (safety first!)
  console.log('💾 Step 1: Creating backup of duplicates...');
  const backupTableName = `chunks_duplicates_backup_${Date.now()}`;
  const backupQuery = `
    CREATE OR REPLACE TABLE \`${PROJECT_ID}.${DATASET_ID}.${backupTableName}\` AS
    SELECT *
    FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c1
    WHERE EXISTS (
      SELECT 1
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c2
      WHERE c1.newsletter_id = c2.newsletter_id
        AND c1.chunk_index = c2.chunk_index
        AND c1.chunk_id != c2.chunk_id
    )
  `;

  await bigquery.query(backupQuery);
  console.log(`✅ Backup created: ${backupTableName}\n`);

  // Step 3: Identify chunks to keep (keep the latest version by created_at)
  console.log('📋 Step 2: Identifying chunks to keep (keeping latest version)...');
  
  // Get total chunk count before
  const [beforeRows] = await bigquery.query(`
    SELECT COUNT(*) as count FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
  `);
  const beforeCount = parseInt((beforeRows[0] as any).count);

  // Step 4: Delete duplicates using ROW_NUMBER window function
  console.log('🗑️  Step 3: Deleting duplicates (keeping latest)...');
  const deleteQuery = `
    DELETE FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
    WHERE chunk_id IN (
      SELECT chunk_id FROM (
        SELECT 
          chunk_id,
          ROW_NUMBER() OVER (PARTITION BY newsletter_id, chunk_index ORDER BY created_at DESC) as rn
        FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      )
      WHERE rn > 1
    )
  `;

  const [deleteResult] = await bigquery.query(deleteQuery);
  const deleteCount = stats.totalDuplicates; // Known from analysis
  
  console.log(`✅ Deleted ${deleteCount.toLocaleString()} duplicate chunks\n`);

  // Step 5: Verify deletion
  console.log('✅ Step 4: Verifying deduplication...');
  const [afterRows] = await bigquery.query(`
    SELECT COUNT(*) as count FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
  `);
  const afterCount = parseInt((afterRows[0] as any).count);

  const verifyQuery = `
    SELECT 
      COUNT(*) as remaining_duplicates
    FROM (
      SELECT newsletter_id, chunk_index, COUNT(*) as dup_count
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      GROUP BY newsletter_id, chunk_index
      HAVING COUNT(*) > 1
    )
  `;

  const [verifyRows] = await bigquery.query(verifyQuery);
  const verify = verifyRows[0] as any;
  const remainingDuplicates = parseInt(verify.remaining_duplicates) || 0;

  console.log(`   Chunks before: ${beforeCount.toLocaleString()}`);
  console.log(`   Chunks after: ${afterCount.toLocaleString()}`);
  console.log(`   Chunks deleted: ${(beforeCount - afterCount).toLocaleString()}`);
  console.log(`   Remaining duplicates: ${remainingDuplicates}\n`);

  if (remainingDuplicates === 0) {
    console.log('✅ Verification passed - no duplicates remaining!\n');
  } else {
    console.log(`⚠️  Warning: ${remainingDuplicates} duplicates still remain\n`);
  }

  return {
    totalDuplicates: deleteCount,
    newslettersAffected: stats.newslettersAffected,
    chunksDeleted: beforeCount - afterCount,
    dryRun: false
  };
}

async function main() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  const dryRun = process.env.DRY_RUN !== 'false';

  try {
    const stats = await deduplicateChunks(bigquery, dryRun);

    console.log('═══════════════════════════════════════════════════════════════');
    console.log('📊 DEDUPLICATION SUMMARY');
    console.log('═══════════════════════════════════════════════════════════════');
    console.log(`Newsletters affected: ${stats.newslettersAffected.toLocaleString()}`);
    console.log(`Chunks to delete: ${stats.totalDuplicates.toLocaleString()}`);
    
    if (dryRun) {
      console.log('\n🔍 This was a DRY RUN');
      console.log('   Set DRY_RUN=false to actually delete duplicates');
    } else {
      console.log(`\n✅ Chunks deleted: ${stats.chunksDeleted.toLocaleString()}`);
    }
    
    console.log('═══════════════════════════════════════════════════════════════\n');

  } catch (error) {
    console.error('\n❌ Deduplication failed:', error);
    process.exit(1);
  }
}

main();
</file>

<file path="scripts/exchange-code-for-token.ts">
import * as dotenv from 'dotenv';
import { google } from 'googleapis';

dotenv.config();

const code = process.argv[2];

if (!code) {
  console.error('Usage: npx tsx scripts/exchange-code-for-token.ts <AUTHORIZATION_CODE>');
  process.exit(1);
}

async function exchangeCode() {
  const { GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET } = process.env;
  
  if (!GMAIL_CLIENT_ID || !GMAIL_CLIENT_SECRET) {
    console.error('❌ Missing GMAIL_CLIENT_ID or GMAIL_CLIENT_SECRET');
    process.exit(1);
  }

  const oauth2Client = new google.auth.OAuth2(
    GMAIL_CLIENT_ID,
    GMAIL_CLIENT_SECRET,
    'urn:ietf:wg:oauth:2.0:oob'
  );

  try {
    console.log('🔄 Exchanging code for tokens...\n');
    
    const { tokens } = await oauth2Client.getToken(code.trim());
    
    if (!tokens.refresh_token) {
      console.error('❌ No refresh token returned. The code may have been used already.');
      console.log('Available tokens:', Object.keys(tokens));
      process.exit(1);
    }
    
    console.log('✅ SUCCESS! New refresh token obtained:\n');
    console.log('='.repeat(70));
    console.log(tokens.refresh_token);
    console.log('='.repeat(70));
    console.log('\n📝 Add this to your .env file as GMAIL_REFRESH_TOKEN');
    console.log('\nThen run:');
    console.log('   npx tsx scripts/refresh-auth.ts');
    console.log('   npx tsx scripts/test-bigquery-auth-simple.ts\n');
    
  } catch (error: any) {
    console.error('❌ Error exchanging code:', error.message);
    if (error.message.includes('invalid_grant')) {
      console.log('\nThis usually means the code has already been used or expired.');
      console.log('Run the OAuth flow again to get a fresh code.');
    }
    process.exit(1);
  }
}

exchangeCode();
</file>

<file path="scripts/find-nsm-emails.ts">
/**
 * Find emails to nsm@internationalintrigue.io
 * Check various search strategies
 */

import * as dotenv from 'dotenv';
import { getGmail } from '../src/lib/gmail';

dotenv.config();

const TARGET_EMAIL = 'nsm@internationalintrigue.io';

async function findNsmEmails() {
  const gmail = getGmail('legacy');
  
  console.log(`🔍 Finding emails for: ${TARGET_EMAIL}\n`);
  
  // Check which account we're connected to
  const profile = await gmail.users.getProfile({ userId: 'me' });
  console.log(`📧 Connected to Gmail account: ${profile.data.emailAddress}\n`);
  
  // Try various search queries
  const queries = [
    `to:${TARGET_EMAIL}`,
    `to:${TARGET_EMAIL} in:inbox`,
    `to:${TARGET_EMAIL} -in:trash -in:spam`,
    `deliveredto:${TARGET_EMAIL}`,
    `"${TARGET_EMAIL}"`,
    `to:${TARGET_EMAIL} newer_than:7d`, // Last week
    `to:${TARGET_EMAIL} newer_than:1d`, // Last 24 hours
  ];
  
  console.log('Trying different search queries:\n');
  
  for (const query of queries) {
    try {
      const res = await gmail.users.messages.list({
        userId: 'me',
        q: query,
        maxResults: 5
      });
      
      const count = res.data.resultSizeEstimate || 0;
      const messages = res.data.messages || [];
      
      console.log(`   "${query}": ${count.toLocaleString()} messages`);
      
      if (messages.length > 0) {
        console.log(`      ✅ Found ${messages.length} messages! Sample message IDs:`);
        messages.slice(0, 3).forEach((msg, i) => {
          console.log(`         ${i + 1}. ${msg.id}`);
        });
      }
    } catch (error: any) {
      console.log(`   "${query}": ERROR - ${error.message}`);
    }
  }
  
  console.log('\n💡 If emails were found, we can use that query for ingestion.\n');
}

findNsmEmails()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/get-bigquery-refresh-token.ts">
import * as dotenv from 'dotenv';
import { google } from 'googleapis';
import * as readline from 'readline';

dotenv.config();

async function getBigQueryRefreshToken() {
  const { GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET } = process.env;
  
  if (!GMAIL_CLIENT_ID || !GMAIL_CLIENT_SECRET) {
    console.error('❌ Missing GMAIL_CLIENT_ID or GMAIL_CLIENT_SECRET in .env file');
    process.exit(1);
  }

  console.log('🔐 Generating OAuth URL for BigQuery access...\n');
  
  const oauth2Client = new google.auth.OAuth2(
    GMAIL_CLIENT_ID,
    GMAIL_CLIENT_SECRET,
    'urn:ietf:wg:oauth:2.0:oob'
  );

  // Request BigQuery and Cloud Platform scopes
  const scopes = [
    'https://www.googleapis.com/auth/bigquery',
    'https://www.googleapis.com/auth/cloud-platform'
  ];

  const authUrl = oauth2Client.generateAuthUrl({
    access_type: 'offline',
    prompt: 'consent',
    scope: scopes
  });

  console.log('📋 INSTRUCTIONS:');
  console.log('='.repeat(60));
  console.log('1. Open this URL in your browser:\n');
  console.log(authUrl);
  console.log('\n2. Sign in with your Google account');
  console.log('3. Click "Allow" to grant access');
  console.log('4. Your browser will show a "Connection Error" - this is expected!');
  console.log('5. Copy the ENTIRE URL from your browser address bar');
  console.log('6. Paste it below and press Enter');
  console.log('='.repeat(60));

  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });

  rl.question('\nPaste the URL here: ', async (url) => {
    rl.close();

    try {
      // Extract code from URL
      const urlObj = new URL(url);
      const code = urlObj.searchParams.get('code');
      
      if (!code) {
        throw new Error('Could not find authorization code in URL');
      }

      console.log('\n🔄 Exchanging code for refresh token...\n');
      
      const { tokens } = await oauth2Client.getToken(code);
      
      console.log('✅ SUCCESS! Your new refresh token:\n');
      console.log('='.repeat(60));
      console.log(tokens.refresh_token);
      console.log('='.repeat(60));
      console.log('\n📝 Add this to your .env file as GMAIL_REFRESH_TOKEN');
      console.log('Then run the refresh script again:\n');
      console.log('   npx tsx scripts/refresh-auth.ts\n');
      
    } catch (error: any) {
      console.error('❌ Error:', error.message);
      process.exit(1);
    }
  });
}

getBigQueryRefreshToken();
</file>

<file path="scripts/get-gmail-token.js">
// scripts/get-gmail-token.js
// One-time helper to mint a Gmail OAuth refresh token (read-only scope).

const { google } = require('googleapis');
const readline = require('readline');

async function main() {
  const clientId = process.env.GMAIL_CLIENT_ID;
  const clientSecret = process.env.GMAIL_CLIENT_SECRET;

  if (!clientId || !clientSecret) {
    console.error('ERROR: Set GMAIL_CLIENT_ID and GMAIL_CLIENT_SECRET env vars first.');
    process.exit(1);
  }

  // Use loopback redirect (supported). We won't actually listen locally;
  // you'll copy the ?code= value from the browser address bar after it "fails" to load.
  const REDIRECT_URI = 'http://localhost';

  const oAuth2Client = new google.auth.OAuth2(
    clientId,
    clientSecret,
    REDIRECT_URI
  );

  const scopes = ['https://www.googleapis.com/auth/gmail.readonly'];

  // Generate the auth URL
  const authUrl = oAuth2Client.generateAuthUrl({
    access_type: 'offline',
    prompt: 'consent',
    scope: scopes,
    redirect_uri: REDIRECT_URI,
  });

  console.log('\n1) Open this URL in your browser and approve access:\n');
  console.log(authUrl, '\n');

  console.log('2) After approving, your browser will try to open http://localhost and show a connection error.');
  console.log('   That is expected. COPY the value after "code=" from the address bar, up to but not including any "&".\n');

  // Prompt for the code
  const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
  rl.question('Paste the code here and press Enter: ', async (code) => {
    rl.close();
    try {
      const { tokens } = await oAuth2Client.getToken(code.trim());
      console.log('\nSUCCESS. Tokens:\n', JSON.stringify(tokens, null, 2));
      if (!tokens.refresh_token) {
        console.error('\nNOTE: No refresh_token returned. Make sure you:\n' +
          ' - Added your email as a Test User on the OAuth consent screen\n' +
          ' - Used prompt=consent (this script does)\n' +
          ' - Selected the account and clicked Allow\n' +
          'Then try again.');
      }
    } catch (e) {
      console.error('\nToken exchange failed:\n', e.response?.data || e.message);
      process.exit(1);
    }
  });
}

main();
</file>

<file path="scripts/ingest-and-chunk-inbox.ts">
/**
 * Quick ingestion script for specific inbox
 * Ingests, chunks, and updates publishers in one go
 */

import * as dotenv from 'dotenv';
import { spawn } from 'child_process';
import { promisify } from 'util';

dotenv.config();

async function runCommand(command: string, args: string[], env?: NodeJS.ProcessEnv): Promise<number> {
  return new Promise((resolve, reject) => {
    console.log(`\n▶️  Running: ${command} ${args.join(' ')}\n`);
    
    const childProcess = spawn(command, args, {
      env: { ...process.env, ...env },
      stdio: 'inherit',
      shell: true
    });
    
    childProcess.on('close', (code) => {
      if (code === 0) {
        resolve(0);
      } else {
        reject(new Error(`Process exited with code ${code}`));
      }
    });
    
    childProcess.on('error', (error) => {
      reject(error);
    });
  });
}

async function ingestAndChunkInbox() {
  console.log('🚀 Quick ingestion: CLEAN inbox (nsm@internationalintrigue.io)\n');
  console.log('This will:');
  console.log('  1. Ingest emails from CLEAN inbox (nsm@internationalintrigue.io)');
  console.log('  2. Chunk the messages');
  console.log('  3. Update publishers list\n');
  
  try {
    // Step 1: Ingest emails from the CLEAN inbox
    console.log('📥 Step 1: Ingesting emails from CLEAN inbox (nsm@internationalintrigue.io)...\n');
    
    // Use clean inbox (GMAIL_INBOX=clean)
    await runCommand('npx', ['tsx', 'scripts/ingest-to-bigquery.ts'], {
      GMAIL_INBOX: 'clean'
    });
    
    console.log('\n✅ Step 1 complete: Emails ingested\n');
    
    // Step 2: Chunk messages
    console.log('📦 Step 2: Chunking messages...\n');
    
    await runCommand('npx', ['tsx', 'scripts/process-newsletters.ts'], {});
    
    console.log('\n✅ Step 2 complete: Messages chunked\n');
    
    // Step 3: Update publishers
    console.log('👥 Step 3: Updating publishers list...\n');
    
    await runCommand('npx', ['tsx', 'scripts/publishers/extract-existing-publishers.ts'], {});
    
    console.log('\n✅ Step 3 complete: Publishers updated\n');
    
    console.log('🎉 All done! Inbox ingested, chunked, and publishers updated.\n');
    
  } catch (error: any) {
    console.error('\n❌ Error:', error.message);
    console.error('\n   The process may have partially completed.');
    console.error('   Check the output above to see which step failed.\n');
    process.exit(1);
  }
}

ingestAndChunkInbox()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/ingest-recent-inbox.ts">
/**
 * Ingest recent emails from inbox (last 24-48 hours)
 * Useful for new subscriptions
 */

import * as dotenv from 'dotenv';
import { spawn } from 'child_process';

dotenv.config();

async function runCommand(command: string, args: string[], env?: NodeJS.ProcessEnv): Promise<number> {
  return new Promise((resolve, reject) => {
    console.log(`\n▶️  Running: ${command} ${args.join(' ')}\n`);
    
    const childProcess = spawn(command, args, {
      env: { ...process.env, ...env },
      stdio: 'inherit',
      shell: true
    });
    
    childProcess.on('close', (code) => {
      if (code === 0) {
        resolve(0);
      } else {
        reject(new Error(`Process exited with code ${code}`));
      }
    });
    
    childProcess.on('error', (error) => {
      reject(error);
    });
  });
}

async function ingestRecentInbox() {
  console.log('🚀 Quick ingestion: Recent inbox emails (last 48 hours)\n');
  console.log('This will:');
  console.log('  1. Ingest emails from last 48 hours');
  console.log('  2. Chunk the messages');
  console.log('  3. Update publishers list\n');
  
  try {
    // Step 1: Ingest recent emails (last 48 hours)
    console.log('📥 Step 1: Ingesting recent emails (last 48 hours)...\n');
    
    // Search for emails from last 48 hours
    const query = 'in:inbox newer_than:2d';
    
    console.log(`   Search query: ${query}\n`);
    
    await runCommand('npx', ['tsx', 'scripts/ingest-to-bigquery.ts'], {
      GMAIL_INGESTION_QUERY: query
    });
    
    console.log('\n✅ Step 1 complete: Emails ingested\n');
    
    // Step 2: Chunk messages
    console.log('📦 Step 2: Chunking messages...\n');
    
    await runCommand('npx', ['tsx', 'scripts/process-newsletters.ts'], {});
    
    console.log('\n✅ Step 2 complete: Messages chunked\n');
    
    // Step 3: Update publishers
    console.log('👥 Step 3: Updating publishers list...\n');
    
    await runCommand('npx', ['tsx', 'scripts/publishers/extract-existing-publishers.ts'], {});
    
    console.log('\n✅ Step 3 complete: Publishers updated\n');
    
    console.log('🎉 All done! Recent emails ingested, chunked, and publishers updated.\n');
    
  } catch (error: any) {
    console.error('\n❌ Error:', error.message);
    console.error('\n   The process may have partially completed.');
    console.error('   Check the output above to see which step failed.\n');
    process.exit(1);
  }
}

ingestRecentInbox()
  .then(() => process.exit(0))
  .catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/ingest-specific-inbox.ts">
/**
 * Ingest emails from a specific inbox (email address)
 * Then chunk and update publishers
 */

import * as dotenv from 'dotenv';
import { getGmail } from '../src/lib/gmail';
import type { gmail_v1 } from 'googleapis';

dotenv.config();

const TARGET_INBOX = 'nsm@internationalintrigue.io';

async function ingestSpecificInbox() {
  console.log(`📥 Ingesting emails from inbox: ${TARGET_INBOX}\n`);

  try {
    // Use the existing ingestion script but with a custom query filter
    // The Gmail query will filter for emails TO this address
    const gmail = getGmail('legacy'); // or 'clean' depending on which inbox
    
    // Gmail search query: emails TO the specific address
    const query = `to:${TARGET_INBOX} in:inbox`;
    
    console.log(`   Search query: ${query}\n`);
    
    // Get message count estimate
    const listRes = await gmail.users.messages.list({
      userId: 'me',
      q: query,
      maxResults: 1
    });
    
    const totalEstimate = listRes.data.resultSizeEstimate || 0;
    console.log(`   Estimated messages: ${totalEstimate}\n`);
    
    if (totalEstimate === 0) {
      console.log('⚠️  No messages found for this inbox.\n');
      return;
    }
    
    // Now run the normal ingestion process with this query
    // We'll need to modify the ingestion script to accept a custom query
    // For now, let's use environment variable to pass the query
    process.env.GMAIL_INGESTION_QUERY = query;
    
    console.log('✅ Setting up ingestion query...\n');
    console.log('   Now running standard ingestion process...\n');
    
    // Import and run the ingestion script
    const { spawn } = require('child_process');
    const ingestProcess = spawn('npx', ['tsx', 'scripts/ingest-to-bigquery.ts'], {
      env: { ...process.env, GMAIL_INGESTION_QUERY: query },
      stdio: 'inherit'
    });
    
    ingestProcess.on('close', (code: number) => {
      if (code === 0) {
        console.log('\n✅ Ingestion complete!');
        console.log('\n   Next: Running chunking and publisher update...\n');
        
        // Run chunking
        const chunkProcess = spawn('npx', ['tsx', 'scripts/process-newsletters.ts'], {
          stdio: 'inherit'
        });
        
        chunkProcess.on('close', (chunkCode: number) => {
          if (chunkCode === 0) {
            console.log('\n✅ Chunking complete!');
            console.log('\n   Next: Updating publishers list...\n');
            
            // Update publishers
            const publisherProcess = spawn('npx', ['tsx', 'scripts/publishers/extract-existing-publishers.ts'], {
              stdio: 'inherit'
            });
            
            publisherProcess.on('close', (pubCode: number) => {
              if (pubCode === 0) {
                console.log('\n✅ All done! Publishers updated.\n');
              } else {
                console.error('\n❌ Publisher update failed');
              }
            });
          } else {
            console.error('\n❌ Chunking failed');
          }
        });
      } else {
        console.error('\n❌ Ingestion failed');
      }
    });
    
  } catch (error: any) {
    console.error('❌ Error:', error.message);
    throw error;
  }
}

// Actually, let's modify the ingestion script to accept a query parameter
// Or create a simpler wrapper that does all three steps

ingestSpecificInbox()
  .catch((error) => {
    console.error('Error:', error);
    process.exit(1);
  });
</file>

<file path="scripts/list-recent.ts">
import * as dotenv from 'dotenv';
import { getGmail } from '../src/lib/gmail';

dotenv.config();

(async () => {
  try {
    const gmail = getGmail();
    
    const res = await gmail.users.messages.list({ 
      userId: 'me', 
      q: 'newer_than:1d', 
      maxResults: 50 
    });
    
    const messages = res.data.messages || [];
    console.log(`found: ${messages.length} messages (showing up to 50 ids)`);
    
    messages.forEach(msg => {
      console.log(msg.id);
    });
    
    process.exit(0);
  } catch (error) {
    console.error('Error listing recent messages:', error);
    process.exit(1);
  }
})();
</file>

<file path="scripts/migrate-is-paid-column.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

async function migrateIsPaidColumn() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  try {
    console.log('🔄 Adding is_paid column to messages table...\n');
    
    // Check if column already exists
    console.log('📋 Checking current schema...');
    const [metadata] = await bigquery
      .dataset(DATASET_ID)
      .table(TABLE_ID)
      .getMetadata();
    
    const schema = metadata.schema?.fields || [];
    const hasIsPaid = schema.some(field => field.name === 'is_paid');
    
    if (hasIsPaid) {
      console.log('✅ is_paid column already exists\n');
      
      // Check if any rows need updating
      const [countResult] = await bigquery.query(`
        SELECT COUNT(*) as total, COUNT(is_paid) as with_is_paid
        FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      `);
      const row = countResult[0];
      
      if (row.with_is_paid < row.total && row.total > 0) {
        console.log(`📝 Found ${row.total - row.with_is_paid} rows with NULL is_paid`);
        console.log('   These will be updated during next ingestion run\n');
        return;
      } else {
        console.log('✅ All rows already have is_paid set\n');
        return;
      }
    }
    
    console.log('📝 Adding is_paid column...');
    
    // Add the new column
    const query = `
      ALTER TABLE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      ADD COLUMN IF NOT EXISTS is_paid BOOLEAN OPTIONS(description="True if newsletter is a paid subscription");
    `;
    
    await bigquery.query(query);
    console.log('✅ Column added\n');
    console.log('📝 Note: Existing rows will be set to NULL (can be updated later if needed)\n');
    
    console.log('🎉 Schema migration complete!\n');
    
  } catch (error) {
    console.error('❌ Migration failed:', error);
    throw error;
  }
}

migrateIsPaidColumn();
</file>

<file path="scripts/migrate-schema-dual-inbox.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

async function migrateSchema() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  try {
    console.log('🔄 Migrating BigQuery schema for dual inbox support...\n');
    
    // Check if column already exists
    console.log('📋 Checking current schema...');
    const [metadata] = await bigquery
      .dataset(DATASET_ID)
      .table(TABLE_ID)
      .getMetadata();
    
    const schema = metadata.schema?.fields || [];
    const hasSourceInbox = schema.some(field => field.name === 'source_inbox');
    
    if (hasSourceInbox) {
      console.log('✅ source_inbox column already exists\n');
      
      // Check if any rows need updating
      const [countResult] = await bigquery.query(`
        SELECT COUNT(*) as total, COUNT(source_inbox) as with_source
        FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      `);
      const row = countResult[0];
      
      if (row.with_source === 0 && row.total > 0) {
        console.log(`📝 Found ${row.total} rows with NULL source_inbox, updating...`);
        // Continue to UPDATE section below
      } else {
        console.log('✅ All rows already have source_inbox set\n');
        return;
      }
    }
    
    console.log('⚠️  source_inbox column not found\n');
    console.log('📝 Adding source_inbox column...');
    
    // Add the new column
    const query = `
      ALTER TABLE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      ADD COLUMN IF NOT EXISTS source_inbox STRING OPTIONS(description="Source inbox: legacy or clean");
    `;
    
    await bigquery.query(query);
    console.log('✅ Column added\n');
    
    // Update existing rows to 'legacy'
    console.log('📝 Setting existing rows to "legacy"...');
    const updateQuery = `
      UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      SET source_inbox = 'legacy'
      WHERE source_inbox IS NULL;
    `;
    
    const [job] = await bigquery.query(updateQuery);
    console.log('✅ All existing rows set to "legacy"\n');
    
    console.log('🎉 Schema migration complete!\n');
    
  } catch (error) {
    console.error('❌ Migration failed:', error);
    throw error;
  }
}

migrateSchema();
</file>

<file path="scripts/optimize-bigquery-tables.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';

async function optimizeTables() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  try {
    console.log('🔄 Optimizing BigQuery tables for partitioning and clustering...\n');
    
    // Table 1: messages
    console.log('📊 Optimizing messages table...');
    await optimizeMessagesTable(bigquery);
    
    // Table 2: chunks
    console.log('\n📊 Optimizing chunks table...');
    await optimizeChunksTable(bigquery);
    
    console.log('\n🎉 Table optimization complete!\n');
    
  } catch (error) {
    console.error('❌ Optimization failed:', error);
    throw error;
  }
}

async function optimizeMessagesTable(bigquery: BigQuery) {
  const tableId = 'messages';
  
  try {
    // Check if table is already partitioned/clustered
    const [metadata] = await bigquery
      .dataset(DATASET_ID)
      .table(tableId)
      .getMetadata();
    
    const isPartitioned = metadata.timePartitioning !== undefined;
    const isClustered = metadata.clustering?.fields !== undefined;
    
    if (isPartitioned && isClustered) {
      console.log('✅ messages table is already optimized');
      return;
    }
    
    console.log(`   Current state: ${isPartitioned ? 'Partitioned' : 'Not partitioned'}, ${isClustered ? 'Clustered' : 'Not clustered'}`);
    console.log('⚠️  NOTE: BigQuery does not support ALTER TABLE for partitioning/clustering');
    console.log('   Tables can only be partitioned/clustered at creation time.');
    console.log('   This table will be optimized on next ingestion run.');
    
  } catch (error) {
    console.error(`❌ Failed to check ${tableId}:`, error);
  }
}

async function optimizeChunksTable(bigquery: BigQuery) {
  const tableId = 'chunks';
  
  try {
    // Check if table is already partitioned/clustered
    const [metadata] = await bigquery
      .dataset(DATASET_ID)
      .table(tableId)
      .getMetadata();
    
    const isPartitioned = metadata.timePartitioning !== undefined;
    const isClustered = metadata.clustering?.fields !== undefined;
    
    if (isPartitioned && isClustered) {
      console.log('✅ chunks table is already optimized');
      return;
    }
    
    console.log(`   Current state: ${isPartitioned ? 'Partitioned' : 'Not partitioned'}, ${isClustered ? 'Clustered' : 'Not clustered'}`);
    console.log('⚠️  NOTE: BigQuery does not support ALTER TABLE for partitioning/clustering');
    console.log('   Tables can only be partitioned/clustered at creation time.');
    console.log('   This table will be optimized on next ingestion run.');
    
  } catch (error) {
    console.error(`❌ Failed to check ${tableId}:`, error);
  }
}

optimizeTables();
</file>

<file path="scripts/preview-vip.ts">
import * as dotenv from 'dotenv';
import { getGmail, extractEmailAddress } from '../src/lib/gmail';
import { extractPlaintext, getHeader } from '../src/lib/parseMessage';
import vipConfig from '../config/vip.json';

dotenv.config();

interface VipMessage {
  fromEmail: string;
  subject: string;
  plaintext: string;
}

function isVip(fromEmail: string): boolean {
  // Check if email exactly matches any VIP sender
  if (vipConfig.senders.includes(fromEmail)) {
    return true;
  }
  
  // Check if domain matches any VIP domain
  const domain = fromEmail.split('@')[1]?.toLowerCase();
  if (domain && vipConfig.domains.includes(domain)) {
    return true;
  }
  
  return false;
}

(async () => {
  try {
    const gmail = getGmail();
    
    // Get recent messages
    const listRes = await gmail.users.messages.list({ 
      userId: 'me', 
      q: 'newer_than:1d', 
      maxResults: 50 
    });
    
    const messageIds = listRes.data.messages || [];
    console.log(`Processing ${messageIds.length} messages for VIP content...`);
    
    const vipMessages: VipMessage[] = [];
    
    // Process each message
    for (const msg of messageIds) {
      if (vipMessages.length >= 10) break; // Stop after finding 10 VIPs
      
      const fullMsg = await gmail.users.messages.get({ 
        userId: 'me', 
        id: msg.id!, 
        format: 'full' 
      });
      
      const from = getHeader(fullMsg.data, 'From');
      const subject = getHeader(fullMsg.data, 'Subject');
      const fromEmail = extractEmailAddress(from);
      
      if (isVip(fromEmail)) {
        const plaintext = extractPlaintext(fullMsg.data);
        
        vipMessages.push({
          fromEmail,
          subject,
          plaintext
        });
      }
    }
    
    console.log(`\nFound ${vipMessages.length} VIP messages in the past 24 h\n`);
    
    vipMessages.forEach(msg => {
      console.log(`${msg.fromEmail} | ${msg.subject}`);
      console.log(`${msg.plaintext.substring(0, 100)}…`);
      console.log('---');
    });
    
    process.exit(0);
  } catch (error) {
    console.error('Error previewing VIP messages:', error);
    process.exit(1);
  }
})();
</file>

<file path="scripts/refresh-auth.ts">
import * as dotenv from 'dotenv';
import { google } from 'googleapis';
import { readFileSync, writeFileSync } from 'fs';
import { homedir } from 'os';
import * as path from 'path';

dotenv.config();

const ADC_PATH = path.join(homedir(), '.config', 'gcloud', 'application_default_credentials.json');

async function refreshAuth() {
  try {
    console.log('🔄 Refreshing Google Cloud authentication...\n');
    
    const { GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET, GMAIL_REFRESH_TOKEN } = process.env;
    
    if (!GMAIL_CLIENT_ID || !GMAIL_CLIENT_SECRET || !GMAIL_REFRESH_TOKEN) {
      throw new Error('Missing Gmail credentials in .env file');
    }

    console.log('Step 1: Getting new access token...');
    const oauth2Client = new google.auth.OAuth2(
      GMAIL_CLIENT_ID,
      GMAIL_CLIENT_SECRET,
      'urn:ietf:wg:oauth:2.0:oob'
    );

    oauth2Client.setCredentials({ refresh_token: GMAIL_REFRESH_TOKEN });
    
    // Force refresh
    const { credentials } = await oauth2Client.refreshAccessToken();
    
    console.log('✅ Successfully obtained new credentials\n');
    
    console.log('Step 2: Writing credentials to ADC file...');
    
    // Create directory if it doesn't exist
    const { mkdirSync } = require('fs');
    mkdirSync(path.dirname(ADC_PATH), { recursive: true });
    
    // Write ADC file
    const adcData = {
      client_id: GMAIL_CLIENT_ID,
      client_secret: GMAIL_CLIENT_SECRET,
      refresh_token: GMAIL_REFRESH_TOKEN,
      type: 'authorized_user',
      quota_project_id: 'newsletter-control-center'
    };
    
    writeFileSync(ADC_PATH, JSON.stringify(adcData, null, 2));
    
    console.log(`✅ Credentials written to: ${ADC_PATH}\n`);
    console.log('🎉 Authentication refresh complete!');
    console.log('\n⚠️  Note: These credentials will expire in ~7 days.');
    console.log('   For long-term authentication, you need to either:');
    console.log('   1. Create a service account key (if org policy allows)');
    console.log('   2. Set up Workload Identity Federation');
    console.log('   3. Run the processing job in Cloud Run/Cloud Build\n');
    
  } catch (error: any) {
    console.error('❌ Failed to refresh authentication:', error.message);
    if (error.message.includes('invalid_grant')) {
      console.log('\n⚠️  Your refresh token has expired or been revoked.');
      console.log('   You need to get a new refresh token using the Gmail OAuth flow.');
      console.log('   Run: npm run get-gmail-token\n');
    }
    process.exit(1);
  }
}

refreshAuth();
</file>

<file path="scripts/update-vip-flags.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { getGmail } from '../src/lib/gmail';
import vipConfig from '../config/vip.json';

dotenv.config();

// BigQuery configuration
const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

// Initialize BigQuery client using Application Default Credentials
const bigquery = new BigQuery({ projectId: PROJECT_ID });

interface VipUpdateResult {
  sender: string;
  updatedCount: number;
}

/**
 * Update VIP flags for a specific sender
 */
async function updateVipFlagsForSender(sender: string): Promise<number> {
  const query = `
    UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
    SET is_vip = true
    WHERE sender = @sender
      AND is_vip = false
  `;
  
  const options = {
    query: query,
    params: {
      sender: sender
    }
  };
  
  const [job] = await bigquery.createQueryJob(options);
  const [rows] = await job.getQueryResults();
  
  // Get the number of affected rows from job metadata
  const [jobMetadata] = await job.getMetadata();
  return jobMetadata.statistics?.query?.numDmlAffectedRows || 0;
}

/**
 * Get current VIP statistics
 */
async function getVipStatistics(): Promise<{ total: number; vip: number; nonVip: number }> {
  const query = `
    SELECT 
      COUNT(*) as total,
      SUM(CASE WHEN is_vip = true THEN 1 ELSE 0 END) as vip,
      SUM(CASE WHEN is_vip = false THEN 1 ELSE 0 END) as non_vip
    FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
  `;
  
  const [rows] = await bigquery.query(query);
  return {
    total: parseInt(rows[0].total),
    vip: parseInt(rows[0].vip),
    nonVip: parseInt(rows[0].non_vip)
  };
}

(async () => {
  try {
    console.log('🚀 Starting VIP flags backfill...');
    console.log(`📊 Project: ${PROJECT_ID}`);
    console.log(`📊 Dataset: ${DATASET_ID}`);
    console.log(`📊 Table: ${TABLE_ID}\n`);
    
    // Initialize Gmail client to establish OAuth2 authentication context
    console.log('🔐 Establishing authentication...');
    const gmail = getGmail();
    console.log('✅ Authentication established\n');
    
    // Get initial statistics
    console.log('📈 Current VIP statistics:');
    const initialStats = await getVipStatistics();
    console.log(`   Total newsletters: ${initialStats.total.toLocaleString()}`);
    console.log(`   VIP newsletters: ${initialStats.vip.toLocaleString()}`);
    console.log(`   Non-VIP newsletters: ${initialStats.nonVip.toLocaleString()}\n`);
    
    // Process each VIP sender
    const vipSenders = vipConfig.senders;
    console.log(`🔄 Processing ${vipSenders.length} VIP senders...\n`);
    
    const results: VipUpdateResult[] = [];
    let totalUpdated = 0;
    
    for (let i = 0; i < vipSenders.length; i++) {
      const sender = vipSenders[i];
      const senderNumber = i + 1;
      
      try {
        console.log(`📤 [${senderNumber}/${vipSenders.length}] Updating ${sender}...`);
        
        const updatedCount = await updateVipFlagsForSender(sender);
        
        results.push({
          sender: sender,
          updatedCount: updatedCount
        });
        
        totalUpdated += updatedCount;
        
        if (updatedCount > 0) {
          console.log(`   ✅ Updated ${updatedCount} newsletters`);
        } else {
          console.log(`   ⏭️  No newsletters found to update`);
        }
        
      } catch (error) {
        console.error(`   ❌ Error updating ${sender}:`, error);
        results.push({
          sender: sender,
          updatedCount: 0
        });
      }
    }
    
    // Get final statistics
    console.log('\n📈 Final VIP statistics:');
    const finalStats = await getVipStatistics();
    console.log(`   Total newsletters: ${finalStats.total.toLocaleString()}`);
    console.log(`   VIP newsletters: ${finalStats.vip.toLocaleString()}`);
    console.log(`   Non-VIP newsletters: ${finalStats.nonVip.toLocaleString()}`);
    
    // Show detailed results
    console.log('\n📊 VIP Update Summary:');
    console.log('='.repeat(60));
    
    const successfulUpdates = results.filter(r => r.updatedCount > 0);
    const failedUpdates = results.filter(r => r.updatedCount === 0);
    
    if (successfulUpdates.length > 0) {
      console.log('\n✅ Successfully updated:');
      successfulUpdates.forEach(result => {
        console.log(`   ${result.sender}: ${result.updatedCount} newsletters`);
      });
    }
    
    if (failedUpdates.length > 0) {
      console.log('\n⏭️  No updates needed:');
      failedUpdates.forEach(result => {
        console.log(`   ${result.sender}: 0 newsletters`);
      });
    }
    
    console.log('\n🎉 BACKFILL COMPLETE!');
    console.log('='.repeat(60));
    console.log(`📊 Total newsletters updated: ${totalUpdated.toLocaleString()}`);
    console.log(`📊 VIP senders processed: ${vipSenders.length}`);
    console.log(`📊 Successful updates: ${successfulUpdates.length}`);
    console.log(`📊 No updates needed: ${failedUpdates.length}`);
    
    process.exit(0);
    
  } catch (error) {
    console.error('💥 Fatal error during VIP flags backfill:', error);
    process.exit(1);
  }
})();
</file>

<file path="scripts/verify-env.ts">
#!/usr/bin/env ts-node

import * as dotenv from 'dotenv';
import { exit } from 'process';

// Load environment variables from .env file
dotenv.config();

// Required environment variables
const requiredEnvVars = [
  'GMAIL_CLIENT_ID',
  'GMAIL_CLIENT_SECRET', 
  'GMAIL_REFRESH_TOKEN'
];

// Check if all required environment variables are present
const missingVars: string[] = [];

for (const envVar of requiredEnvVars) {
  if (!process.env[envVar]) {
    missingVars.push(envVar);
  }
}

// If any variables are missing, exit with error
if (missingVars.length > 0) {
  console.error('❌ Missing required environment variables:');
  missingVars.forEach(varName => {
    console.error(`   - ${varName}`);
  });
  console.error('\n💡 Please check your .env file and ensure all required variables are set.');
  console.error('   You can copy .env.example to .env and fill in the values.');
  exit(1);
}

// All variables are present
console.log('✅ All required environment variables are present:');
requiredEnvVars.forEach(varName => {
  const value = process.env[varName];
  const maskedValue = value ? value.substring(0, 4) + '...' : 'undefined';
  console.log(`   - ${varName}: ${maskedValue}`);
});

console.log('\n🎉 Environment validation passed!');
exit(0);
</file>

<file path="scripts/whoami.ts">
import * as fs from 'fs';
import * as path from 'path';
import * as dotenv from 'dotenv';
import { getGmail } from '../src/lib/gmail';

const envPath = path.resolve(process.cwd(), '.env');
const hasEnv = fs.existsSync(envPath);
console.log('whoami.ts starting…');
console.log('cwd:', process.cwd());
console.log('.env present:', hasEnv);

dotenv.config();

const hasId = !!process.env.GMAIL_CLIENT_ID;
const hasSecret = !!process.env.GMAIL_CLIENT_SECRET;
const hasRefresh = !!process.env.GMAIL_REFRESH_TOKEN;
console.log('env present -> client_id:', hasId, ' client_secret:', hasSecret, ' refresh_token:', hasRefresh);

(async () => {
  try {
    console.log('creating gmail client…');
    const gmail = getGmail();
    console.log('calling users.getProfile…');
    const res = await gmail.users.getProfile({ userId: 'me' });
    console.log('✅ Authenticated as:', res.data.emailAddress);
    process.exit(0);
  } catch (error) {
    console.error('❌ Error getting Gmail profile:', error);
    process.exit(1);
  }
})();
</file>

<file path="src/lib/deduplication.ts">
import type { gmail_v1 } from 'googleapis';

/**
 * Generate a unique deduplication key for a Gmail message
 * Uses Message-ID + List-Id headers for newsletter uniqueness
 */
export interface DedupeKey {
  messageId: string;      // Gmail Message-ID header
  listId?: string;        // List-Id header (newsletter unique)
  sender: string;         // From email address
  subject: string;        // Subject line
  sentDate: string;       // Sent date
}

/**
 * Extract deduplication key from a Gmail message
 */
export function generateDedupeKey(message: gmail_v1.Schema$Message): DedupeKey {
  const headers = message.payload?.headers || [];
  
  const getHeader = (name: string): string => {
    return headers.find(h => h.name?.toLowerCase() === name.toLowerCase())?.value || '';
  };
  
  const messageId = getHeader('Message-ID') || '';
  const listId = getHeader('List-Id') || getHeader('List-ID') || '';
  const fromHeader = getHeader('From') || '';
  const subject = getHeader('Subject') || '';
  const date = getHeader('Date') || '';
  
  // Extract email address from From header
  const sender = extractEmailFromHeader(fromHeader);
  
  return {
    messageId,
    listId: listId || undefined,
    sender,
    subject,
    sentDate: date
  };
}

/**
 * Generate a canonical string key for comparison
 */
export function keyToString(key: DedupeKey): string {
  // Use List-Id for newsletters (most reliable), fall back to Message-ID
  const primaryId = key.listId || key.messageId;
  return `${primaryId}|${key.sender}|${key.subject}|${key.sentDate}`;
}

/**
 * Extract email address from From header
 */
function extractEmailFromHeader(fromHeader: string): string {
  if (!fromHeader) return '';
  
  // Extract from angle brackets: "Name <user@example.com>"
  const angleMatch = fromHeader.match(/<([^>]+)>/);
  if (angleMatch && angleMatch[1]) return angleMatch[1].trim().toLowerCase();
  
  // Fallback: try bare email in string
  const emailMatch = fromHeader.match(/[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}/i);
  return emailMatch ? emailMatch[0].trim().toLowerCase() : fromHeader.trim().toLowerCase();
}

/**
 * Check if a message is a duplicate based on existing keys
 */
export function isDuplicate(key: DedupeKey, existingKeys: Set<string>): boolean {
  const keyStr = keyToString(key);
  return existingKeys.has(keyStr);
}
</file>

<file path="src/lib/gmail.ts">
import { google } from 'googleapis';

import type { gmail_v1 } from 'googleapis';

/**
 * Returns an authenticated Gmail client using a long-lived refresh token.
 * Supports multiple inboxes: 'legacy' or 'clean'
 * 
 * Legacy mode: Uses GMAIL_LEGACY_REFRESH_TOKEN (backward compatible with GMAIL_REFRESH_TOKEN)
 * Clean mode: Uses GMAIL_CLEAN_REFRESH_TOKEN
 * 
 * Assumes the following environment variables are already set at runtime:
 *   GMAIL_CLIENT_ID, GMAIL_CLIENT_SECRET, 
 *   GMAIL_LEGACY_REFRESH_TOKEN (or GMAIL_REFRESH_TOKEN for backward compatibility),
 *   GMAIL_CLEAN_REFRESH_TOKEN (for clean inbox)
 */
export function getGmail(inboxType: 'legacy' | 'clean' = 'legacy'): gmail_v1.Gmail {
  const {
    GMAIL_CLIENT_ID,
    GMAIL_CLIENT_SECRET,
    GMAIL_REFRESH_TOKEN,           // Backward compatible
    GMAIL_LEGACY_REFRESH_TOKEN,    // New dual inbox
    GMAIL_CLEAN_REFRESH_TOKEN,     // New dual inbox
  } = process.env;

  if (!GMAIL_CLIENT_ID || !GMAIL_CLIENT_SECRET) {
    throw new Error('Missing Gmail env vars: GMAIL_CLIENT_ID/GMAIL_CLIENT_SECRET');
  }

  // Select the appropriate refresh token
  let refreshToken: string | undefined;
  
  if (inboxType === 'clean') {
    refreshToken = GMAIL_CLEAN_REFRESH_TOKEN;
  } else {
    // For legacy, try GMAIL_LEGACY_REFRESH_TOKEN first, fall back to GMAIL_REFRESH_TOKEN
    refreshToken = GMAIL_LEGACY_REFRESH_TOKEN || GMAIL_REFRESH_TOKEN;
  }
  
  if (!refreshToken) {
    throw new Error(`Missing refresh token for ${inboxType} inbox. Check your .env file for GMAIL_${inboxType.toUpperCase()}_REFRESH_TOKEN or GMAIL_REFRESH_TOKEN`);
  }

  const oAuth2Client = new google.auth.OAuth2({
    clientId: GMAIL_CLIENT_ID,
    clientSecret: GMAIL_CLIENT_SECRET,
    redirectUri: 'urn:ietf:wg:oauth:2.0:oob', // unused with refresh token but required by constructor
  });

  oAuth2Client.setCredentials({ refresh_token: refreshToken });

  return google.gmail({ version: 'v1', auth: oAuth2Client });
}

/**
 * Extracts a plain email address from a From header.
 * Examples:
 *   "Name <user@example.com>" -> "user@example.com"
 *   "user@example.com"        -> "user@example.com"
 */
export function extractEmailAddress(fromHeader: string): string {
  if (!fromHeader) return '';

  // Common cases: `"Name" <user@example.com>` or `Name <user@example.com>`
  const angleMatch = fromHeader.match(/<([^>]+)>/);
  if (angleMatch && angleMatch[1]) return angleMatch[1].trim().toLowerCase();

  // Fallback: try a bare email inside the string
  const emailMatch = fromHeader.match(/[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}/i);
  return emailMatch ? emailMatch[0].trim().toLowerCase() : fromHeader.trim().toLowerCase();
}

/**
 * Apply "Ingested" label to a message in Gmail
 * 
 * This function automatically creates the "Ingested" label if it doesn't exist,
 * then applies it to the specified message.
 * 
 * Used to mark newsletters that have been successfully processed.
 * 
 * @param gmail Authenticated Gmail client
 * @param messageId Gmail message ID to label
 * @param labelName Optional label name (defaults to "Ingested")
 * @returns void (logs on failure, doesn't throw)
 */
export async function markAsIngested(
  gmail: gmail_v1.Gmail, 
  messageId: string,
  labelName: string = 'Ingested'
): Promise<void> {
  try {
    // Get or create the label
    const labels = await gmail.users.labels.list({ userId: 'me' });
    let ingestedLabel = labels.data.labels?.find(l => l.name?.toLowerCase() === labelName.toLowerCase());
    
    if (!ingestedLabel) {
      // Create it if doesn't exist
      const newLabel = await gmail.users.labels.create({
        userId: 'me',
        requestBody: { name: labelName }
      });
      ingestedLabel = newLabel.data;
    }
    
    if (!ingestedLabel.id) {
      throw new Error(`Could not get or create "${labelName}" label`);
    }
    
    // Apply the label
    await gmail.users.messages.modify({
      userId: 'me',
      id: messageId,
      requestBody: {
        addLabelIds: [ingestedLabel.id]
      }
    });
    
  } catch (error) {
    console.error(`⚠️  Failed to apply "${labelName}" label to ${messageId}:`, error instanceof Error ? error.message : error);
    // Don't throw - labeling failure shouldn't stop ingestion
  }
}
</file>

<file path="src/lib/parseMessage.ts">
import type { gmail_v1 } from 'googleapis';

// Safe base64url decoder with fallback to base64
function decodeBase64Url(s: string): string {
  try {
    // Gmail parts are base64url-encoded
    const b = Buffer.from(s, 'base64url');
    return b.toString('utf8');
  } catch {
    try {
      return Buffer.from(s, 'base64').toString('utf8');
    } catch {
      return '';
    }
  }
}

// HTML to text converter with basic entity decoding and whitespace collapse
export function htmlToText(html: string): string {
  const dec = html
    .replace(/<script[\s\S]*?<\/script>/gi, ' ')
    .replace(/<style[\s\S]*?<\/style>/gi, ' ')
    .replace(/<[^>]+>/g, ' ')
    .replace(/&nbsp;/g, ' ')
    .replace(/&amp;/g, '&')
    .replace(/&lt;/g, '<')
    .replace(/&gt;/g, '>')
    .replace(/\s+/g, ' ')
    .trim();
  return dec;
}

// Try to find body content; prefer HTML, fall back to text/plain, then snippet.
export function extractPlaintext(msg: gmail_v1.Schema$Message): string {
  if (!msg || !msg.payload) return '';

  const parts: gmail_v1.Schema$MessagePart[] = [];

  // Flatten all parts (payload or payload.parts recursively)
  function walk(part?: gmail_v1.Schema$MessagePart) {
    if (!part) return;
    parts.push(part);
    if (part.parts) part.parts.forEach(walk);
  }

  walk(msg.payload);

  // Helper to get body text for a part
  const getBody = (p: gmail_v1.Schema$MessagePart) => {
    const data = p.body?.data;
    if (!data) return '';
    return decodeBase64Url(data);
  };

  // Prefer HTML (more complete content)
  for (const p of parts) {
    if ((p.mimeType || '').toLowerCase().startsWith('text/html')) {
      const html = getBody(p);
      const text = htmlToText(html);
      if (text.length < 10) {
        console.warn(`parse: body too short for gmail_id=${msg.id}`);
      }
      return text;
    }
  }

  // Fallback to text/plain
  for (const p of parts) {
    if ((p.mimeType || '').toLowerCase().startsWith('text/plain')) {
      const text = getBody(p).trim();
      if (text.length < 10) {
        console.warn(`parse: body too short for gmail_id=${msg.id}`);
      }
      return text;
    }
  }

  // Last resort: Gmail snippet
  const snippet = (msg.snippet || '').trim();
  if (snippet.length < 10 && snippet.length > 0) {
    console.warn(`parse: body too short for gmail_id=${msg.id}`);
  }
  return snippet;
}

// Grab a header value by name (e.g., 'From', 'Subject', 'Date')
export function getHeader(msg: gmail_v1.Schema$Message, name: string): string {
  const headers = msg.payload?.headers || [];
  const h = headers.find(h => (h.name || '').toLowerCase() === name.toLowerCase());
  return (h?.value || '').trim();
}
</file>

<file path="src/types.ts">
export type Email = {
  id: string;           // Gmail message ID
  threadId: string;     // Gmail thread ID
  from: string;         // full "From" header
  fromEmail: string;    // extracted plain email address
  subject: string;
  date: string;         // ISO timestamp
  snippet: string;      // short Gmail preview
  plaintext: string;    // plain-text body (best effort)
  gmailLink: string;    // direct link to open in Gmail
};

export type FetchResult = {
  vip: Email[];
  nonVip: Email[];
};
</file>

<file path=".env.example">
# Gmail API Configuration
# Copy this file to .env and fill in your actual values
# Never commit the .env file to version control

GMAIL_CLIENT_ID=your_gmail_client_id_here
GMAIL_CLIENT_SECRET=your_gmail_client_secret_here
GMAIL_REFRESH_TOKEN=your_gmail_refresh_token_here

# BigQuery Configuration
BIGQUERY_PROJECT_ID=newsletter-control-center

# Google Custom Search API (for Beehiiv and web search discovery)
GOOGLE_CUSTOM_SEARCH_API_KEY=your_google_custom_search_api_key_here
GOOGLE_CUSTOM_SEARCH_ENGINE_ID=your_google_custom_search_engine_id_here

# Optional: Perplexity API (alternative to Google Custom Search)
PERPLEXITY_API_KEY=your_perplexity_api_key_here

# BigQuery Production Configuration
BQ_PROJECT_ID=your-gcp-project-id
BQ_DATASET=ncc_production
BQ_LOCATION=US

# Admin Configuration
ADMIN_TOKEN=replace-with-a-strong-random-string

# Gmail Labels
GMAIL_INGEST_LABEL=Ingested
GMAIL_PAID_LABEL=Paid $

# --- Google Cloud auth (local dev) ---
# Path to your service account JSON key (absolute or relative to repo root).
# Example (relative): ./secrets/gcp/ncc-local-dev.json
GOOGLE_APPLICATION_CREDENTIALS=
</file>

<file path="CHECK_28K_CORPUS.sh">
#!/bin/bash

# Quality checks for 28K corpus
# Run this in Cloud Shell

echo "═══════════════════════════════════════════════════════════════"
echo "📊 QUALITY CHECKS FOR 28K CORPUS"
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Check 1: Total counts
echo "✅ CHECK 1: Total counts"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT COUNT(*) as chunks, 
          COUNT(DISTINCT newsletter_id) as newsletters,
          MAX(created_at) as most_recent
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`"
echo ""

# Check 2: No duplicates
echo "✅ CHECK 2: Duplicate detection"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT newsletter_id, chunk_index, COUNT(*) as dup_count 
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`
   GROUP BY newsletter_id, chunk_index 
   HAVING COUNT(*) > 1 
   LIMIT 10"
echo ""

# Check 3: Chunk distribution
echo "✅ CHECK 3: Chunk distribution"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT MIN(chunk_count) as min_chunks,
          MAX(chunk_count) as max_chunks,
          AVG(chunk_count) as avg_chunks
   FROM (
     SELECT newsletter_id, COUNT(*) as chunk_count
     FROM \`newsletter-control-center.ncc_newsletters.chunks\`
     GROUP BY newsletter_id
   )"
echo ""

# Check 4: Embeddings quality
echo "✅ CHECK 4: Embeddings quality"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT COUNT(*) as total_chunks,
          SUM(CASE WHEN chunk_embedding IS NULL THEN 1 ELSE 0 END) as null_embeddings,
          SUM(CASE WHEN ARRAY_LENGTH(chunk_embedding) != 768 THEN 1 ELSE 0 END) as wrong_dim
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`"
echo ""

# Check 5: Publisher diversity
echo "✅ CHECK 5: Publisher diversity"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT COUNT(DISTINCT publisher_name) as unique_publishers
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`"
echo ""

# Check 6: Sample content
echo "✅ CHECK 6: Content samples (readability)"
echo "─────────────────────────────────────────────────────────────"
bq query --use_legacy_sql=false \
  "SELECT newsletter_id, 
          chunk_index,
          SUBSTR(chunk_text, 1, 150) as text_sample
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`
   TABLESAMPLE SYSTEM (0.1 PERCENT)
   ORDER BY RAND()
   LIMIT 5"
echo ""

echo "═══════════════════════════════════════════════════════════════"
echo "✅ ALL CHECKS COMPLETE"
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="create-service-account-key.sh">
#!/bin/bash
# Script to create service account key
# Policies can take 5-10 minutes to propagate, so this will retry

SA_EMAIL="newsletter-local-dev@newsletter-control-center.iam.gserviceaccount.com"
KEY_FILE="$HOME/.gcloud/newsletter-local-dev-key.json"

echo "═══════════════════════════════════════════════════════════════"
echo "Creating Service Account Key"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "Service Account: $SA_EMAIL"
echo "Key File: $KEY_FILE"
echo ""
echo "Note: If org policy was just changed, it can take 5-10 minutes"
echo "      to propagate. This script will retry every 30 seconds."
echo ""

MAX_ATTEMPTS=10
ATTEMPT=1

while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
  echo "[Attempt $ATTEMPT/$MAX_ATTEMPTS] Trying to create key..."
  
  if gcloud iam service-accounts keys create "$KEY_FILE" --iam-account="${SA_EMAIL}" 2>&1; then
    echo ""
    echo "🎉 SUCCESS! Key created!"
    
    if [ -f "$KEY_FILE" ] && [ -s "$KEY_FILE" ]; then
      KEY_SIZE=$(wc -c < "$KEY_FILE")
      echo "   File: $KEY_FILE"
      echo "   Size: $KEY_SIZE bytes"
      
      if command -v jq >/dev/null 2>&1; then
        EMAIL=$(jq -r '.client_email' "$KEY_FILE" 2>/dev/null)
        echo "   Service Account: $EMAIL"
      fi
      
      echo ""
      echo "✅ Key created successfully!"
      echo ""
      echo "Environment variable is already set in ~/.zshrc"
      echo "Run: source ~/.zshrc (or restart terminal)"
      echo ""
      exit 0
    fi
  fi
  
  if [ $ATTEMPT -lt $MAX_ATTEMPTS ]; then
    echo "   Still blocked (policy may still be propagating...)"
    echo "   Waiting 30 seconds before retry..."
    sleep 30
  fi
  
  ATTEMPT=$((ATTEMPT + 1))
done

echo ""
echo "⚠️  Key creation still blocked after $MAX_ATTEMPTS attempts"
echo ""
echo "Possible reasons:"
echo "  1. Policy hasn't propagated yet (wait 10-15 minutes total)"
echo "  2. Managed constraint still enforced (check console)"
echo "  3. Cached policy (might need to wait longer)"
echo ""
echo "Check policy status:"
echo "  https://console.cloud.google.com/iam-admin/org-policies?organizationId=454540305091"
echo ""
exit 1
</file>

<file path="DEPLOY_EVAL.sh">
#!/bin/bash

# Deploy RAG Evaluation Harness to Cloud Run

set -e

echo "═══════════════════════════════════════════════════════════════"
echo "🚀 DEPLOYING RAG EVALUATION HARNESS"
echo "═══════════════════════════════════════════════════════════════"
echo ""

PROJECT_ID="newsletter-control-center"
REGION="us-central1"
IMAGE="gcr.io/${PROJECT_ID}/eval-rag"
JOB_NAME="eval-rag"

# Step 1: Build the Docker image
echo "📦 Building Docker image..."
gcloud builds submit --tag ${IMAGE} --file Dockerfile.eval

# Step 2: Create or update Cloud Run job
echo ""
echo "☁️  Creating Cloud Run job..."

# Check if job exists
if gcloud run jobs describe ${JOB_NAME} --region=${REGION} &>/dev/null; then
    echo "   ℹ️  Job exists, updating..."
    gcloud run jobs update ${JOB_NAME} \
        --image=${IMAGE} \
        --region=${REGION} \
        --memory=2Gi \
        --cpu=2 \
        --max-retries=0 \
        --task-timeout=1200 \
        --set-env-vars="BIGQUERY_PROJECT_ID=${PROJECT_ID}"
else
    echo "   ℹ️  Creating new job..."
    gcloud run jobs create ${JOB_NAME} \
        --image=${IMAGE} \
        --region=${REGION} \
        --memory=2Gi \
        --cpu=2 \
        --max-retries=0 \
        --task-timeout=1200 \
        --set-env-vars="BIGQUERY_PROJECT_ID=${PROJECT_ID}" \
        --set-secrets="GMAIL_CLIENT_ID=GMAIL_CLIENT_ID:latest,GMAIL_CLIENT_SECRET=GMAIL_CLIENT_SECRET:latest"
fi

echo ""
echo "✅ Deployment complete!"
echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "🏃 TO RUN THE EVALUATION:"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "  gcloud run jobs execute ${JOB_NAME} --region=${REGION}"
echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "📊 TO VIEW RESULTS:"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "  gcloud logging read \"resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME}\" --limit 50 --format json"
echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""
</file>

<file path="DEPLOY_FIX.sh">
#!/bin/bash

# Deploy the fixed Cloud Run job
# Run this in Google Cloud Shell after pulling the latest code

set -e  # Exit on error

echo "═══════════════════════════════════════════════════════════════"
echo "🔧 DEPLOYING FIXED VERSION"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "This will:"
echo "  1. Rebuild the Docker image with the cursor-based pagination fix"
echo "  2. Update the Cloud Run job"
echo "  3. Show you how to restart the job"
echo ""

# Step 1: Rebuild Docker image
echo "Step 1: Rebuilding Docker image (this takes ~5 minutes)..."
echo "═══════════════════════════════════════════════════════════════"
gcloud builds submit --tag gcr.io/newsletter-control-center/process-newsletters:latest

echo ""
echo "✅ Image built successfully!"
echo ""

# Step 2: Update Cloud Run job
echo "Step 2: Updating Cloud Run job..."
echo "═══════════════════════════════════════════════════════════════"
gcloud run jobs update process-newsletters \
  --image gcr.io/newsletter-control-center/process-newsletters:latest \
  --region us-central1

echo ""
echo "✅ Job updated successfully!"
echo ""

# Step 3: Check if there's a failed execution
echo "Step 3: Checking for previous failed executions..."
echo "═══════════════════════════════════════════════════════════════"

LATEST_EXEC=$(gcloud run jobs executions list \
  --job=process-newsletters \
  --region=us-central1 \
  --limit=1 \
  --format="value(name)" 2>/dev/null || echo "")

if [ -n "$LATEST_EXEC" ]; then
  FAILED_COUNT=$(gcloud run jobs executions describe "$LATEST_EXEC" \
    --region=us-central1 \
    --format="value(status.failedCount)" 2>/dev/null || echo "0")
  
  if [ "$FAILED_COUNT" = "1" ]; then
    echo "⚠️  Found a failed execution. The job will automatically resume from the last processed ID."
    echo ""
  fi
fi

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "✅ DEPLOYMENT COMPLETE!"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "🚀 To restart the job:"
echo "   gcloud run jobs execute process-newsletters --region us-central1"
echo ""
echo "📊 To monitor the job:"
echo "   gcloud logging tail \"resource.type=cloud_run_job AND resource.labels.job_name=process-newsletters\""
echo ""
echo "🔍 To check status:"
echo "   ./scripts/monitor-job.sh"
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="Dockerfile.discovery">
# Dockerfile for Newsletter Discovery Cloud Run Job
# Use Node.js 20 slim image as base
FROM node:20-slim

# Install Puppeteer dependencies (needed for web scraping)
RUN apt-get update && apt-get install -y \
    chromium \
    chromium-sandbox \
    fonts-liberation \
    libappindicator3-1 \
    libasound2 \
    libatk-bridge2.0-0 \
    libatk1.0-0 \
    libcups2 \
    libdbus-1-3 \
    libdrm2 \
    libgbm1 \
    libgtk-3-0 \
    libnspr4 \
    libnss3 \
    libx11-xcb1 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxrandr2 \
    xdg-utils \
    && rm -rf /var/lib/apt/lists/*

# Set Puppeteer to use system Chromium
ENV PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
ENV PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium

# Set working directory
WORKDIR /app

# Copy package files for dependency installation
COPY package*.json ./
COPY tsconfig.json ./

# Install dependencies (including dev dependencies for tsx)
RUN npm ci

# Copy source code
COPY scripts/ ./scripts/
COPY config/ ./config/
COPY . .

# Set entrypoint to run the discovery orchestrator with tsx
ENTRYPOINT ["npx", "tsx", "scripts/discovery/discover-orchestrator.ts"]

# Default command (empty - entrypoint handles it)
CMD []
</file>

<file path="Dockerfile.eval">
# Use Node.js 20 slim image as base
FROM node:20-slim

# Set working directory
WORKDIR /app

# Copy package files for dependency installation
COPY package*.json ./
COPY tsconfig.json ./

# Install dependencies (including dev dependencies for tsx)
RUN npm ci

# Copy source code
COPY scripts/ ./scripts/
COPY newsletter-search/src/lib/ ./newsletter-search/src/lib/
COPY config/ ./config/
COPY . .

# Set entrypoint to run the evaluation script with tsx
ENTRYPOINT ["npx", "tsx", "scripts/evaluate-rag.ts"]

# Default command (empty - entrypoint handles it)
CMD []
</file>

<file path="FIX_AND_RESTART.sh">
#!/bin/bash

# Stop the currently running job and restart with fixed code
# Run this in Google Cloud Shell

echo "═══════════════════════════════════════════════════════════════"
echo "🛑 STOPPING CURRENT JOB"
echo "═══════════════════════════════════════════════════════════════"
echo ""

# List running executions
echo "Checking for running executions..."
EXECUTIONS=$(gcloud run jobs executions list \
  --job process-newsletters \
  --region us-central1 \
  --filter="status=Succeeded OR status=Running OR status=Pending" \
  --limit=1 \
  --format="value(name)" 2>/dev/null || echo "")

if [ -n "$EXECUTIONS" ]; then
  echo "Found running/pending executions (this is OK, Cloud Run will stop them)"
fi

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Pull latest code
echo "Step 1: Pulling latest code..."
cd ~/newsletter-control-center/newsletter-control-center
git pull

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Rebuild
echo "Step 2: Rebuilding Docker image (takes ~5 minutes)..."
gcloud builds submit --tag gcr.io/newsletter-control-center/process-newsletters

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Update job
echo "Step 3: Updating Cloud Run job..."
gcloud run jobs update process-newsletters \
  --image gcr.io/newsletter-control-center/process-newsletters:latest \
  --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Wait a moment for updates to propagate
echo "Waiting 10 seconds for updates to propagate..."
sleep 10

# Start new execution
echo "Step 4: Starting new job execution..."
gcloud run jobs execute process-newsletters --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "✅ Job started with fixed code!"
echo ""
echo "Monitor with:"
echo "  gcloud logging read 'resource.type=cloud_run_job AND logName:\"projects/newsletter-control-center/logs/run.googleapis.com%2Fstdout\"' --limit 50 --format=\"value(textPayload)\""
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="live-monitor.sh">
#!/bin/bash

JOB_NAME="discover-newsletters"
PROJECT="newsletter-control-center"
REGION="us-central1"
INTERVAL=10

echo "═══════════════════════════════════════════════════════════════"
echo "📊 LIVE DISCOVERY MONITORING"
echo "═══════════════════════════════════════════════════════════════"
echo "Refreshing every ${INTERVAL} seconds. Press Ctrl+C to stop."
echo ""

# Get latest execution
LATEST_EXEC=$(gcloud run jobs executions list \
  --job "${JOB_NAME}" \
  --region "${REGION}" \
  --project "${PROJECT}" \
  --limit 1 \
  --format="value(name)" 2>/dev/null)

EXEC_ID=$(echo "$LATEST_EXEC" | awk -F'/' '{print $NF}')
echo "Monitoring execution: ${EXEC_ID}"
echo ""

ITERATION=0
while true; do
  ITERATION=$((ITERATION + 1))
  clear
  
  echo "═══════════════════════════════════════════════════════════════"
  echo "📊 Update #${ITERATION} - $(date '+%H:%M:%S')"
  echo "═══════════════════════════════════════════════════════════════"
  echo ""
  
  # Check execution status
  STATUS_OUTPUT=$(gcloud run jobs executions describe "${LATEST_EXEC}" \
    --region "${REGION}" \
    --project "${PROJECT}" \
    --format="value(status.conditions[0].type,status.conditions[0].status,status.startTime,status.completionTime)" 2>/dev/null)
  
  if [ ! -z "$STATUS_OUTPUT" ]; then
    echo "Execution Status: ${STATUS_OUTPUT}"
  fi
  echo ""
  
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "📋 RECENT LOGS:"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  gcloud logging read \
    "resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME} AND resource.labels.location=${REGION}" \
    --limit 40 \
    --format="value(textPayload)" \
    --project="${PROJECT}" \
    --freshness=5m 2>/dev/null | tail -35
  
  echo ""
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "Next update in ${INTERVAL}s..."
  
  sleep ${INTERVAL}
done
</file>

<file path="monitor-discovery-cloud.sh">
#!/bin/bash

# Monitor Discovery Job Progress
# Run this to check if discovery is working or stuck

JOB_NAME="discover-newsletters"
PROJECT="newsletter-control-center"
REGION="us-central1"

echo "═══════════════════════════════════════════════════════════════"
echo "📊 DISCOVERY JOB STATUS"
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Check latest execution
LATEST_EXEC=$(gcloud run jobs executions list \
  --job="$JOB_NAME" \
  --region="$REGION" \
  --project="$PROJECT" \
  --limit=1 \
  --format="value(name)" 2>/dev/null)

if [ -z "$LATEST_EXEC" ]; then
  echo "❌ No executions found. Job may not have started yet."
  exit 1
fi

echo "Latest Execution: $(basename $LATEST_EXEC)"
echo ""

# Get execution status
STATUS=$(gcloud run jobs executions describe "$LATEST_EXEC" \
  --region="$REGION" \
  --project="$PROJECT" \
  --format="value(status.conditions[0].type,status.conditions[0].status)" 2>/dev/null)

if echo "$STATUS" | grep -q "Ready.*True"; then
  echo "✅ Status: RUNNING"
elif echo "$STATUS" | grep -q "Complete.*True"; then
  echo "✅ Status: COMPLETED"
elif echo "$STATUS" | grep -q "Failed"; then
  echo "❌ Status: FAILED"
else
  echo "⏳ Status: $STATUS"
fi

echo ""

# Get recent logs (last 10 lines)
echo "═══════════════════════════════════════════════════════════════"
echo "📝 RECENT LOGS (Last 10 lines)"
echo "═══════════════════════════════════════════════════════════════"
echo ""

gcloud logging read \
  "resource.type=cloud_run_job AND resource.labels.job_name=$JOB_NAME" \
  --limit 10 \
  --format="value(textPayload)" \
  --project="$PROJECT" 2>/dev/null | tail -10

if [ $? -ne 0 ]; then
  echo "⚠️  No logs yet (job may still be starting)"
fi

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "💡 To watch logs continuously:"
echo "   ./WATCH_LOGS.sh"
echo "   (or run CHECK_PROGRESS.sh repeatedly)"
echo ""
echo "💡 To check progress in BigQuery:"
echo "   npm run discovery:progress"
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="monitor-discovery-live.sh">
#!/bin/bash

JOB_NAME="discover-newsletters"
PROJECT="newsletter-control-center"
REGION="us-central1"
INTERVAL=10 # seconds between refreshes

echo "═══════════════════════════════════════════════════════════════"
echo "📊 LIVE DISCOVERY MONITORING"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "Job: ${JOB_NAME}"
echo "Refresh interval: ${INTERVAL}s"
echo "Press Ctrl+C to stop"
echo ""
echo "Looking for latest execution..."
sleep 2

# Get the latest execution
LATEST_EXECUTION=$(gcloud run jobs executions list \
  --job "${JOB_NAME}" \
  --region "${REGION}" \
  --project "${PROJECT}" \
  --limit 1 \
  --format="value(name)" 2>/dev/null)

if [ -z "$LATEST_EXECUTION" ]; then
  echo "❌ No executions found. Job may still be starting..."
  echo "Will check again in 10 seconds..."
  sleep 10
  LATEST_EXECUTION=$(gcloud run jobs executions list \
    --job "${JOB_NAME}" \
    --region "${REGION}" \
    --project "${PROJECT}" \
    --limit 1 \
    --format="value(name)" 2>/dev/null)
fi

EXECUTION_ID=$(echo "$LATEST_EXECUTION" | awk -F'/' '{print $NF}')
echo "✅ Monitoring execution: ${EXECUTION_ID}"
echo ""

# Function to get job status
get_status() {
  gcloud run jobs executions describe "${LATEST_EXECUTION}" \
    --region "${REGION}" \
    --project "${PROJECT}" \
    --format="value(status.conditions[0].type,status.conditions[0].status)" 2>/dev/null
}

# Main monitoring loop
ITERATION=0
while true; do
  ITERATION=$((ITERATION + 1))
  clear
  
  echo "═══════════════════════════════════════════════════════════════"
  echo "📊 DISCOVERY PROGRESS - Update #${ITERATION}"
  echo "═══════════════════════════════════════════════════════════════"
  echo "Time: $(date '+%H:%M:%S')"
  echo ""
  
  # Check execution status
  STATUS=$(get_status)
  if [ ! -z "$STATUS" ]; then
    echo "Status: ${STATUS}"
  else
    echo "Status: Running..."
  fi
  echo ""
  
  # Get recent logs (last 30 lines)
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "📋 RECENT LOGS (latest first):"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  gcloud logging read \
    "resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME} AND resource.labels.location=${REGION}" \
    --limit 30 \
    --format="value(textPayload)" \
    --project="${PROJECT}" 2>/dev/null | tail -30
  
  echo ""
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "Next update in ${INTERVAL}s... (Ctrl+C to stop)"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  sleep ${INTERVAL}
done
</file>

<file path="monitor-discovery.sh">
#!/bin/bash
# Monitor discovery progress

echo "🔍 Monitoring Discovery Progress..."
echo "Press Ctrl+C to stop"
echo ""

while true; do
  clear
  echo "╔═══════════════════════════════════════════════════════╗"
  echo "║      Newsletter Discovery Progress Monitor            ║"
  echo "╚═══════════════════════════════════════════════════════╝"
  echo ""
  npm run discovery:progress 2>/dev/null | tail -20
  echo ""
  echo "Process running: $(ps aux | grep -i 'discover-orchestrator' | grep -v grep | wc -l | xargs)"
  echo ""
  echo "Last updated: $(date '+%H:%M:%S')"
  echo "Checking again in 30 seconds..."
  sleep 30
done
</file>

<file path="monitor-live.sh">
#!/bin/bash

# Live monitoring for discovery job
EXEC_ID="discover-newsletters-xxmg5"
PROJECT="newsletter-control-center"
REGION="us-central1"
JOB_NAME="discover-newsletters"
UPDATE_INTERVAL=30

echo "═══════════════════════════════════════════════════════════════"
echo "📊 LIVE DISCOVERY PROGRESS MONITOR"
echo "═══════════════════════════════════════════════════════════════"
echo "Execution: ${EXEC_ID}"
echo "Update Interval: ${UPDATE_INTERVAL} seconds"
echo "Press Ctrl+C to stop"
echo ""

iteration=0

while true; do
  iteration=$((iteration + 1))
  clear
  echo "═══════════════════════════════════════════════════════════════"
  echo "📊 Update #${iteration} - $(date '+%H:%M:%S')"
  echo "═══════════════════════════════════════════════════════════════"
  
  # Get latest execution
  LATEST_EXEC=$(gcloud run jobs executions list \
    --job="${JOB_NAME}" \
    --region="${REGION}" \
    --project="${PROJECT}" \
    --limit=1 \
    --format="value(name)" 2>/dev/null)
  
  if [ ! -z "$LATEST_EXEC" ]; then
    echo "Execution: ${LATEST_EXEC}"
    
    # Get status
    STATUS=$(gcloud run jobs executions describe "${LATEST_EXEC}" \
      --region="${REGION}" \
      --project="${PROJECT}" \
      --format="value(status.conditions[0].type,status.conditions[0].status)" 2>/dev/null)
    echo "Status: ${STATUS}"
  else
    echo "Status: Finding execution..."
  fi
  
  echo ""
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "📋 RECENT LOGS (Last 20 lines):"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  gcloud logging read \
    "resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME}" \
    --limit 50 \
    --format="value(textPayload)" \
    --project="${PROJECT}" 2>/dev/null | \
    grep -v "^\[dotenv" | \
    grep -v "^$" | \
    tail -20
  
  echo ""
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "🔍 STEP STATUS CHECK:"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  # Check for steps
  LOGS=$(gcloud logging read \
    "resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME}" \
    --limit 500 \
    --format="value(textPayload)" \
    --project="${PROJECT}" 2>/dev/null)
  
  STEP1=$(echo "$LOGS" | grep -c "Step 1\|Substack.*Search" || echo "0")
  STEP2=$(echo "$LOGS" | grep -c "Step 2\|Recommendation" || echo "0")
  STEP3=$(echo "$LOGS" | grep -c "Step 3\|Directory" || echo "0")
  STEP4=$(echo "$LOGS" | grep -c "Step 4\|Beehiiv\|beehiiv" || echo "0")
  STEP5=$(echo "$LOGS" | grep -c "Step 5\|Web Search\|web search" || echo "0")
  COMPLETE=$(echo "$LOGS" | grep -c "DISCOVERY.*COMPLETE\|FINAL STATISTICS" || echo "0")
  
  echo "Step 1 (Substack Search):      $(if [ "$STEP1" -gt 0 ]; then echo "✅ Executed"; else echo "⏳ Pending"; fi)"
  echo "Step 2 (Recommendations):     $(if [ "$STEP2" -gt 0 ]; then echo "✅ Executed"; else echo "⏳ Pending"; fi)"
  echo "Step 3 (Directories):         $(if [ "$STEP3" -gt 0 ]; then echo "✅ Executed"; else echo "⏳ Pending"; fi)"
  echo "Step 4 (Beehiiv):             $(if [ "$STEP4" -gt 0 ]; then echo "✅ EXECUTING/EXECUTED"; else echo "⏳ Waiting..."; fi)"
  echo "Step 5 (Web Search):          $(if [ "$STEP5" -gt 0 ]; then echo "✅ EXECUTING/EXECUTED"; else echo "⏳ Waiting..."; fi)"
  echo "Final Summary:                $(if [ "$COMPLETE" -gt 0 ]; then echo "✅ COMPLETE"; else echo "⏳ In Progress"; fi)"
  
  echo ""
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  echo "🔑 API KEY STATUS:"
  echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
  
  API_WARNINGS=$(echo "$LOGS" | grep -c "API.*not configured\|not configured" || echo "0")
  if [ "$API_WARNINGS" -gt 0 ]; then
    echo "⚠️  API Key Warnings: ${API_WARNINGS} (Secrets may not be accessible)"
  else
    echo "✅ No API key warnings found"
  fi
  
  echo ""
  echo "⏱️  Next update in ${UPDATE_INTERVAL} seconds... (Ctrl+C to stop)"
  sleep ${UPDATE_INTERVAL}
done
</file>

<file path="REDEPLOY_AND_RUN.sh">
#!/bin/bash

# Redeploy and run the fixed job
# Run this in Google Cloud Shell

echo "═══════════════════════════════════════════════════════════════"
echo "🔧 REBUILDING & REDEPLOYING FIXED VERSION"
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 1: Pull latest code
echo "Step 1: Pulling latest code..."
cd ~/newsletter-control-center/newsletter-control-center
git pull

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 2: Rebuild Docker image
echo "Step 2: Rebuilding Docker image (this takes ~5 minutes)..."
gcloud builds submit --tag gcr.io/newsletter-control-center/process-newsletters

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 3: Update Cloud Run job
echo "Step 3: Updating Cloud Run job..."
gcloud run jobs update process-newsletters \
  --image gcr.io/newsletter-control-center/process-newsletters:latest \
  --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 4: Start the job
echo "Step 4: Starting the job..."
gcloud run jobs execute process-newsletters --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "✅ Job started!"
echo ""
echo "Monitor with:"
echo "  gcloud logging read 'resource.type=cloud_run_job AND logName:\"projects/newsletter-control-center/logs/run.googleapis.com%2Fstdout\"' --limit 50 --format=\"value(textPayload)\""
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="refresh-adc.sh">
#!/bin/bash
# Helper script to refresh Application Default Credentials
# Use this when you see "invalid_grant" errors

echo "Checking Application Default Credentials..."

if gcloud auth application-default print-access-token >/dev/null 2>&1; then
  echo "✅ ADC is valid"
  EXPIRY=$(gcloud auth application-default print-access-token 2>&1 | head -1)
  echo "   Token is valid"
else
  echo "⚠️  ADC expired or missing"
  echo "   Refreshing credentials..."
  gcloud auth application-default login
  echo ""
  echo "✅ ADC refreshed! Good for ~24 hours."
fi
</file>

<file path="RUN_EVAL_IN_CLOUD_SHELL.sh">
#!/bin/bash
# Run this in Cloud Shell to deploy and execute the evaluation harness

set -e

echo "🚀 Deploying and running RAG evaluation harness..."
echo ""

# Pull latest code
cd ~/newsletter-control-center
git pull origin main

# Build image
echo "📦 Building Docker image..."
gcloud builds submit --tag gcr.io/newsletter-control-center/eval-rag --file Dockerfile.eval

# Update job
echo "☁️  Updating Cloud Run job..."
gcloud run jobs update eval-rag \
    --image=gcr.io/newsletter-control-center/eval-rag \
    --region=us-central1 \
    --memory=2Gi \
    --cpu=2 \
    --max-retries=0 \
    --task-timeout=1200 \
    --set-env-vars="BIGQUERY_PROJECT_ID=newsletter-control-center" 2>/dev/null || \
gcloud run jobs create eval-rag \
    --image=gcr.io/newsletter-control-center/eval-rag \
    --region=us-central1 \
    --memory=2Gi \
    --cpu=2 \
    --max-retries=0 \
    --task-timeout=1200 \
    --set-env-vars="BIGQUERY_PROJECT_ID=newsletter-control-center"

# Execute
echo "🏃 Executing evaluation..."
gcloud run jobs execute eval-rag --region=us-central1

echo ""
echo "✅ Done! Check logs for results:"
echo "  gcloud logging read \"resource.type=cloud_run_job AND resource.labels.job_name=eval-rag\" --limit 100"
</file>

<file path="START_25K_BATCH.sh">
#!/bin/bash

# Start 25K Newsletter Batch Processing Job
# Run this in Google Cloud Shell

echo "═══════════════════════════════════════════════════════════════"
echo "🚀 Starting 25K Newsletter Batch"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "Configuration:"
echo "  - Starting from: Newsletter #5,000"
echo "  - Processing: 25,000 newsletters"
echo "  - Ending at: Newsletter #30,000"
echo "  - Expected duration: ~8 hours"
echo "  - Final corpus: ~30,000 newsletters processed (40% of 73K)"
echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 1: Update job configuration
echo "Step 1: Updating Cloud Run job configuration..."
gcloud run jobs update process-newsletters \
  --update-env-vars PROCESS_LIMIT=25000,START_FROM=5000 \
  --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 2: Execute the job
echo "Step 2: Starting job execution..."
gcloud run jobs execute process-newsletters --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "✅ Job started successfully!"
echo ""
echo "To monitor progress:"
echo "  gcloud logging read \\"
echo "    \"resource.type=cloud_run_job AND logName:\\\"projects/newsletter-control-center/logs/run.googleapis.com%2Fstdout\\\"\" \\"
echo "    --limit 50 \\"
echo "    --format=\"value(textPayload)\""
echo ""
echo "To check status:"
echo "  gcloud run jobs executions list \\"
echo "    --job process-newsletters \\"
echo "    --region us-central1"
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="START_REMAINING_BATCH.sh">
#!/bin/bash

# Start Remaining Newsletter Batch Processing (51K newsletters)
# This will process the remaining newsletters to complete the corpus

echo "═══════════════════════════════════════════════════════════════"
echo "🚀 Starting Remaining Newsletter Batch"
echo "═══════════════════════════════════════════════════════════════"
echo ""
echo "Configuration:"
echo "  - Starting from: 0"
echo "  - Processing: 60,000 newsletters (will skip already processed)"
echo "  - Already processed: ~22K newsletters"
echo "  - Expected new: ~51K newsletters"
echo "  - Expected duration: ~8-9 hours"
echo "  - Final corpus: ~73K newsletters"
echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 1: Update job configuration
echo "Step 1: Updating Cloud Run job configuration..."
gcloud run jobs update process-newsletters \
  --update-env-vars PROCESS_LIMIT=60000,START_FROM=0 \
  --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Step 2: Execute the job
echo "Step 2: Starting job execution..."
gcloud run jobs execute process-newsletters --region us-central1

echo ""
echo "═══════════════════════════════════════════════════════════════"
echo "✅ Job started successfully!"
echo ""
echo "Expected completion: ~6-7 hours from now"
echo ""
echo "To monitor progress:"
echo "  gcloud logging read \\"
echo "    \"resource.type=cloud_run_job AND resource.labels.job_name=process-newsletters AND logName:\\\"projects/newsletter-control-center/logs/run.googleapis.com%2Fstdout\\\"\" \\"
echo "    --limit 50 \\"
echo "    --format=\"value(textPayload)\""
echo ""
echo "To check status:"
echo "  gcloud run jobs executions list \\"
echo "    --job process-newsletters \\"
echo "    --region us-central1"
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="WATCH_LOGS.sh">
#!/bin/bash

# Watch discovery logs - refreshes every 5 seconds

JOB_NAME="discover-newsletters"
PROJECT="newsletter-control-center"

echo "📺 Watching discovery logs (press Ctrl+C to stop)..."
echo "Refreshing every 5 seconds..."
echo ""

while true; do
  clear
  echo "═══════════════════════════════════════════════════════════════"
  echo "📊 Discovery Job Logs (Last 20 lines) - $(date '+%H:%M:%S')"
  echo "═══════════════════════════════════════════════════════════════"
  echo ""
  
  gcloud logging read \
    "resource.type=cloud_run_job AND resource.labels.job_name=$JOB_NAME" \
    --limit 20 \
    --format="value(textPayload)" \
    --project="$PROJECT" 2>/dev/null | tail -20
  
  echo ""
  echo "Refreshing in 5 seconds... (Ctrl+C to stop)"
  sleep 5
done
</file>

<file path="newsletter-search/src/app/api/search/route.ts">
import { NextRequest, NextResponse } from 'next/server';

import { BigQuery } from '@google-cloud/bigquery';

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

const bigquery = new BigQuery({ projectId: PROJECT_ID });

export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const query = searchParams.get('q') || '';
    const startDate = searchParams.get('startDate') || '';
    const endDate = searchParams.get('endDate') || '';
    const publisher = searchParams.get('publisher') || '';
    const vipOnly = searchParams.get('vipOnly') === 'true';
    const page = parseInt(searchParams.get('page') || '1');
    const limit = 20;
    const offset = (page - 1) * limit;

    // Build WHERE conditions
    const whereConditions = [];
    
    if (query) {
      whereConditions.push(`(
        LOWER(body_text) LIKE LOWER('%${query.replace(/'/g, "''")}%') OR
        LOWER(subject) LIKE LOWER('%${query.replace(/'/g, "''")}%') OR
        LOWER(sender) LIKE LOWER('%${query.replace(/'/g, "''")}%')
      )`);
    }
    
    if (startDate) {
      whereConditions.push(`sent_date >= '${startDate}'`);
    }
    
    if (endDate) {
      whereConditions.push(`sent_date <= '${endDate}'`);
    }
    
    if (publisher) {
      whereConditions.push(`LOWER(sender) LIKE LOWER('%${publisher.replace(/'/g, "''")}%')`);
    }
    
    if (vipOnly) {
      whereConditions.push(`is_vip = true`);
    }

    const whereClause = whereConditions.length > 0 ? `WHERE ${whereConditions.join(' AND ')}` : '';

    // Build snippet logic for search context
    let snippetLogic = 'SUBSTR(body_text, 1, 200) as snippet';
    if (query) {
      const escapedQuery = query.replace(/'/g, "''");
      snippetLogic = `
        CASE 
          WHEN LOWER(body_text) LIKE LOWER('%${escapedQuery}%') THEN
            CONCAT(
              '...',
              SUBSTR(
                body_text, 
                GREATEST(1, REGEXP_INSTR(LOWER(body_text), LOWER('${escapedQuery}')) - 100),
                200
              ),
              '...'
            )
          ELSE SUBSTR(body_text, 1, 200)
        END as snippet
      `;
    }

    // Build the query
    const sqlQuery = `
      SELECT 
        id,
        sender,
        subject,
        sent_date,
        received_date,
        body_text,
        body_html,
        is_vip,
        publisher_name,
        source_type,
        word_count,
        has_attachments,
        ${snippetLogic}
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      ${whereClause}
      ORDER BY sent_date DESC
      LIMIT ${limit}
      OFFSET ${offset}
    `;

    console.log('Executing query:', sqlQuery);
    const [rows] = await bigquery.query(sqlQuery);

    // Get total count for pagination
    const countQuery = `
      SELECT COUNT(*) as total
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      ${whereClause}
    `;
    
    const [countRows] = await bigquery.query(countQuery);
    const total = countRows[0]?.total || 0;

    return NextResponse.json({
      results: rows,
      pagination: {
        page,
        limit,
        total,
        totalPages: Math.ceil(total / limit)
      }
    });

  } catch (error) {
    console.error('Search error:', error);
    return NextResponse.json(
      { error: 'Search failed', details: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}
</file>

<file path="newsletter-search/src/app/page.tsx">
'use client';

import { useState } from 'react';
import Link from 'next/link';

interface SemanticResult {
  query: string;
  answer: string;
  citations: Array<{
    chunk_id: string;
    newsletter_id: string;
    chunk_index?: number;
    citation: string;
    publisher: string;
    date: any;
    subject: string;
  }>;
  chunks_used: number;
  cost_usd: number;
  chunks: Array<{
    chunk_id: string;
    newsletter_id: string;
    subject: string;
    publisher: string;
    score: number;
  }>;
  publisher_rankings?: Array<{
    publisher: string;
    relevance_score: number;
    chunk_count: number;
    avg_score: number;
    latest_date?: any;
  }>;
}

export default function Home() {
  const [query, setQuery] = useState('');
  const [results, setResults] = useState<SemanticResult | null>(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');

  const searchSemantic = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!query.trim()) return;

    setLoading(true);
    setError('');
    setResults(null);

    try {
      const response = await fetch('/api/intelligence/query', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ query }),
      });

      const data = await response.json();

      if (!response.ok) {
        throw new Error(data.error || data.message || 'Search failed');
      }

      setResults(data);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Search failed');
      setResults(null);
    } finally {
      setLoading(false);
    }
  };

  const formatDate = (dateInput: any) => {
    if (!dateInput) return 'Date unknown';
    
    if (dateInput && typeof dateInput === 'object' && dateInput.value) {
      try {
        return new Date(dateInput.value).toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'short',
          day: 'numeric'
        });
      } catch {
        return 'Date unknown';
      }
    }
    
    if (typeof dateInput === 'string') {
      try {
        return new Date(dateInput).toLocaleDateString('en-US', {
          year: 'numeric',
          month: 'short',
          day: 'numeric'
        });
      } catch {
        return 'Date unknown';
      }
    }
    
    return 'Date unknown';
  };

  return (
    <div className="min-h-screen bg-gray-50">
      <div className="container mx-auto px-4 py-8 max-w-5xl">
        <div className="mb-8">
          <h1 className="text-4xl font-bold text-gray-900 mb-2">Semantic Newsletter Search</h1>
          <p className="text-gray-600">Ask questions and get intelligent answers from 69,673 newsletters</p>
        </div>

        {/* Search Form */}
        <form onSubmit={searchSemantic} className="bg-white p-6 rounded-lg shadow-md mb-6">
          <div className="flex gap-4">
            <input
              type="text"
              value={query}
              onChange={(e) => setQuery(e.target.value)}
              placeholder="Ask a question... (e.g., 'What are the latest developments in AI regulation?')"
              className="flex-1 px-4 py-3 border border-gray-300 rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500 text-lg"
              disabled={loading}
            />
            <button
              type="submit"
              disabled={loading || !query.trim()}
              className="px-8 py-3 bg-blue-600 text-white rounded-lg hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 disabled:opacity-50 disabled:cursor-not-allowed font-medium text-lg"
            >
              {loading ? 'Searching...' : 'Search'}
            </button>
          </div>
        </form>

        {/* Error Message */}
        {error && (
          <div className="bg-red-50 border border-red-200 text-red-700 px-4 py-3 rounded-lg mb-6">
            <strong>Error:</strong> {error}
          </div>
        )}

        {/* Loading State */}
        {loading && (
          <div className="bg-white rounded-lg shadow-md p-8 text-center">
            <div className="animate-pulse space-y-4">
              <div className="h-4 bg-gray-200 rounded w-3/4 mx-auto"></div>
              <div className="h-4 bg-gray-200 rounded w-1/2 mx-auto"></div>
              <div className="h-4 bg-gray-200 rounded w-5/6 mx-auto"></div>
            </div>
            <p className="mt-6 text-gray-500">Searching 938,601 chunks with semantic embeddings...</p>
          </div>
        )}

        {/* Results */}
        {results && !loading && (
          <div className="space-y-6">
            {/* AI Answer */}
            <div className="bg-white rounded-lg shadow-md p-8">
              <h2 className="text-2xl font-bold text-gray-900 mb-4">Answer</h2>
              <div className="prose max-w-none text-gray-700 whitespace-pre-wrap leading-relaxed">
                {results.answer}
              </div>
              <div className="mt-6 pt-4 border-t border-gray-200 text-sm text-gray-500">
                Based on {results.chunks_used} relevant chunks • Cost: ${results.cost_usd.toFixed(4)}
              </div>
            </div>

            {/* Citations */}
            {results.citations && results.citations.length > 0 && (
              <div className="bg-white rounded-lg shadow-md p-6">
                <h3 className="text-xl font-semibold text-gray-900 mb-4">Sources ({results.citations.length})</h3>
                <div className="space-y-3">
                  {results.citations.map((citation, idx) => (
                    <Link
                      key={idx}
                      href={`/newsletter/${citation.newsletter_id}${citation.chunk_index !== undefined ? `?highlight_chunk=${citation.chunk_index}` : ''}`}
                      className="block border-l-4 border-blue-500 pl-4 py-2 hover:bg-blue-50 transition-colors rounded-r hover:shadow-sm"
                    >
                      <div className="font-medium text-gray-900 hover:text-blue-700">{citation.citation}</div>
                      <div className="text-sm text-gray-500 mt-1">
                        {formatDate(citation.date)}
                      </div>
                      <div className="text-xs text-blue-600 mt-1 opacity-75">
                        Click to read full newsletter →
                      </div>
                    </Link>
                  ))}
                </div>
              </div>
            )}

            {/* Publisher Rankings */}
            {results.publisher_rankings && results.publisher_rankings.length > 0 && (
              <div className="bg-white rounded-lg shadow-md p-6">
                <h3 className="text-xl font-semibold text-gray-900 mb-4">Top Publishers</h3>
                <div className="space-y-3">
                  {results.publisher_rankings.slice(0, 5).map((pub, idx) => (
                    <div key={idx} className="flex items-center justify-between border-b border-gray-100 pb-3 last:border-0 last:pb-0">
                      <div className="flex-1">
                        <div className="font-medium text-gray-900">{pub.publisher}</div>
                        <div className="text-sm text-gray-500">
                          {pub.chunk_count} relevant {pub.chunk_count === 1 ? 'chunk' : 'chunks'}
                        </div>
                      </div>
                      <div className="text-right">
                        <div className="text-lg font-semibold text-blue-600">
                          {(pub.relevance_score * 100).toFixed(0)}%
                        </div>
                        <div className="text-xs text-gray-400">relevance</div>
                      </div>
                    </div>
                  ))}
                </div>
              </div>
            )}

            {/* Top Chunks */}
            {results.chunks && results.chunks.length > 0 && (
              <div className="bg-white rounded-lg shadow-md p-6">
                <h3 className="text-xl font-semibold text-gray-900 mb-4">
                  Relevant Newsletters ({results.chunks.length})
                </h3>
                <div className="space-y-3">
                  {results.chunks.slice(0, 10).map((chunk, idx) => (
                    <div key={chunk.chunk_id} className="border border-gray-200 rounded-lg p-4 hover:bg-gray-50 transition-colors">
                      <div className="flex justify-between items-start">
                        <div className="flex-1">
                          <div className="font-medium text-gray-900 mb-1">{chunk.subject}</div>
                          <div className="text-sm text-gray-500">{chunk.publisher}</div>
                        </div>
                        <div className="text-right ml-4">
                          <Link
                            href={`/newsletter/${chunk.newsletter_id}`}
                            className="text-sm font-medium text-blue-600 hover:text-blue-800 hover:underline block"
                          >
                            {(chunk.score).toFixed(0)}% match →
                          </Link>
                        </div>
                      </div>
                    </div>
                  ))}
                </div>
              </div>
            )}
          </div>
        )}

        {/* No Results State */}
        {!loading && !results && !error && query && (
          <div className="bg-white p-8 rounded-lg shadow-md text-center">
            <p className="text-gray-500 text-lg">No results found. Try a different query.</p>
          </div>
        )}

        {/* Empty State */}
        {!loading && !results && !error && !query && (
          <div className="bg-white p-8 rounded-lg shadow-md text-center">
            <p className="text-gray-500 text-lg">Enter a question above to search through 938,601 chunks from 69,673 newsletters.</p>
            <div className="mt-4 text-sm text-gray-400">
              Example: "What are the latest developments in AI regulation?"
            </div>
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="src/index.js">
const express = require('express');

const app = express();
app.use(express.json());

const PORT = process.env.PORT || 8080;
const SLACK_WEBHOOK_URL = process.env.SLACK_WEBHOOK_URL || "";

// --- light notifier (logs by default; posts to Slack if webhook is set)
async function notify(message) {
  console.log(`[notify] ${message}`);
  if (!SLACK_WEBHOOK_URL) return { ok: true, via: 'log' };
  try {
    const res = await fetch(SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: message })
    });
    const ok = res.ok;
    if (!ok) console.error(`[notify] Slack error ${res.status}`);
    return { ok, via: 'slack', status: res.status };
  } catch (err) {
    console.error('[notify] Slack exception', err);
    return { ok: false, via: 'slack', error: String(err) };
  }
}

app.get('/status', (_req, res) => res.type('text').send('ok'));

app.get('/ping', async (_req, res) => {
  const ts = new Date().toISOString();
  const note = await notify(`✅ NCC pipeline alive @ ${ts}`);
  res.json({ ok: true, ping: ts, notify: note });
});

app.get('/run', async (_req, res) => {
  const ts = new Date().toISOString();
  await notify(`🚀 NCC daily run triggered @ ${ts}`);
  // TODO: pull newsletters, summarize, deliver digest
  res.json({ ok: true, message: 'Daily run placeholder', ts });
});

app.use((req, res) => res.status(404).json({ ok: false, error: 'not_found', path: req.path }));

app.listen(PORT, () => {
  console.log(`Server listening on ${PORT}`);
});
</file>

<file path="CHECK_PROGRESS.sh">
#!/bin/bash

# Simple progress check - just shows what's happening

echo "🔍 Checking discovery job progress..."
echo ""

# Show execution status
echo "📊 Job Status:"
gcloud run jobs executions list \
  --job discover-newsletters \
  --region us-central1 \
  --project newsletter-control-center \
  --limit 1 \
  --format="table(EXECUTION,RUNNING,COMPLETE,CREATED)"

echo ""
echo "📝 Latest Activity (last 15 log lines):"
echo "─────────────────────────────────────────"

gcloud logging read \
  "resource.type=cloud_run_job AND resource.labels.job_name=discover-newsletters" \
  --limit 30 \
  --format="value(textPayload)" \
  --project newsletter-control-center 2>/dev/null | \
  grep -E "(Searching|Found|Classifying|Complete|Progress|Stored|Step)" | \
  tail -15

echo ""
echo "💡 View full logs in Console:"
echo "   https://console.cloud.google.com/run/jobs/executions/details/us-central1/discover-newsletters-n6lv4?project=newsletter-control-center"
</file>

<file path=".gitignore">
.env
.env.local
.env.*.local
token.json
token.*.json
service-account*.json
*credentials*.json
processing-progress.json
*.log
overnight-run*.log
CLOUD_SHELL_SECRET_COMMANDS.sh

# Dependencies
node_modules/

# Build outputs
dist/
build/

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db
.gcloud/
output/
_archive/

# secrets
secrets/
*.json
*.p12
*.pem
.tokens/
</file>

<file path="DEPLOY_DISCOVERY.sh">
#!/bin/bash

# Deploy Newsletter Discovery to Cloud Run Job
# Run this in Google Cloud Shell after pulling latest code

set -e  # Exit on error

echo "═══════════════════════════════════════════════════════════════"
echo "🚀 DEPLOYING NEWSLETTER DISCOVERY TO CLOUD RUN"
echo "═══════════════════════════════════════════════════════════════"
echo ""

# Project configuration
PROJECT_ID="newsletter-control-center"
REGION="us-central1"
JOB_NAME="discover-newsletters"
IMAGE_NAME="gcr.io/${PROJECT_ID}/${JOB_NAME}"

# Step 1: Build Docker image
echo "Step 1: Building Docker image (this takes ~5-7 minutes)..."
echo "   Image: ${IMAGE_NAME}"
echo ""
# Temporarily rename Dockerfile.discovery to Dockerfile for build
if [ -f "Dockerfile" ]; then
  mv Dockerfile Dockerfile.backup
  RESTORE_DOCKERFILE=true
else
  RESTORE_DOCKERFILE=false
fi

cp Dockerfile.discovery Dockerfile

gcloud builds submit \
  --tag "${IMAGE_NAME}" \
  --project "${PROJECT_ID}"

# Restore original Dockerfile
rm Dockerfile
if [ "$RESTORE_DOCKERFILE" = "true" ]; then
  mv Dockerfile.backup Dockerfile
fi

echo ""
echo "✅ Docker image built successfully"
echo ""

# Step 2: Create or update Cloud Run job
echo "═══════════════════════════════════════════════════════════════"
echo "Step 2: Creating/updating Cloud Run job..."
echo ""

# Check if job exists
if gcloud run jobs describe "${JOB_NAME}" --region="${REGION}" --project="${PROJECT_ID}" &>/dev/null; then
  echo "   Job exists - updating..."
  gcloud run jobs update "${JOB_NAME}" \
    --image "${IMAGE_NAME}:latest" \
    --region "${REGION}" \
    --project "${PROJECT_ID}" \
    --memory 2Gi \
    --cpu 2 \
    --max-retries 0 \
    --task-timeout 10800 \
    --set-secrets=GOOGLE_CUSTOM_SEARCH_API_KEY=google-custom-search-api-key:latest,GOOGLE_CUSTOM_SEARCH_ENGINE_ID=google-custom-search-engine-id:latest
else
  echo "   Job doesn't exist - creating..."
  gcloud run jobs create "${JOB_NAME}" \
    --image "${IMAGE_NAME}:latest" \
    --region "${REGION}" \
    --project "${PROJECT_ID}" \
    --memory 2Gi \
    --cpu 2 \
    --max-retries 0 \
    --task-timeout 10800 \
    --set-secrets=GOOGLE_CUSTOM_SEARCH_API_KEY=google-custom-search-api-key:latest,GOOGLE_CUSTOM_SEARCH_ENGINE_ID=google-custom-search-engine-id:latest
fi

echo ""
echo "✅ Cloud Run job ready"
echo ""

# Step 3: Show execution command
echo "═══════════════════════════════════════════════════════════════"
echo "✅ DEPLOYMENT COMPLETE!"
echo ""
echo "To execute the discovery job, run:"
echo ""
echo "  gcloud run jobs execute ${JOB_NAME} --region ${REGION} --project ${PROJECT_ID}"
echo ""
echo "To monitor logs, run:"
echo ""
echo "  gcloud logging read \"resource.type=cloud_run_job AND resource.labels.job_name=${JOB_NAME}\" --limit 50 --format=\"value(textPayload)\" --project ${PROJECT_ID}"
echo ""
echo "Or view in Console:"
echo "  https://console.cloud.google.com/run/jobs?project=${PROJECT_ID}"
echo ""
echo "═══════════════════════════════════════════════════════════════"
</file>

<file path="README.md">
# newsletter-control-center
A briefing service based on my newsletters

## Environment Setup

### Authentication

This project uses **Google Cloud service account credentials** for BigQuery, Vertex AI, and other Google Cloud services.

**Local Development:**
- Service account key is stored at: `~/.gcloud/newsletter-local-dev-key.json`
- Environment variable is set in `~/.zshrc`: `GOOGLE_APPLICATION_CREDENTIALS`
- **No need to run `gcloud auth application-default login`** - the service account key provides long-lived credentials

**Cloud Run:**
- Automatically uses compute service account
- Tokens auto-refresh (never expire)

### Environment Variables

1. Copy the environment template file:
   ```bash
   cp .env.example .env
   ```

2. Edit `.env` and fill in your actual credentials:
   - **Gmail API** (for newsletter ingestion):
     - `GMAIL_CLIENT_ID`: Your Gmail API client ID
     - `GMAIL_CLIENT_SECRET`: Your Gmail API client secret  
     - `GMAIL_REFRESH_TOKEN`: Your Gmail API refresh token
   - **Google Custom Search API** (for discovery, optional):
     - `GOOGLE_CUSTOM_SEARCH_API_KEY`: Your API key
     - `GOOGLE_CUSTOM_SEARCH_ENGINE_ID`: Your search engine ID

3. **Important**: Never commit the `.env` file to version control. It contains sensitive credentials.

4. Verify your environment setup:
   ```bash
   npm run verify-env
   ```

   This will check that all required environment variables are present and exit with a clear error message if any are missing.

## Architecture

See [docs/ARCHITECTURE.md](./docs/ARCHITECTURE.md) for complete system architecture including:

- Data flow diagram
- Publisher canonicalization logic
- Database schema
- Migration path from v1 to v2

## Runbook

See [docs/RUNBOOK.md](./docs/RUNBOOK.md) for:

- Daily operational procedures
- Monitoring queries
- Troubleshooting guides
- Manual intervention procedures
</file>

<file path="newsletter-search/src/app/api/intelligence/query/route.ts">
import { NextRequest, NextResponse } from 'next/server';
import { BigQuery } from '@google-cloud/bigquery';

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const CHUNKS_TABLE = 'chunks';
const LOCATION = 'us-central1';

// Budget configuration
const DAILY_BUDGET_USD = 10.00; // Max spend per day
const INPUT_COST_PER_1M = 1.25;
const OUTPUT_COST_PER_1M = 5.00;

// Simple in-memory daily spend tracking (will reset on server restart)
// In production, this should be stored in BigQuery or Redis
let dailySpend = 0;
let dailySpendResetDate = new Date().toDateString();

/**
 * Check and update daily spend
 */
function checkDailyBudget(cost: number): boolean {
  const today = new Date().toDateString();
  
  // Reset daily spend if it's a new day
  if (today !== dailySpendResetDate) {
    dailySpend = 0;
    dailySpendResetDate = today;
  }
  
  // Check if adding this cost would exceed budget
  if (dailySpend + cost > DAILY_BUDGET_USD) {
    return false;
  }
  
  // Update daily spend
  dailySpend += cost;
  return true;
}

/**
 * Generate embedding for a query using Vertex AI
 */
async function generateEmbedding(text: string): Promise<number[]> {
  const { GoogleAuth } = require('google-auth-library');
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/text-embedding-004:predict`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      instances: [
        {
          content: text,
          task_type: 'RETRIEVAL_QUERY',
        }
      ]
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  
  if (data.predictions && data.predictions[0] && data.predictions[0].embeddings) {
    const embedding = data.predictions[0].embeddings.values || data.predictions[0].embeddings;
    if (Array.isArray(embedding)) {
      return embedding;
    }
  }
  
  throw new Error('No embedding returned from API');
}

/**
 * Vector search using cosine distance
 * Since we don't have VECTOR_SEARCH built-in yet, we'll use cosine similarity
 */
async function vectorSearch(
  bigquery: BigQuery, 
  queryEmbedding: number[], 
  topK: number = 20
): Promise<any[]> {
  // Convert embedding array to SQL array literal
  const embeddingStr = `[${queryEmbedding.join(',')}]`;
  
  // Use cosine distance: 1 - (dot product) / (||a|| * ||b||)
  const query = `
    WITH query_embedding AS (
      SELECT ${embeddingStr} AS embedding
    ),
    chunk_distances AS (
      SELECT
        c.chunk_id,
        c.newsletter_id,
        c.chunk_index,
        c.chunk_text,
        c.subject,
        c.publisher_name,
        c.sent_date,
        -- Cosine similarity calculation
        1 - (
          SELECT SUM(a * b) / (SQRT(SUM(a * a)) * SQRT(SUM(b * b)))
          FROM 
            UNNEST(query_embedding.embedding) AS a WITH OFFSET i
            JOIN 
            UNNEST(c.chunk_embedding) AS b WITH OFFSET j
          WHERE i = j
        ) AS distance
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c, query_embedding
    )
    SELECT 
      chunk_id,
      newsletter_id,
      chunk_index,
      chunk_text,
      subject,
      publisher_name,
      sent_date,
      distance
    FROM chunk_distances
    ORDER BY distance ASC
    LIMIT ${topK}
  `;

  const [rows] = await bigquery.query(query);
  return rows;
}

/**
 * Keyword search (full-text search)
 */
async function keywordSearch(
  bigquery: BigQuery,
  query: string,
  topK: number = 20
): Promise<any[]> {
  // Escape single quotes for SQL
  const escapedQuery = query.replace(/'/g, "''");
  
  // Only perform keyword search if query doesn't have apostrophes
  // or if it's a simple keyword match
  if (query.includes("'")) {
    // Skip keyword search for queries with apostrophes to avoid SQL errors
    return [];
  }
  
  const searchQuery = `
    SELECT
      chunk_id,
      newsletter_id,
      chunk_index,
      chunk_text,
      subject,
      publisher_name,
      sent_date,
      -- Simple relevance score based on keyword frequency
      (
        (LENGTH(chunk_text) - LENGTH(REPLACE(LOWER(chunk_text), LOWER('${escapedQuery}'), ''))) 
        / LENGTH('${escapedQuery}')
      ) AS relevance
    FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
    WHERE LOWER(chunk_text) LIKE LOWER('%${escapedQuery}%')
      OR LOWER(subject) LIKE LOWER('%${escapedQuery}%')
    ORDER BY relevance DESC
    LIMIT ${topK}
  `;

  try {
    const [rows] = await bigquery.query(searchQuery);
    return rows;
  } catch (error) {
    // If keyword search fails, just return empty results
    console.warn('Keyword search failed, returning empty results:', error);
    return [];
  }
}

/**
 * Hybrid search: combine vector and keyword results
 */
async function hybridSearch(
  bigquery: BigQuery,
  query: string,
  queryEmbedding: number[],
  topK: number = 20
): Promise<any[]> {
  // Get results from both searches
  const [vectorResults, keywordResults] = await Promise.all([
    vectorSearch(bigquery, queryEmbedding, topK * 2),
    keywordSearch(bigquery, query, topK * 2)
  ]);

  // Combine and deduplicate by chunk_id
  const combined = new Map<string, any>();
  
  // Add vector results (weight: 0.7)
  vectorResults.forEach((result, idx) => {
    const score = 1 - result.distance; // Convert distance to similarity
    combined.set(result.chunk_id, {
      ...result,
      vector_score: score,
      keyword_score: 0,
      combined_score: score * 0.7
    });
  });

  // Add keyword results (weight: 0.3)
  keywordResults.forEach((result) => {
    const existing = combined.get(result.chunk_id);
    if (existing) {
      existing.keyword_score = result.relevance;
      existing.combined_score = existing.vector_score * 0.7 + result.relevance * 0.3;
    } else {
      combined.set(result.chunk_id, {
        ...result,
        vector_score: 0,
        keyword_score: result.relevance,
        combined_score: result.relevance * 0.3
      });
    }
  });

  // Sort by combined score and return top K
  let sorted = Array.from(combined.values())
    .sort((a, b) => b.combined_score - a.combined_score)
    .slice(0, topK * 2); // Get more candidates for reranking

  // Apply freshness bias (boost recent newsletters)
  const now = Date.now();
  sorted = sorted.map(chunk => {
    let freshnessBonus = 0;
    if (chunk.sent_date) {
      let chunkDate: number;
      if (chunk.sent_date && typeof chunk.sent_date === 'object' && chunk.sent_date.value) {
        chunkDate = new Date(chunk.sent_date.value).getTime();
      } else if (typeof chunk.sent_date === 'string') {
        chunkDate = new Date(chunk.sent_date).getTime();
      } else {
        chunkDate = 0;
      }
      
      if (chunkDate > 0) {
        // Boost by 10% for items from last 30 days, 5% for last 90 days
        const daysAgo = (now - chunkDate) / (1000 * 60 * 60 * 24);
        if (daysAgo <= 30) {
          freshnessBonus = 0.1;
        } else if (daysAgo <= 90) {
          freshnessBonus = 0.05;
        }
      }
    }
    
    return {
      ...chunk,
      combined_score: Math.min(chunk.combined_score + freshnessBonus, 1.0) // Cap at 1.0
    };
  });

  // Rerank with freshness
  sorted.sort((a, b) => b.combined_score - a.combined_score);

  // Normalize scores relative to top result (top = 100%)
  const topScore = sorted[0]?.combined_score || 1;
  sorted = sorted.map(chunk => ({
    ...chunk,
    normalized_score: topScore > 0 ? chunk.combined_score / topScore : chunk.combined_score
  }));

  return sorted.slice(0, topK);
}

/**
 * Fetch full chunk text from BigQuery
 */
async function getFullChunks(bigquery: BigQuery, chunkIds: string[]): Promise<any[]> {
  const ids = chunkIds.map(id => `'${id}'`).join(',');
  
  const query = `
    SELECT 
      chunk_id,
      newsletter_id,
      chunk_index,
      chunk_text,
      subject,
      publisher_name,
      sent_date
    FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
    WHERE chunk_id IN (${ids})
  `;

  const [rows] = await bigquery.query(query);
  return rows;
}

/**
 * Call Gemini 2.5 Pro to extract facts from chunks
 */
async function extractFacts(chunks: any[], userQuery: string): Promise<{facts: any[], tokens_in: number, tokens_out: number}> {
  const { GoogleAuth } = require('google-auth-library');
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  // Build context from chunks with metadata for better citations
  const context = chunks.map((chunk, idx) => `
Chunk ${idx + 1}:
Publisher: ${chunk.publisher_name}
Date: ${chunk.sent_date ? new Date(chunk.sent_date.value || chunk.sent_date).toLocaleDateString() : 'Unknown'}
Subject: ${chunk.subject}
Content: ${chunk.chunk_text}
`).join('\n---\n');

  const prompt = `Extract all facts, quotes, and data points from the following chunks that are relevant to the query: "${userQuery}"

Return your response as a JSON array where each item has:
- fact: The extracted fact or data point
- chunk_id: The ID of the chunk it came from

Only extract facts that directly answer the query. If no relevant facts exist, return an empty array.

Chunks:
${context}

Return ONLY valid JSON, no additional text:`;

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-2.5-pro:generateContent`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        role: 'user',
        parts: [{ text: prompt }]
      }],
      generationConfig: {
        temperature: 0.1,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 4096,
        responseMimeType: 'application/json'
      }
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Gemini API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  const text = data.candidates[0].content.parts[0].text;
  
  // Get token usage
  const usageMetadata = data.usageMetadata || {};
  const tokensIn = usageMetadata.promptTokenCount || 0;
  const tokensOut = usageMetadata.candidatesTokenCount || 0;
  
  // Try to parse as JSON, fallback to empty array
  try {
    const facts = JSON.parse(text);
    return {
      facts: Array.isArray(facts) ? facts : [],
      tokens_in: tokensIn,
      tokens_out: tokensOut
    };
  } catch (error) {
    console.warn('Failed to parse facts as JSON:', text);
    return {
      facts: [],
      tokens_in: tokensIn,
      tokens_out: tokensOut
    };
  }
}

/**
 * Format citation as "Publisher · Date · Subject"
 */
function formatCitation(chunk: any): string {
  const publisher = chunk.publisher_name || 'Unknown Publisher';
  const date = chunk.sent_date 
    ? new Date(chunk.sent_date.value || chunk.sent_date).toLocaleDateString('en-US', { month: 'short', day: 'numeric', year: 'numeric' })
    : 'Date unknown';
  const subject = chunk.subject || 'No subject';
  
  return `${publisher} · ${date} · ${subject}`;
}

/**
 * Calculate publisher relevance rankings based on chunk results
 */
function calculatePublisherRankings(chunks: any[]): Array<{
  publisher: string;
  relevance_score: number;
  chunk_count: number;
  avg_score: number;
  latest_date: any;
}> {
  const publisherMap = new Map<string, {
    chunks: any[];
    scores: number[];
    dates: any[];
  }>();

  chunks.forEach(chunk => {
    const publisher = chunk.publisher_name || 'Unknown';
    if (!publisherMap.has(publisher)) {
      publisherMap.set(publisher, { chunks: [], scores: [], dates: [] });
    }
    const data = publisherMap.get(publisher)!;
    data.chunks.push(chunk);
    data.scores.push(chunk.combined_score || (1 - chunk.distance) || 0);
    if (chunk.sent_date) {
      data.dates.push(chunk.sent_date);
    }
  });

  const rankings = Array.from(publisherMap.entries()).map(([publisher, data]) => {
    const avgScore = data.scores.reduce((a, b) => a + b, 0) / data.scores.length;
    const maxScore = Math.max(...data.scores);
    const chunkCount = data.chunks.length;
    
    // Latest date (for freshness calculation)
    let latestDate: any = null;
    if (data.dates.length > 0) {
      const dates = data.dates.map(d => {
        if (d && typeof d === 'object' && d.value) {
          return new Date(d.value).getTime();
        } else if (typeof d === 'string') {
          return new Date(d).getTime();
        }
        return 0;
      }).filter(t => t > 0);
      if (dates.length > 0) {
        latestDate = data.dates[dates.indexOf(Math.max(...dates))];
      }
    }

    // Relevance score combines:
    // - Average similarity (40%)
    // - Maximum similarity (30%) 
    // - Number of relevant chunks (20%)
    // - Freshness bonus (10%) - applied later if we have dates
    const relevanceScore = (avgScore * 0.4) + (maxScore * 0.3) + (Math.min(chunkCount / 5, 1) * 0.2);

    return {
      publisher,
      relevance_score: Math.min(relevanceScore, 1), // Normalize to 0-1
      chunk_count: chunkCount,
      avg_score: avgScore,
      latest_date: latestDate
    };
  });

  // Sort by relevance score descending
  return rankings.sort((a, b) => b.relevance_score - a.relevance_score);
}

/**
 * Call Gemini 2.5 Pro to synthesize answer from facts
 */
async function synthesizeAnswer(facts: any[], userQuery: string, chunks: any[]): Promise<{answer: string, tokens_in: number, tokens_out: number}> {
  if (facts.length === 0) {
    return {
      answer: 'No information found in the newsletter archive that answers this query.',
      tokens_in: 0,
      tokens_out: 0
    };
  }

  const { GoogleAuth } = require('google-auth-library');
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  // Build facts list with citations (format: "Publisher · Date · Subject")
  const factsList = facts.map(f => {
    const chunk = chunks.find(c => c.chunk_id === f.chunk_id);
    const citation = chunk ? formatCitation(chunk) : `[${f.chunk_id}]`;
    return `- ${citation}: ${f.fact}`;
  }).join('\n');

  const prompt = `You are an intelligence analyst answering questions based on newsletter content.

Query: "${userQuery}"

Facts extracted from newsletters:
${factsList}

CRITICAL RULES:
1. Answer the query using ONLY the provided facts
2. Include inline citations: (Publisher · Date · Subject) after each statement
3. If information isn't in the facts, don't make it up
4. Write naturally and concisely
5. If facts are contradictory, mention both perspectives

Provide your answer:`;

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-2.5-pro:generateContent`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        role: 'user',
        parts: [{ text: prompt }]
      }],
      generationConfig: {
        temperature: 0.3,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 4096
      }
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Gemini API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  const answer = data.candidates[0].content.parts[0].text.trim();
  
  // Get token usage
  const usageMetadata = data.usageMetadata || {};
  const tokensIn = usageMetadata.promptTokenCount || 0;
  const tokensOut = usageMetadata.candidatesTokenCount || 0;
  
  return {
    answer,
    tokens_in: tokensIn,
    tokens_out: tokensOut
  };
}

export async function POST(request: NextRequest) {
  try {
    const { query } = await request.json();

    if (!query || query.trim().length === 0) {
      return NextResponse.json(
        { error: 'Query is required' },
        { status: 400 }
      );
    }

    console.log(`🔍 Processing query: "${query}"`);

    const bigquery = new BigQuery({ projectId: PROJECT_ID });
    
    // Note: We can't check budget before processing since we don't know the cost yet
    // Will check after calculating actual cost

    // Step 1: Generate query embedding
    console.log('📊 Generating query embedding...');
    const queryEmbedding = await generateEmbedding(query);

    // Step 2: Perform hybrid search (get top 10 most relevant)
    console.log('🔎 Performing hybrid search...');
    const chunks = await hybridSearch(bigquery, query, queryEmbedding, 10);
    console.log(`✅ Found ${chunks.length} relevant chunks`);

    // Fetch full chunk text for fact extraction
    console.log('📝 Fetching full chunk text...');
    const chunkIds = chunks.map(c => c.chunk_id);
    const fullChunks = await getFullChunks(bigquery, chunkIds);
    console.log(`✅ Retrieved ${fullChunks.length} full chunks`);

    // Step 3: Extract facts from chunks
    console.log('📝 Extracting facts from chunks...');
    const extractResult = await extractFacts(fullChunks, query);
    console.log(`✅ Extracted ${extractResult.facts.length} facts`);

    // Step 4: Synthesize answer from facts
    console.log('🤖 Synthesizing answer...');
    const synthResult = await synthesizeAnswer(extractResult.facts, query, fullChunks);
    console.log(`✅ Generated answer`);
    
    // Format citations for response with newsletter_id for linking
    const citations = Array.from(new Set(
      extractResult.facts.map(f => {
        const chunk = fullChunks.find(c => c.chunk_id === f.chunk_id);
        return chunk ? f.chunk_id : null;
      }).filter(Boolean)
    )).map(chunkId => {
      const chunk = fullChunks.find(c => c.chunk_id === chunkId);
      if (!chunk) return null;
      return {
        chunk_id: chunk.chunk_id,
        newsletter_id: chunk.newsletter_id, // Add for linking
        chunk_index: chunk.chunk_index, // Add for highlighting specific chunk
        citation: formatCitation(chunk),
        publisher: chunk.publisher_name,
        date: chunk.sent_date,
        subject: chunk.subject
      };
    }).filter(Boolean).slice(0, 5); // Max 5 citations

    // Calculate total costs
    const totalTokensIn = extractResult.tokens_in + synthResult.tokens_in;
    const totalTokensOut = extractResult.tokens_out + synthResult.tokens_out;
    const totalCost = (totalTokensIn / 1_000_000) * INPUT_COST_PER_1M + (totalTokensOut / 1_000_000) * OUTPUT_COST_PER_1M;

    // Check daily budget (after processing, so we've already incurred the cost)
    // But log it for monitoring
    const withinBudget = checkDailyBudget(totalCost);
    if (!withinBudget) {
      console.warn(`⚠️  Daily budget exceeded: ${dailySpend.toFixed(4)} / ${DAILY_BUDGET_USD}`);
    }

    return NextResponse.json({
      query,
      answer: synthResult.answer,
      citations,
      chunks_used: chunks.length,
      cost_usd: totalCost,
      tokens_in: totalTokensIn,
      tokens_out: totalTokensOut,
      chunks: chunks.map(c => ({
        chunk_id: c.chunk_id,
        newsletter_id: c.newsletter_id, // Add for linking
        subject: c.subject,
        publisher: c.publisher_name,
        score: (c.normalized_score || c.combined_score || (1 - c.distance)) * 100 // Convert to percentage
      })),
      // Add publisher rankings
      publisher_rankings: calculatePublisherRankings(chunks)
    });

  } catch (error) {
    console.error('❌ Query failed:', error);
    return NextResponse.json(
      { 
        error: 'Query failed',
        message: error instanceof Error ? error.message : String(error)
      },
      { status: 500 }
    );
  }
}
</file>

<file path="scripts/ingest-to-bigquery.ts">
import * as dotenv from 'dotenv';
import * as fs from 'fs';
import * as path from 'path';
import { createHash } from 'crypto';
import { BigQuery } from '@google-cloud/bigquery';
import { getGmail, extractEmailAddress, markAsIngested } from '../src/lib/gmail';
import { extractPlaintext, getHeader } from '../src/lib/parseMessage';
import vipConfig from '../config/vip.json';
import paidConfig from '../config/paid-senders.json';
import type { gmail_v1 } from 'googleapis';

dotenv.config();

// BigQuery configuration
const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const TABLE_ID = 'messages';

// Batch processing configuration
const BATCH_SIZE = 500;
const PROGRESS_LOG_INTERVAL = 100;
const BIGQUERY_INSERT_CHUNK_SIZE = 50;  // Insert messages in chunks of 50

const TEST_MODE = false;  // Set to false for full ingestion
const TEST_LIMIT = 100;  // Only used when TEST_MODE is true

// Initialize BigQuery client using Application Default Credentials
const bigquery = new BigQuery({ projectId: PROJECT_ID });

// Statistics tracking
interface ProcessingStats {
  totalFetched: number;
  totalProcessed: number;
  totalInserted: number;
  duplicatesSkipped: number;
  failures: number;
  startTime: Date;
  batchTimes: number[];
}

interface FailedMessage {
  id: string;
  error: string;
  timestamp: string;
}

interface NewsletterMessage {
  id: string;
  sender: string;
  subject: string;
  sent_date: string | null;
  received_date: string | null;
  body_text: string;
  body_html: string | null;
  is_vip: boolean;
  is_paid: boolean | null;
  publisher_name: string;
  source_type: string;
  word_count: number;
  has_attachments: boolean;
  doc_id: string | null;
  doc_version: number | null;
  from_domain: string | null;
  list_id: string | null;
  was_forwarded: boolean | null;
  source_inbox: string | null;
}

/**
 * Generate stable doc_id from message metadata
 */
function generateDocId(sender: string, subject: string, sentDate: string | null): string {
  const data = `${sender}|${subject}|${sentDate || ''}`;
  return createHash('sha256').update(data).digest('hex').substring(0, 32);
}

/**
 * Check if an email address is VIP based on config
 */
function isVipEmail(fromEmail: string): boolean {
  // Check if email exactly matches any VIP sender
  if (vipConfig.senders.includes(fromEmail)) {
    return true;
  }
  
  // Check if domain matches any VIP domain
  const domain = fromEmail.split('@')[1]?.toLowerCase();
  if (domain && vipConfig.domains.includes(domain)) {
    return true;
  }
  
  return false;
}

/**
 * Check if an email address is a paid newsletter based on config
 */
function isPaidNewsletter(fromEmail: string): boolean {
  // Check if email exactly matches any paid sender
  if (paidConfig.senders.includes(fromEmail)) {
    return true;
  }
  
  return false;
}

/**
 * Extract publisher name from sender email or From header
 */
function extractPublisherName(fromHeader: string, fromEmail: string): string {
  // Try to extract name from "Name <email@domain.com>" format
  const nameMatch = fromHeader.match(/^(.+?)\s*<.+>$/);
  if (nameMatch && nameMatch[1]) {
    return nameMatch[1].trim();
  }
  
  // Fallback to domain name (everything before @)
  return fromEmail.split('@')[0] || 'Unknown';
}

/**
 * Count words in text
 */
function countWords(text: string): number {
  return text.trim().split(/\s+/).filter(word => word.length > 0).length;
}

/**
 * Check if message has attachments
 */
function hasAttachments(msg: gmail_v1.Schema$Message): boolean {
  return !!(msg.payload?.parts?.some((part: gmail_v1.Schema$MessagePart) => 
    part.filename && part.filename.length > 0
  ));
}

/**
 * Extract HTML content from message parts
 */
function extractHtmlContent(msg: gmail_v1.Schema$Message): string | null {
  if (!msg || !msg.payload) return null;

  const parts: gmail_v1.Schema$MessagePart[] = [];
  
  // Flatten all parts recursively
  function walk(part?: gmail_v1.Schema$MessagePart) {
    if (!part) return;
    parts.push(part);
    if (part.parts) part.parts.forEach(walk);
  }
  
  walk(msg.payload);

  // Look for text/html part
  for (const part of parts) {
    if ((part.mimeType || '').toLowerCase().startsWith('text/html')) {
      const data = part.body?.data;
      if (data) {
        // Decode base64 URL
        const normalized = data.replace(/-/g, '+').replace(/_/g, '/');
        const buff = Buffer.from(normalized, 'base64');
        return buff.toString('utf-8');
      }
    }
  }
  
  return null;
}

/**
 * Convert Gmail internal date to ISO string
 */
function convertInternalDate(internalDate: string): string | null {
  try {
    const timestamp = parseInt(internalDate);
    return new Date(timestamp).toISOString();
  } catch {
    return null;
  }
}

/**
 * Convert Date header to ISO string
 */
function convertDateHeader(dateHeader: string): string | null {
  try {
    return new Date(dateHeader).toISOString();
  } catch {
    return null;
  }
}

/**
 * Check which message IDs already exist in BigQuery
 */
async function getExistingMessageIds(messageIds: string[]): Promise<Set<string>> {
  try {
    const query = `
      SELECT id 
      FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\`
      WHERE id IN (${messageIds.map(id => `'${id}'`).join(', ')})
    `;
    
    const [rows] = await bigquery.query(query);
    return new Set(rows.map((row: any) => row.id));
  } catch (error) {
    console.error('Error checking existing messages:', error);
    return new Set(); // Return empty set on error to be safe
  }
}

/**
 * Process a single message with error handling
 */
async function processMessage(
  gmail: gmail_v1.Gmail, 
  msgId: string, 
  stats: ProcessingStats, 
  failedMessages: FailedMessage[]
): Promise<NewsletterMessage | null> {
  try {
    // Get full message content
    const fullMsg = await gmail.users.messages.get({
      userId: 'me',
      id: msgId,
      format: 'full'
    });
    
    // Extract headers
    const fromHeader = getHeader(fullMsg.data, 'From');
    const subject = getHeader(fullMsg.data, 'Subject') || '(no subject)';
    const dateHeader = getHeader(fullMsg.data, 'Date');
    const listIdHeader = getHeader(fullMsg.data, 'List-Id');
    
    // Extract email address
    const fromEmail = extractEmailAddress(fromHeader);
    const fromDomain = fromEmail.split('@')[1] || null;
    
    // Extract content
    const bodyText = extractPlaintext(fullMsg.data);
    const bodyHtml = extractHtmlContent(fullMsg.data);
    
    // Check if forwarded
    const wasForwarded = getHeader(fullMsg.data, 'X-Forwarded-For') !== '' || 
                         getHeader(fullMsg.data, 'X-Original-To') !== '' ||
                         fromHeader.toLowerCase().includes('forwarded message');
    
    // Generate doc_id
    const sentDate = convertDateHeader(dateHeader);
    const docId = generateDocId(fromEmail, subject, sentDate);
    
    // Build message object
    const message: NewsletterMessage = {
      id: msgId,
      sender: fromEmail,
      subject: subject,
      sent_date: sentDate,
      received_date: convertInternalDate(fullMsg.data.internalDate || ''),
      body_text: bodyText,
      body_html: bodyHtml,
      is_vip: isVipEmail(fromEmail),
      is_paid: isPaidNewsletter(fromEmail),
      publisher_name: extractPublisherName(fromHeader, fromEmail),
      source_type: 'newsletter',
      word_count: countWords(bodyText),
      has_attachments: hasAttachments(fullMsg.data),
      doc_id: docId,
      doc_version: 1,
      from_domain: fromDomain,
      list_id: listIdHeader || null,
      was_forwarded: wasForwarded,
      source_inbox: null  // Will be set by caller if needed
    };
    
    return message;
  } catch (error) {
    const errorMsg = error instanceof Error ? error.message : String(error);
    failedMessages.push({
      id: msgId,
      error: errorMsg,
      timestamp: new Date().toISOString()
    });
    stats.failures++;
    console.error(`Failed to process message ${msgId}:`, errorMsg);
    return null;
  }
}

/**
 * Insert messages to BigQuery in chunks to avoid 413 errors
 * Optionally applies Gmail labels for successfully inserted messages
 */
async function insertMessagesInChunks(
  messages: NewsletterMessage[],
  stats: ProcessingStats,
  gmail?: gmail_v1.Gmail,
  inboxType?: 'legacy' | 'clean'
): Promise<void> {
  if (messages.length === 0) return;
  
  const dataset = bigquery.dataset(DATASET_ID);
  const table = dataset.table(TABLE_ID);
  
  // Split messages into chunks
  const chunks: NewsletterMessage[][] = [];
  for (let i = 0; i < messages.length; i += BIGQUERY_INSERT_CHUNK_SIZE) {
    chunks.push(messages.slice(i, i + BIGQUERY_INSERT_CHUNK_SIZE));
  }
  
  console.log(`💾 Inserting ${messages.length} messages in ${chunks.length} chunks of ${BIGQUERY_INSERT_CHUNK_SIZE}...`);
  
  // Insert each chunk
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i];
    const chunkNumber = i + 1;
    
    try {
      console.log(`📤 Inserting chunk ${chunkNumber}/${chunks.length} (${chunk.length} messages)...`);
      
      const insertResult = await table.insert(chunk);
      
      // Check for insertion errors
      const response = insertResult[0] as any;
      if (response?.insertErrors && response.insertErrors.length > 0) {
        console.error(`❌ Chunk ${chunkNumber} had insertion errors:`, response.insertErrors);
        // Continue with next chunk even if this one failed
      } else {
        stats.totalInserted += chunk.length;
        console.log(`✅ Chunk ${chunkNumber} inserted successfully (${chunk.length} messages)`);
        
        // Apply Gmail labels for successfully inserted messages (only for clean inbox)
        if (gmail && inboxType === 'clean') {
          console.log(`🏷️  Applying labels to ${chunk.length} messages...`);
          for (const msg of chunk) {
            await markAsIngested(gmail, msg.id);
          }
        }
      }
      
    } catch (error) {
      console.error(`❌ Failed to insert chunk ${chunkNumber}:`, error);
      // Continue with next chunk
    }
  }
}

/**
 * Write failed messages to file
 */
function writeFailedMessages(failedMessages: FailedMessage[]): void {
  if (failedMessages.length === 0) return;
  
  const failedFile = path.join(process.cwd(), 'failed-messages.json');
  try {
    fs.writeFileSync(failedFile, JSON.stringify(failedMessages, null, 2));
    console.log(`\n⚠️  ${failedMessages.length} failed messages written to: ${failedFile}`);
  } catch (error) {
    console.error('Failed to write failed messages file:', error);
  }
}

(async () => {
  const stats: ProcessingStats = {
    totalFetched: 0,
    totalProcessed: 0,
    totalInserted: 0,
    duplicatesSkipped: 0,
    failures: 0,
    startTime: new Date(),
    batchTimes: []
  };
  
  const failedMessages: FailedMessage[] = [];
  
  try {
    console.log('🚀 Starting newsletter ingestion...');
    console.log(`📊 Batch size: ${BATCH_SIZE} messages`);
    console.log(`⏰ Started at: ${stats.startTime.toISOString()}\n`);
    
    // Get Gmail client (default to legacy inbox)
    const GMAIL_INBOX = (process.env.GMAIL_INBOX as 'legacy' | 'clean') || 'legacy';
    const gmail = getGmail(GMAIL_INBOX);
    
    // Get custom query from env or default to inbox
    const customQuery = process.env.GMAIL_INGESTION_QUERY || 'in:inbox';
    
    // Get total message count estimate
    const initialListRes = await gmail.users.messages.list({
      userId: 'me',
      q: customQuery,
      maxResults: 1
    });
    
    const totalEstimate = initialListRes.data.resultSizeEstimate || 0;
    console.log(`📧 Estimated total messages: ${totalEstimate.toLocaleString()}`);
    
    let pageToken: string | undefined;
    let batchNumber = 1;
    
    // Process messages in batches
    while (true) {
      const batchStartTime = Date.now();
      console.log(`\n📦 Fetching batch ${batchNumber}...`);
      
      // Fetch batch of messages
      const listRes = await gmail.users.messages.list({
        userId: 'me',
        q: customQuery,
        maxResults: BATCH_SIZE,
        pageToken: pageToken || undefined
      });
      
      const messageIds = (listRes.data.messages || []).map((msg: gmail_v1.Schema$Message) => msg.id!);
      
      // Check if we should stop for test mode
      if (TEST_MODE && stats.totalFetched >= TEST_LIMIT) {
        console.log(`\n⚠️  TEST MODE: Stopping after ${stats.totalFetched} messages\n`);
        break;
      }
      
      if (messageIds.length === 0) {
        console.log('✅ No more messages to process');
        break;
      }
      
      stats.totalFetched += messageIds.length;
      console.log(`📥 Fetched ${messageIds.length} message IDs`);
      
      // Check for duplicates
      console.log('🔍 Checking for duplicates...');
      const existingIds = await getExistingMessageIds(messageIds);
      const newMessageIds = messageIds.filter(id => !existingIds.has(id));
      const duplicatesInBatch = messageIds.length - newMessageIds.length;
      
      stats.duplicatesSkipped += duplicatesInBatch;
      
      if (duplicatesInBatch > 0) {
        console.log(`⏭️  Skipping ${duplicatesInBatch} duplicates`);
      }
      
      if (newMessageIds.length === 0) {
        console.log('⏭️  All messages in this batch are duplicates, skipping...');
        pageToken = listRes.data.nextPageToken || undefined;
        batchNumber++;
        continue;
      }
      
      console.log(`🔄 Processing ${newMessageIds.length} new messages...`);
      
      // Process each message in the batch
      const messages: NewsletterMessage[] = [];
      
      for (let i = 0; i < newMessageIds.length; i++) {
        const msgId = newMessageIds[i];
        stats.totalProcessed++;
        
        // Progress logging
        if (stats.totalProcessed % PROGRESS_LOG_INTERVAL === 0) {
          const progress = totalEstimate > 0 ? (stats.totalProcessed / totalEstimate * 100).toFixed(1) : '?';
          console.log(`📈 Processed ${stats.totalProcessed}/${totalEstimate} (${progress}% complete)`);
        }
        
        const message = await processMessage(gmail, msgId, stats, failedMessages);
        if (message) {
          message.source_inbox = GMAIL_INBOX;
          messages.push(message);
        }
      }
      
      // Insert batch to BigQuery in chunks
      if (messages.length > 0) {
        await insertMessagesInChunks(messages, stats, gmail, GMAIL_INBOX);
      }
      
      // Update pagination
      pageToken = listRes.data.nextPageToken || undefined;
      if (!pageToken) {
        console.log('✅ Reached end of messages');
        break;
      }
      
      // Log batch timing
      const batchTime = (Date.now() - batchStartTime) / 1000;
      stats.batchTimes.push(batchTime);
      console.log(`⏱️  Batch ${batchNumber} took ${batchTime.toFixed(1)} seconds`);
      
      batchNumber++;
    }
    
    // Final summary
    const totalTime = (Date.now() - stats.startTime.getTime()) / 1000;
    const avgBatchTime = stats.batchTimes.length > 0 
      ? stats.batchTimes.reduce((a, b) => a + b, 0) / stats.batchTimes.length 
      : 0;
    
    console.log('\n🎉 INGESTION COMPLETE!');
    console.log('='.repeat(50));
    console.log(`📊 Total fetched: ${stats.totalFetched.toLocaleString()}`);
    console.log(`🔄 Total processed: ${stats.totalProcessed.toLocaleString()}`);
    console.log(`💾 Total inserted: ${stats.totalInserted.toLocaleString()}`);
    console.log(`⏭️  Duplicates skipped: ${stats.duplicatesSkipped.toLocaleString()}`);
    console.log(`❌ Failures: ${stats.failures.toLocaleString()}`);
    console.log(`⏰ Total time: ${(totalTime / 60).toFixed(1)} minutes`);
    console.log(`⚡ Average batch time: ${avgBatchTime.toFixed(1)} seconds`);
    console.log(`📈 Processing rate: ${(stats.totalProcessed / totalTime).toFixed(1)} messages/second`);
    
    // Write failed messages to file
    writeFailedMessages(failedMessages);
    
    if (stats.failures > 0) {
      console.log(`\n⚠️  ${stats.failures} messages failed to process. Check failed-messages.json for details.`);
    }
    
    process.exit(0);
    
  } catch (error) {
    console.error('💥 Fatal error during ingestion:', error);
    writeFailedMessages(failedMessages);
    process.exit(1);
  }
})();
</file>

<file path="Dockerfile">
# ---------- Builder ----------
FROM node:20-slim AS builder

WORKDIR /app

# Only package files first for better caching
COPY package*.json ./
RUN npm ci

# Bring in the rest of the source
COPY . .

# Compile TypeScript
RUN npm run build

# ---------- Runner ----------
FROM node:20-slim AS runner

WORKDIR /app
ENV NODE_ENV=production

# Install only production deps
COPY package*.json ./
RUN npm ci --omit=dev

# Copy compiled JS only
COPY --from=builder /app/dist ./dist

# Default command is harmless (we override in Cloud Run Jobs)
CMD ["node", "-e", "console.log('ncc-worker image ready')"]
</file>

<file path="scripts/evaluate-rag.ts">
import * as dotenv from 'dotenv';
import { BigQuery } from '@google-cloud/bigquery';
import { GoogleAuth } from 'google-auth-library';
import goldSet from '../config/gold-set.json';

dotenv.config();

const PROJECT_ID = process.env.BIGQUERY_PROJECT_ID || 'newsletter-control-center';
const DATASET_ID = 'ncc_newsletters';
const CHUNKS_TABLE = 'chunks';
const EVAL_RESULTS_TABLE = 'eval_results';
const LOCATION = 'us-central1';

interface EvaluationResult {
  question_id: string;
  question: string;
  answer: string;
  facts_extracted: number;
  citations_count: number;
  chunks_retrieved: number;
  latency_ms: number;
  tokens_in: number;
  tokens_out: number;
  cost_usd: number;
  error?: string;
  timestamp: string;
}

/**
 * Generate embedding for a query using Vertex AI
 */
async function generateEmbedding(text: string): Promise<number[]> {
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/text-embedding-004:predict`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      instances: [
        {
          content: text,
          task_type: 'RETRIEVAL_QUERY',
        }
      ]
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  
  if (data.predictions && data.predictions[0] && data.predictions[0].embeddings) {
    const embedding = data.predictions[0].embeddings.values || data.predictions[0].embeddings;
    if (Array.isArray(embedding)) {
      return embedding;
    }
  }
  
  throw new Error('No embedding returned from API');
}

/**
 * Vector search using cosine similarity
 */
async function vectorSearch(bigquery: BigQuery, queryEmbedding: number[], topK: number = 10): Promise<any[]> {
  const embeddingStr = `[${queryEmbedding.join(',')}]`;
  
  const query = `
    WITH query_embedding AS (
      SELECT ${embeddingStr} AS embedding
    ),
    chunk_distances AS (
      SELECT
        c.chunk_id,
        c.newsletter_id,
        c.chunk_index,
        c.chunk_text,
        c.subject,
        c.publisher_name,
        c.sent_date,
        c.is_paid,
        1 - (
          SELECT SUM(a * b) / (SQRT(SUM(a * a)) * SQRT(SUM(b * b)))
          FROM 
            UNNEST(query_embedding.embedding) AS a WITH OFFSET i
            JOIN 
            UNNEST(c.chunk_embedding) AS b WITH OFFSET j
          WHERE i = j
        ) AS distance
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\` c, query_embedding
      WHERE c.chunk_embedding IS NOT NULL
    )
    SELECT 
      chunk_id,
      newsletter_id,
      chunk_index,
      chunk_text,
      subject,
      publisher_name,
      sent_date,
      is_paid,
      distance
    FROM chunk_distances
    ORDER BY distance ASC
    LIMIT ${topK}
  `;

  const [rows] = await bigquery.query(query);
  return rows;
}

/**
 * Keyword search with SQL LIKE
 */
async function keywordSearch(bigquery: BigQuery, userQuery: string, topK: number = 10): Promise<any[]> {
  const escapedQuery = userQuery.replace(/'/g, "''");
  const keywords = escapedQuery.split(/\s+/).filter(k => k.length > 2);
  
  if (keywords.length === 0) return [];
  
  try {
    const conditions = keywords.map(kw => `chunk_text LIKE '%${kw}%'`).join(' AND ');
    const query = `
      SELECT 
        chunk_id,
        newsletter_id,
        chunk_index,
        chunk_text,
        subject,
        publisher_name,
        sent_date,
        is_paid,
        (LENGTH(chunk_text) - LENGTH(REPLACE(chunk_text, '${keywords[0]}', ''))) / LENGTH('${keywords[0]}') AS relevance
      FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
      WHERE ${conditions}
      ORDER BY relevance DESC
      LIMIT ${topK}
    `;

    const [rows] = await bigquery.query(query);
    return rows.map((r: any) => ({ ...r, relevance: Math.min(r.relevance / 10, 1) }));
  } catch (error) {
    console.warn('Keyword search failed:', error);
    return [];
  }
}

/**
 * Hybrid search combining vector and keyword results
 */
async function hybridSearch(bigquery: BigQuery, userQuery: string, queryEmbedding: number[], topK: number = 10): Promise<any[]> {
  const [vectorResults, keywordResults] = await Promise.all([
    vectorSearch(bigquery, queryEmbedding, topK * 2),
    keywordSearch(bigquery, userQuery, topK * 2)
  ]);

  const combined = new Map();

  // Add vector results (weight: 0.7)
  vectorResults.forEach((result) => {
    const score = result.distance || 0;
    combined.set(result.chunk_id, {
      ...result,
      vector_score: score,
      keyword_score: 0,
      combined_score: score * 0.7
    });
  });

  // Add keyword results (weight: 0.3)
  keywordResults.forEach((result) => {
    const existing = combined.get(result.chunk_id);
    if (existing) {
      existing.keyword_score = result.relevance;
      existing.combined_score = existing.vector_score * 0.7 + result.relevance * 0.3;
    } else {
      combined.set(result.chunk_id, {
        ...result,
        vector_score: 0,
        keyword_score: result.relevance,
        combined_score: result.relevance * 0.3
      });
    }
  });

  const sorted = Array.from(combined.values())
    .sort((a, b) => b.combined_score - a.combined_score)
    .slice(0, topK);

  return sorted;
}

/**
 * Fetch full chunk text from BigQuery
 */
async function getFullChunks(bigquery: BigQuery, chunkIds: string[]): Promise<any[]> {
  if (chunkIds.length === 0) return [];
  
  // Escape single quotes and wrap in quotes
  const ids = chunkIds.map(id => `'${id.replace(/'/g, "''")}'`).join(',');
  
  const query = `
    SELECT 
      chunk_id,
      newsletter_id,
      chunk_index,
      chunk_text,
      subject,
      publisher_name,
      sent_date
    FROM \`${PROJECT_ID}.${DATASET_ID}.${CHUNKS_TABLE}\`
    WHERE chunk_id IN (${ids})
  `;

  const [rows] = await bigquery.query(query);
  return rows;
}

/**
 * Format citation
 */
function formatCitation(chunk: any): string {
  const publisher = chunk.publisher_name || 'Unknown Publisher';
  const date = chunk.sent_date 
    ? new Date(chunk.sent_date.value || chunk.sent_date).toLocaleDateString('en-US', { month: 'short', day: 'numeric', year: 'numeric' })
    : 'Date unknown';
  const subject = chunk.subject || 'No subject';
  
  return `${publisher} · ${date} · ${subject}`;
}

/**
 * Extract facts from chunks
 */
async function extractFacts(chunks: any[], userQuery: string): Promise<any[]> {
  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  const context = chunks.map((chunk) => `
Chunk ${chunk.chunk_id}:
Publisher: ${chunk.publisher_name}
Date: ${chunk.sent_date ? new Date(chunk.sent_date.value || chunk.sent_date).toLocaleDateString() : 'Unknown'}
Subject: ${chunk.subject}
Content: ${chunk.chunk_text}
`).join('\n---\n');

  const prompt = `Extract all facts, quotes, and data points from the following chunks that are relevant to the query: "${userQuery}"

Return your response as a JSON array where each item has:
- fact: The extracted fact or data point
- chunk_id: The ID of the chunk it came from

Only extract facts that directly answer the query. If no relevant facts exist, return an empty array.

Chunks:
${context}

Return ONLY valid JSON, no additional text:`;


  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-2.5-pro:generateContent`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        role: 'user',
        parts: [{ text: prompt }]
      }],
      generationConfig: {
        temperature: 0.1,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 4096,
        responseMimeType: 'application/json'
      }
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Gemini API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  const text = data.candidates[0].content.parts[0].text;
  
  try {
    const facts = JSON.parse(text);
    return Array.isArray(facts) ? facts : [];
  } catch (error) {
    // Try to fix common JSON issues
    try {
      // Sometimes Gemini wraps in ```json blocks or adds markdown
      let cleaned = text.replace(/```json\n?/g, '').replace(/```\n?/g, '').trim();
      
      // Sometimes response is truncated - try to fix if we can find where it was cut
      if (!cleaned.endsWith(']') && cleaned.includes('\n  },')) {
        // Try to add closing brackets if response was truncated
        const lastCompleteFact = cleaned.lastIndexOf('}');
        if (lastCompleteFact > 0) {
          cleaned = cleaned.substring(0, lastCompleteFact + 1) + '\n]';
        }
      }
      
      const facts = JSON.parse(cleaned);
      return Array.isArray(facts) ? facts : [];
    } catch (retryError) {
      console.warn('❌ Could not parse facts:', retryError);
      return [];
    }
  }
}

/**
 * Synthesize answer from facts
 */
async function synthesizeAnswer(facts: any[], userQuery: string, chunks: any[]): Promise<string> {
  if (facts.length === 0) {
    return 'No information found in the newsletter archive that answers this query.';
  }

  const auth = new GoogleAuth({
    scopes: ['https://www.googleapis.com/auth/cloud-platform']
  });
  const client = await auth.getClient();
  const accessToken = await client.getAccessToken();

  const factsList = facts.map(f => {
    const chunk = chunks.find(c => c.chunk_id === f.chunk_id);
    const citation = chunk ? formatCitation(chunk) : `[${f.chunk_id}]`;
    return `- ${citation}: ${f.fact}`;
  }).join('\n');

  const prompt = `You are an intelligence analyst answering questions based on newsletter content.

Query: "${userQuery}"

Facts extracted from newsletters:
${factsList}

CRITICAL RULES:
1. Answer the query using ONLY the provided facts
2. Include inline citations: (Publisher · Date · Subject) after each statement
3. If information isn't in the facts, don't make it up
4. Write naturally and concisely
5. If facts are contradictory, mention both perspectives

Provide your answer:`;

  const endpoint = `https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/gemini-2.5-pro:generateContent`;
  
  const response = await fetch(endpoint, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${accessToken.token}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      contents: [{
        role: 'user',
        parts: [{ text: prompt }]
      }],
      generationConfig: {
        temperature: 0.3,
        topK: 40,
        topP: 0.95,
        maxOutputTokens: 4096
      }
    })
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Gemini API returned ${response.status}: ${errorText}`);
  }

  const data = await response.json();
  return data.candidates[0].content.parts[0].text.trim();
}

/**
 * Run RAG query
 */
export async function runRAGQuery(userQuery: string): Promise<{
  answer: string;
  facts: any[];
  citations: any[];
  chunks_used: number;
  latency_ms: number;
  tokens_in: number;
  tokens_out: number;
}> {
  const startTime = Date.now();
  const bigquery = new BigQuery({ projectId: PROJECT_ID });

  // Step 1: Generate query embedding
  const queryEmbedding = await generateEmbedding(userQuery);

  // Step 2: Perform hybrid search
  const chunks = await hybridSearch(bigquery, userQuery, queryEmbedding, 10);

  if (chunks.length === 0) {
    const latency = Date.now() - startTime;
    return {
      answer: 'No information found in the newsletter archive that answers this query.',
      facts: [],
      citations: [],
      chunks_used: 0,
      latency_ms: latency,
      tokens_in: 0,
      tokens_out: 0
    };
  }

  // Step 3: Fetch full chunks
  const chunkIds = chunks.map(c => c.chunk_id);
  const fullChunks = await getFullChunks(bigquery, chunkIds);

  // Step 4: Extract facts
  const facts = await extractFacts(fullChunks, userQuery);

  // Step 5: Synthesize answer
  const answer = await synthesizeAnswer(facts, userQuery, fullChunks);
  
  // Format citations
  const citations = Array.from(new Set(
    facts.map(f => {
      const chunk = fullChunks.find(c => c.chunk_id === f.chunk_id);
      return chunk ? formatCitation(chunk) : null;
    }).filter(Boolean)
  )).slice(0, 5);

  const latency = Date.now() - startTime;
  
  // Estimate tokens (rough: ~4 chars per token)
  const tokens_in = Math.floor((userQuery.length + fullChunks.reduce((sum, c) => sum + c.chunk_text.length, 0)) / 4);
  const tokens_out = Math.floor(answer.length / 4);

  return {
    answer,
    facts,
    citations,
    chunks_used: chunks.length,
    latency_ms: latency,
    tokens_in,
    tokens_out
  };
}

/**
 * Calculate cost (rough estimate)
 */
function calculateCost(tokensIn: number, tokensOut: number): number {
  // Gemini 2.5 Pro pricing (approximate)
  const INPUT_COST_PER_1M = 1.25;  // $1.25 per 1M tokens
  const OUTPUT_COST_PER_1M = 5.00;  // $5.00 per 1M tokens
  
  return (tokensIn / 1_000_000) * INPUT_COST_PER_1M + (tokensOut / 1_000_000) * OUTPUT_COST_PER_1M;
}

/**
 * Evaluate single question
 */
async function evaluateQuestion(question: any, bigquery: BigQuery): Promise<EvaluationResult> {
  console.log(`\n📋 Testing: "${question.question}"`);
  
  try {
    const result = await runRAGQuery(question.question);
    
    const evalResult: EvaluationResult = {
      question_id: question.id,
      question: question.question,
      answer: result.answer,
      facts_extracted: result.facts.length,
      citations_count: result.citations.length,
      chunks_retrieved: result.chunks_used,
      latency_ms: result.latency_ms,
      tokens_in: result.tokens_in,
      tokens_out: result.tokens_out,
      cost_usd: calculateCost(result.tokens_in, result.tokens_out),
      timestamp: new Date().toISOString()
    };

    console.log(`   ✅ Extracted ${result.facts.length} facts, ${result.citations.length} citations`);
    console.log(`   ⏱️  Latency: ${result.latency_ms}ms`);
    console.log(`   💰 Cost: $${calculateCost(result.tokens_in, result.tokens_out).toFixed(4)}`);
    
    return evalResult;
  } catch (error) {
    console.log(`   ❌ Error: ${error instanceof Error ? error.message : error}`);
    return {
      question_id: question.id,
      question: question.question,
      answer: '',
      facts_extracted: 0,
      citations_count: 0,
      chunks_retrieved: 0,
      latency_ms: 0,
      tokens_in: 0,
      tokens_out: 0,
      cost_usd: 0,
      error: error instanceof Error ? error.message : String(error),
      timestamp: new Date().toISOString()
    };
  }
}

/**
 * Main evaluation function
 */
async function runEvaluation() {
  const bigquery = new BigQuery({ projectId: PROJECT_ID });
  
  console.log('═══════════════════════════════════════════════════════════');
  console.log('🧪 RAG EVALUATION HARNESS');
  console.log('═══════════════════════════════════════════════════════════\n');
  console.log(`📊 Testing ${goldSet.questions.length} questions\n`);
  
  const results: EvaluationResult[] = [];
  
  // Run evaluation for each question
  for (const question of goldSet.questions) {
    const result = await evaluateQuestion(question, bigquery);
    results.push(result);
  }
  
  // Calculate summary stats
  const totalCost = results.reduce((sum, r) => sum + r.cost_usd, 0);
  const avgLatency = results.reduce((sum, r) => sum + r.latency_ms, 0) / results.length;
  const avgFacts = results.reduce((sum, r) => sum + r.facts_extracted, 0) / results.length;
  const avgCitations = results.reduce((sum, r) => sum + r.citations_count, 0) / results.length;
  const errors = results.filter(r => r.error).length;
  
  console.log('\n═══════════════════════════════════════════════════════════');
  console.log('📊 EVALUATION RESULTS');
  console.log('═══════════════════════════════════════════════════════════\n');
  console.log(`Total Questions: ${results.length}`);
  console.log(`Errors: ${errors}`);
  console.log(`Avg Facts Extracted: ${avgFacts.toFixed(1)}`);
  console.log(`Avg Citations: ${avgCitations.toFixed(1)}`);
  console.log(`Avg Latency: ${avgLatency.toFixed(0)}ms`);
  console.log(`Total Cost: $${totalCost.toFixed(4)}`);
  console.log('');
  
  // Store results in BigQuery
  try {
    const dataset = bigquery.dataset(DATASET_ID);
    const table = dataset.table(EVAL_RESULTS_TABLE);
    
    // Check if table exists
    try {
      await table.getMetadata();
    } catch {
      console.log('📝 Creating eval_results table...');
      await dataset.createTable(EVAL_RESULTS_TABLE, {
        schema: [
          { name: 'question_id', type: 'STRING', mode: 'REQUIRED' },
          { name: 'question', type: 'STRING', mode: 'REQUIRED' },
          { name: 'answer', type: 'STRING', mode: 'NULLABLE' },
          { name: 'facts_extracted', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'citations_count', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'chunks_retrieved', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'latency_ms', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'tokens_in', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'tokens_out', type: 'INTEGER', mode: 'NULLABLE' },
          { name: 'cost_usd', type: 'FLOAT', mode: 'NULLABLE' },
          { name: 'error', type: 'STRING', mode: 'NULLABLE' },
          { name: 'timestamp', type: 'TIMESTAMP', mode: 'REQUIRED' }
        ]
      });
    }
    
    await table.insert(results);
    console.log('✅ Results saved to BigQuery\n');
  } catch (error) {
    console.error('⚠️  Failed to save results:', error);
  }
  
  console.log('═══════════════════════════════════════════════════════════\n');
}

runEvaluation().catch(console.error);
</file>

</files>
