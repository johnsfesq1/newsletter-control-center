# Cloud Run Job Fix Summary

**Date**: November 1, 2025  
**Issue**: Cloud Run job `process-newsletters` exited with error  
**Status**: âœ… Fixed

---

## ðŸ” Root Cause Analysis

### The Problem
The job failed with a BigQuery error:
```
Resources exceeded during query execution: The query could not be executed in the allotted memory.
Peak usage: 100% of limit.
Top memory consumer(s):
  ORDER BY operations: 95%
  other/unattributed: 5%
```

### Why It Happened
The script used **OFFSET-based pagination** which becomes extremely inefficient at large offsets:

```sql
SELECT * FROM messages
WHERE ...
ORDER BY id ASC
LIMIT 1000
OFFSET 49000  -- âŒ BigQuery has to scan and sort 49,000 rows!
```

When the job reached ~4,987 processed newsletters (offset ~9,000 including skipped), BigQuery ran out of memory trying to execute the ORDER BY with large OFFSET.

---

## âœ… The Fix

### 1. Cursor-Based Pagination
Replaced OFFSET with cursor-based pagination using `WHERE id > lastProcessedId`:

```sql
SELECT * FROM messages
WHERE ...
  AND id > @lastProcessedId  -- âœ… Efficient: uses index!
ORDER BY id ASC
LIMIT 1000
```

**Benefits**:
- âœ… Scales to millions of rows
- âœ… Uses BigQuery indexes efficiently
- âœ… Constant memory usage regardless of position
- âœ… Fast query execution

### 2. Progress Persistence
Added `lastProcessedId` to progress tracking:
- Progress file now stores the last processed newsletter ID
- Job can resume exactly where it left off
- No duplicate processing on restart

### 3. Robust Error Handling
- **Retry logic**: Automatic retry (3 attempts) for BigQuery resource errors
- **Exponential backoff**: Prevents overwhelming BigQuery
- **Graceful failure**: Saves progress before exiting
- **Clear error messages**: Shows exactly what went wrong

### 4. Parameterized Queries
- Uses BigQuery parameterized queries for safety
- Prevents SQL injection
- Better query performance

---

## ðŸš€ How to Resume the Job

### Option 1: Automatic Resume (Recommended)
The job will automatically resume from the last processed ID:

```bash
gcloud run jobs execute process-newsletters --region us-central1
```

The script loads the progress file and continues from `lastProcessedId`.

### Option 2: Check Progress First
```bash
# Check how many are processed
cat processing-progress.json | grep lastProcessedId

# Or query BigQuery
bq query --use_legacy_sql=false \
  "SELECT COUNT(DISTINCT newsletter_id) as processed
   FROM \`newsletter-control-center.ncc_newsletters.chunks\`"
```

### Option 3: Monitor Job Status
```bash
# List recent executions
gcloud run jobs executions list \
  --job=process-newsletters \
  --region=us-central1 \
  --limit=5

# View logs
gcloud logging read \
  "resource.type=cloud_run_job AND resource.labels.job_name=process-newsletters" \
  --limit=50 \
  --format="table(timestamp,severity,textPayload)"
```

---

## ðŸ“Š Expected Behavior After Fix

1. **First Run**: Starts from beginning, processes newsletters sequentially
2. **On Error**: Saves progress with `lastProcessedId`, exits gracefully
3. **On Restart**: Loads `lastProcessedId`, resumes from that point
4. **On Completion**: Deletes progress file, exits with success

---

## ðŸ›¡ï¸ Improvements Made

### Before
- âŒ OFFSET pagination (fails at large offsets)
- âŒ No progress persistence (couldn't resume)
- âŒ No retry logic (single failure = job fails)
- âŒ Poor error messages

### After
- âœ… Cursor-based pagination (scales infinitely)
- âœ… Progress persistence (can resume anytime)
- âœ… Retry logic (handles transient errors)
- âœ… Clear error messages and recovery instructions
- âœ… Parameterized queries (safer, faster)

---

## ðŸ”„ Migration Path

The fix is **backward compatible**:
- Existing progress files still work
- If no `lastProcessedId` exists, starts from beginning
- Automatically migrates to cursor-based pagination

---

## ðŸ“ Files Changed

1. `scripts/process-newsletters.ts`
   - Replaced OFFSET with cursor-based pagination
   - Added `lastProcessedId` to progress tracking
   - Added retry logic for BigQuery errors
   - Improved error handling

---

## âœ… Testing Recommendations

Before running the full job:
1. Test with small batch: `PROCESS_LIMIT=10`
2. Verify it resumes correctly
3. Test error recovery (kill job mid-run)
4. Verify no duplicates are created

---

## ðŸŽ¯ Next Steps

1. **Deploy the fix**: Rebuild and redeploy the Docker image
2. **Restart the job**: Execute the Cloud Run job
3. **Monitor**: Watch logs to ensure it's working
4. **Verify**: Check that it resumes from the correct position

---

## ðŸ’¡ Lessons Learned

1. **OFFSET is the enemy**: Never use OFFSET for large-scale pagination
2. **Cursor-based pagination**: Always use WHERE id > lastId for scale
3. **Progress persistence**: Critical for long-running batch jobs
4. **Error resilience**: Automatic retries save time and money

---

**The job should now be able to process all 60,000+ newsletters without running into memory issues!** ðŸŽ‰
